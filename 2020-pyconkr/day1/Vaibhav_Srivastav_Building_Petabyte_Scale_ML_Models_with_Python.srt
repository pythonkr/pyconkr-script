1
00:00:10,731 --> 00:00:13,313
Hello, good time of the day.

2
00:00:13,337 --> 00:00:15,461
Welcome to my talk.

3
00:00:15,485 --> 00:00:17,432
Today, I'm going to be talking about

4
00:00:17,456 --> 00:00:20,682
Building Petabyte Scale Machine
Learning Models Using Python,

5
00:00:20,706 --> 00:00:25,143
specifically Tensorflow and Pytorch.

6
00:00:25,167 --> 00:00:28,541
Before we get started
just a quick introduction.

7
00:00:28,565 --> 00:00:31,799
My name is Vaibhav Srivastav.
You can call me VB.

8
00:00:31,823 --> 00:00:36,827
I have spoken at PyCon
Korea by before as well -

9
00:00:36,852 --> 00:00:39,340
in fact, last year I gave a
workshop as well as a talk.

10
00:00:39,364 --> 00:00:43,240
Throughout this talk duration,
if you have any questions

11
00:00:43,264 --> 00:00:49,213
and whenever you're watching the talk,
feel free to tweet out to me @reach_vb.

12
00:00:49,237 --> 00:00:54,553
If you want to find my blog, or
if you want to find my email ID,

13
00:00:54,578 --> 00:00:56,348
you can go to vaibhav.blog.

14
00:00:56,372 --> 00:00:59,811
You would find the information
on the bottom left

15
00:00:59,835 --> 00:01:02,788
about my Twitter handle
as well as my website.

16
00:01:02,812 --> 00:01:05,784
Now, without further ado
let's get started.

17
00:01:05,808 --> 00:01:10,979
Now before we get started, I
want to try something new here, right?

18
00:01:11,003 --> 00:01:15,876
So, before we begin the actual talk,
I want to spend a minute

19
00:01:15,900 --> 00:01:19,745
and I want to convince you why distributed
machine learning is something

20
00:01:19,769 --> 00:01:21,495
that you should be passionate about,

21
00:01:21,519 --> 00:01:23,331
and something that you
should be excited about.

22
00:01:23,355 --> 00:01:26,213
For that, what we did was

23
00:01:26,237 --> 00:01:30,955
we ran a couple of benchmarks
on Tensorflow and Pytorch,

24
00:01:30,979 --> 00:01:37,191
both on CPU as well as a couple of
GPUs for an image classification task.

25
00:01:37,215 --> 00:01:42,205
Now what we did for this task was
we took the CIFAR-10 dataset.

26
00:01:42,229 --> 00:01:46,861
This data set, mind, has
roughly about 10 classes,

27
00:01:46,885 --> 00:01:52,598
some 60,000-odd images,
and what we did was we--

28
00:01:52,622 --> 00:01:58,150
we provisioned an n1-standard-4 instance
on Google Cloud Platform.

29
00:01:58,174 --> 00:02:03,096
And you can find
the hardware details down below.

30
00:02:03,120 --> 00:02:08,618
We also provisioned a couple of GPUs
which were NVIDIA Tesla T4.

31
00:02:08,642 --> 00:02:11,954
And the intention was that
we run multiple experiments

32
00:02:11,978 --> 00:02:17,124
by changing the number of work
codes, by changing the number of GPUs,

33
00:02:17,148 --> 00:02:21,642
changing the number of CPUs,
and then measuring how much

34
00:02:21,666 --> 00:02:24,449
of a time difference or how
much of a speedup difference

35
00:02:24,473 --> 00:02:26,110
does the code actually get?

36
00:02:27,371 --> 00:02:29,488
All right, let's go to the results.

37
00:02:29,512 --> 00:02:32,579
Now, after running
multiple such experiments,

38
00:02:32,603 --> 00:02:39,416
we found that both for Tensorflow
as well as for Pytorch,

39
00:02:39,440 --> 00:02:44,713
the overall speedup when
you're training the model

40
00:02:44,737 --> 00:02:49,744
it increases manifold times as you
increase the number of workers.

41
00:02:49,768 --> 00:02:57,723
Now specifically in the case of both
in the case of Tensorflow, as well as

42
00:02:57,747 --> 00:03:03,291
in the case of Pytorch GPU, on your left
you would see that the overall speedup

43
00:03:03,315 --> 00:03:08,254
relative to one worker, meaning how
much time would you spend while you're

44
00:03:08,278 --> 00:03:13,131
running your entire machine learning
modeling exercise on your own laptop

45
00:03:13,155 --> 00:03:15,123
or just on one worker.

46
00:03:15,147 --> 00:03:19,774
If you compare that with running the
same exercise with multiple workers,

47
00:03:19,798 --> 00:03:24,799
you would see that the overall speedup
is you know, manifold times more than

48
00:03:24,823 --> 00:03:26,604
what it should be, right?

49
00:03:26,628 --> 00:03:29,431
What it should be
when you only have one worker.

50
00:03:29,455 --> 00:03:35,004
And you would see that the
overall speedup is pretty much the…

51
00:03:35,028 --> 00:03:37,165
is pretty much the...

52
00:03:37,189 --> 00:03:41,329
it follows the same trend across
CPU, as well as GPUs, right?

53
00:03:41,353 --> 00:03:44,528
What that tells us is as
you increase the parallelism,

54
00:03:44,553 --> 00:03:47,534
as you increase the distribution
in your overall cluster,

55
00:03:47,558 --> 00:03:53,666
your overall time to train your machine
learning model decreases.

56
00:03:53,690 --> 00:04:00,291
And you can also get the same inferences
from the graph on the right as well.

57
00:04:00,315 --> 00:04:05,770
The graph on the right talks
specifically about two key--

58
00:04:05,794 --> 00:04:07,486
about two key parameters, right?

59
00:04:07,510 --> 00:04:12,301
The first one is the compute
that is-- how much of an--

60
00:04:12,325 --> 00:04:17,853
how much of my overall clocks
are being put into training my module.

61
00:04:17,877 --> 00:04:22,949
Second is communication. How much
of an IO overhead do I have

62
00:04:22,973 --> 00:04:30,126
between my workers or between the dataset
and my overall training exercise.

63
00:04:30,150 --> 00:04:34,947
And you would see clearly that as
we increase the number of workers

64
00:04:34,971 --> 00:04:41,099
both the overall communication overhead
also decreases, as well as the--

65
00:04:41,123 --> 00:04:47,799
the overall number of-- the overall
time taken to achieve 80% accuracy,

66
00:04:47,823 --> 00:04:50,426
meaning, the overall time taken
for the model

67
00:04:50,450 --> 00:04:55,410
to become 80% accurate on the dataset
also decreases.

68
00:04:55,434 --> 00:05:00,150
So, this is a clear case for building
distributed machine learning pipelines.

69
00:05:00,174 --> 00:05:07,279
If this looks a bit confusing to you
bear with me for a couple more minutes.

70
00:05:07,304 --> 00:05:11,021
I'm just going to get into
demystifying this-- this entire stuff.

71
00:05:11,071 --> 00:05:15,009
But I wanted to start off and then give
you like a real-life example of how much

72
00:05:15,033 --> 00:05:19,946
easy does your life becomes when you build
a distributed machine learning pipeline

73
00:05:19,971 --> 00:05:23,777
and when you train it—
when you train your models at scale.

74
00:05:23,801 --> 00:05:31,122
They not only run faster, they
also execute your entire pipeline in a

75
00:05:31,147 --> 00:05:39,142
much faster fashion, and reach the
benchmark accuracy in a more faster way.

76
00:05:40,161 --> 00:05:41,754
Just a quick thing.

77
00:05:41,778 --> 00:05:45,037
All of these benchmarks
were taken from MLBench.

78
00:05:45,069 --> 00:05:50,219
MLBench is a joint
initiative with PWC, Google,

79
00:05:50,243 --> 00:05:54,002
a couple other academic
institutes as well.

80
00:05:54,026 --> 00:05:57,307
You can find the link to that
on the bottom right.

81
00:05:57,331 --> 00:05:59,864
So, just click on that and
you'll be able to get both the code

82
00:05:59,888 --> 00:06:04,772
as well as the benchmark results as well
and a detailed explanation of how those

83
00:06:04,796 --> 00:06:08,508
jobs were run, and what actually
happened around those as well.

84
00:06:10,048 --> 00:06:13,495
All right, now that you've got
a bit of feel of what

85
00:06:13,519 --> 00:06:17,099
distributed machine learning can do,
let's go through

86
00:06:17,123 --> 00:06:19,929
let's go through the
agenda real quick, right?

87
00:06:19,953 --> 00:06:23,874
So, today we're going to be
covering like six key things.

88
00:06:23,898 --> 00:06:27,800
First, we're going to be talking about why
do we need distributed machine learning.

89
00:06:27,824 --> 00:06:32,466
We're going to get a broader intuition
of how distributed machine learning works.

90
00:06:32,490 --> 00:06:36,862
What are different types of machine
learning, distributed machine learning?

91
00:06:36,886 --> 00:06:40,911
Then we're going to be specifically
talking about out of core machine learning

92
00:06:40,935 --> 00:06:42,786
which is a type of
distributed machine learning,

93
00:06:42,810 --> 00:06:47,733
and then we spend a
bit of time talking about

94
00:06:47,757 --> 00:06:51,319
how do we actually build these scalable
machine learning workflows?

95
00:06:51,343 --> 00:06:57,134
And do some sort of discussion around
what sort of frameworks Tensorflow does,

96
00:06:57,158 --> 00:07:01,386
Pytorch. What should you use
in order to be able to build

97
00:07:01,410 --> 00:07:03,456
those scalable machine
learning workflows?

98
00:07:03,480 --> 00:07:07,157
Last but not the least my favorite part,

99
00:07:07,181 --> 00:07:11,042
we're going to be doing code walk-through.
We're going to be, you know, talking

100
00:07:11,066 --> 00:07:12,878
by Python, we're going
to be talking Tensorflow

101
00:07:12,902 --> 00:07:16,255
and we're going to be going
through both of these frameworks

102
00:07:16,279 --> 00:07:20,577
and I've prepared some sweet Colab
notebooks for you that you can use

103
00:07:20,601 --> 00:07:26,126
and leverage later on when you're
building your machine learning pipelines.

104
00:07:26,150 --> 00:07:30,925
Then towards the end, we're going
to finish up with some sort of questions

105
00:07:30,949 --> 00:07:34,021
and answers that you can
put on the comments or

106
00:07:34,045 --> 00:07:39,914
you know, you can tweet out to me on
my tweet handle on the bottom left side.

107
00:07:41,610 --> 00:07:43,880
Cool. Let's get started.

108
00:07:43,904 --> 00:07:47,962
So, first of all, why do we need
distributed machine learning?

109
00:07:47,986 --> 00:07:52,933
While we answer this entire
question, let's take a step back.

110
00:07:52,957 --> 00:07:57,830
And let's look at how a typical
machine learning exercise look like.

111
00:07:57,854 --> 00:08:01,949
In a typical machine learning exercise,
you would have some sort of data, right?

112
00:08:01,973 --> 00:08:07,654
This data can be, images, can
be text, can be audio signals,

113
00:08:07,678 --> 00:08:09,967
can be anything like that.

114
00:08:09,991 --> 00:08:14,857
And you would have some
sort of a task to do with it.

115
00:08:14,885 --> 00:08:17,574
You would have a task, like,
for example, classification

116
00:08:17,598 --> 00:08:19,441
or you could have a task like regression

117
00:08:19,465 --> 00:08:22,066
or forecasting,
something like that, right?

118
00:08:22,090 --> 00:08:26,822
Let's take an example.
Say you have a box full of balls.

119
00:08:26,846 --> 00:08:31,707
So, a simple classification
task in that would be that you

120
00:08:31,731 --> 00:08:35,686
take all the balls of red color
and put them into another box.

121
00:08:35,710 --> 00:08:40,571
You take all the balls of blue color
and put them into one different box.

122
00:08:40,595 --> 00:08:44,071
That's basically classification, right?
So, you're classifying your data.

123
00:08:44,095 --> 00:08:48,501
So, you have some data and you
have a task to perform with that. Simple.

124
00:08:48,525 --> 00:08:53,588
Now, how do you actually train a machine
learning model to learn that task?

125
00:08:53,612 --> 00:08:59,040
You would have some sort of an algorithm
which would be consuming this data

126
00:08:59,064 --> 00:09:02,675
and would be churning
out your entire task.

127
00:09:02,699 --> 00:09:07,318
Now for a very simple, you know,

128
00:09:07,342 --> 00:09:10,318
classification example, let's
pick the balls example itself.

129
00:09:10,342 --> 00:09:17,021
For that example, you will have some
sort of a model like logistic regression

130
00:09:17,052 --> 00:09:20,593
or say a neural network or Naive Bayes,

131
00:09:20,617 --> 00:09:22,876
some sort of an algorithm
like that …

132
00:09:22,900 --> 00:09:29,727
to sort of take the data that you
have and classify that data for that.

133
00:09:29,751 --> 00:09:34,338
And in case of regression, you would
have either say an ARIMA model

134
00:09:34,362 --> 00:09:37,998
or say a linear regression
or random forest regression,

135
00:09:38,023 --> 00:09:41,590
decision tree regression, all sorts
of things that are present right now.

136
00:09:41,614 --> 00:09:47,076
Bottom line is for a given data and
for a given task, you would have a model

137
00:09:47,100 --> 00:09:50,703
or some sort of an
algorithm solving for X, right?

138
00:09:50,727 --> 00:09:56,838
And then you would have, like--
for this model to be able to

139
00:09:56,862 --> 00:10:01,285
give you the best possible results, you
would have some sort of a loss criteria

140
00:10:01,309 --> 00:10:03,340
or some sort of an optimization criteria.

141
00:10:03,364 --> 00:10:10,740
Now, what this optimization
criteria effectively would be,

142
00:10:10,764 --> 00:10:16,410
is that this loss criteria would tell you
at what point in your equation plane--

143
00:10:16,434 --> 00:10:22,377
At what time, at what point in the
entire plane of your n-dimensional data

144
00:10:22,401 --> 00:10:28,376
where exactly do you get a curve
or do you get like-- get a surface

145
00:10:28,400 --> 00:10:32,955
that best describes
your entire data?

146
00:10:32,979 --> 00:10:38,754
So, in a two dimensional
plane, that can be Y=MX+C.

147
00:10:38,778 --> 00:10:40,733
So, how do you get that?

148
00:10:40,757 --> 00:10:46,787
By finding the exact equation
of that line wherein the overall loss

149
00:10:46,811 --> 00:10:49,371
from where your data
is, is the least.

150
00:10:49,396 --> 00:10:51,072
So, you have some
sort of a loss function.

151
00:10:51,097 --> 00:10:55,075
In like a regression scenario you would
have something like mean squared error

152
00:10:55,099 --> 00:11:00,375
or, you know, R squared score, or
RMSE, something like that, right?

153
00:11:00,399 --> 00:11:01,866
So, you have a loss function.

154
00:11:01,890 --> 00:11:05,983
So again, you have the data, you
have a task, you have a model

155
00:11:06,007 --> 00:11:09,140
and then you have your loss function.

156
00:11:09,164 --> 00:11:14,746
Then like, you know, like what
function do you have to minimize, right?

157
00:11:14,770 --> 00:11:17,857
You know what function
do you have to minimize.

158
00:11:17,881 --> 00:11:20,779
Then you will have some sort of
an optimization procedure.

159
00:11:20,803 --> 00:11:26,393
In a very simple neural network,
you would typically use something like

160
00:11:26,417 --> 00:11:31,012
either gradient descent or stochastic
gradient descent wherein you're like--

161
00:11:31,036 --> 00:11:34,678
you’re taking small, small steps
across the plain of your data

162
00:11:34,702 --> 00:11:38,100
and you're trying to see what's
the value of your loss there.

163
00:11:38,124 --> 00:11:41,672
If your loss is very low, then you say--

164
00:11:41,696 --> 00:11:44,455
if your loss is lower than
the previous loss,

165
00:11:44,479 --> 00:11:46,994
then you say that this is a
step in the right direction, right?

166
00:11:47,018 --> 00:11:49,963
And you keep on traversing
through all the curves,

167
00:11:49,963 --> 00:11:54,232
all the data points within
your datasets across n dimensions.

168
00:11:54,256 --> 00:11:56,583
And that's how towards the end,

169
00:11:56,583 --> 00:12:00,496
once your loss starts plateauing
to a particular point,

170
00:12:00,496 --> 00:12:03,974
then you know that
this is the most optimal

171
00:12:03,998 --> 00:12:08,650
plain or most optimal equation
for representing your data.

172
00:12:08,674 --> 00:12:12,480
So again, you have the data, you
have the task, you have your model,

173
00:12:12,504 --> 00:12:16,334
you have your loss
criteria and you have your--

174
00:12:17,584 --> 00:12:19,623
optimization procedure, right?

175
00:12:19,647 --> 00:12:24,363
So, far so good, a very simple neural net
or a very simple machine learning model,

176
00:12:24,388 --> 00:12:27,398
which is performing a
task that you've given to it.

177
00:12:27,422 --> 00:12:29,010
Now--

178
00:12:30,487 --> 00:12:32,885
all the way up until here,
let me go back.

179
00:12:32,909 --> 00:12:36,018
And now all the way up until
here, everything's good. Right?

180
00:12:36,042 --> 00:12:38,364
You can read your data into pandas.

181
00:12:38,364 --> 00:12:42,951
You can import some sort of a library

182
00:12:42,951 --> 00:12:47,946
say, scikit-learn, you can import
say random forest or decision trees,

183
00:12:47,970 --> 00:12:52,744
build your model, look at the overall
evaluation and you're good to go, right?

184
00:12:52,768 --> 00:12:57,387
And you know this kind of setup
would work all right.

185
00:12:57,411 --> 00:13:01,907
And effectively, right now, you won't see
a need for distributed machine learning.

186
00:13:01,931 --> 00:13:04,108
So, then why do we actually
need machine learning?

187
00:13:04,132 --> 00:13:06,291
Why do we actually
need distributed machine learning?

188
00:13:06,315 --> 00:13:11,004
We need it specifically in cases
where you have massive data scale

189
00:13:11,028 --> 00:13:14,678
or you have massive model scale.

190
00:13:14,702 --> 00:13:18,971
And you know, in the third case,

191
00:13:18,971 --> 00:13:22,118
maybe you just want efficient computation
of your algorithms.

192
00:13:22,118 --> 00:13:26,891
Maybe you have
a very complex optimization strategy

193
00:13:26,891 --> 00:13:30,238
and you know it takes a lot
of time and you want to divide that across

194
00:13:30,262 --> 00:13:32,488
a couple of machines
to be able to reduce that.

195
00:13:32,512 --> 00:13:35,393
So let's spend a minute
on this slide, right?

196
00:13:35,417 --> 00:13:37,709
So, what does massive data scale mean?

197
00:13:37,733 --> 00:13:41,609
So, one of the first things is that
when we talk about data scale,

198
00:13:41,633 --> 00:13:43,776
it is very relative.

199
00:13:43,801 --> 00:13:49,935
What I mean by that is, for
a computer or for a laptop

200
00:13:49,959 --> 00:13:56,612
with 16GB of RAM, you
know, 32GB of data is something

201
00:13:56,636 --> 00:13:58,997
which is big data, right?

202
00:13:59,021 --> 00:14:04,059
Which is massive data for a
laptop with 16GB of RAM.

203
00:14:04,083 --> 00:14:08,846
Whereas for a Raspberry Pi with just 1GB of RAM or with just 2GB of RAM,

204
00:14:08,870 --> 00:14:13,941
even like 4GB of data would be very huge.

205
00:14:13,965 --> 00:14:19,724
So, your massive data scale depends upon
what kind of a machine or what kind of

206
00:14:19,748 --> 00:14:23,657
what kind of cluster are you
running your analytics pipeline

207
00:14:23,657 --> 00:14:26,570
or your machine learning pipeline on.

208
00:14:26,594 --> 00:14:31,222
Similarly, model scale
can also have multiple meanings.

209
00:14:31,246 --> 00:14:35,396
So, a model can be big.

210
00:14:35,420 --> 00:14:42,119
Say for example, a neural network can
be, say 500MB in size or say --

211
00:14:42,144 --> 00:14:46,079
say 1000MB in size.
Actually, know what, let me go back, sorry.

212
00:14:46,123 --> 00:14:49,395
So, when we talk about
massive model scales,

213
00:14:49,419 --> 00:14:56,389
effectively, what we mean
by a massive model scale is

214
00:14:56,414 --> 00:14:58,572
the overall architecture
of your model itself, right?

215
00:14:58,596 --> 00:15:02,642
So, for example, when we talk about
say Naïve Bayes or say a random forest

216
00:15:02,666 --> 00:15:05,628
or decision trees, those are
actually quite small models, right?

217
00:15:05,652 --> 00:15:09,739
Why? Because those are effectively
a bunch of mathematical equations

218
00:15:09,763 --> 00:15:14,718
and their corresponding values in
an overall rated circuit. Right?

219
00:15:14,742 --> 00:15:16,788
Rated graph.

220
00:15:16,812 --> 00:15:21,438
Whereas when you talk about, say
something which is as complex as,

221
00:15:21,462 --> 00:15:23,628
say for example neural
collaborative filtering,

222
00:15:23,652 --> 00:15:27,088
or say, convolutional neural
networks, or recurrent neural networks,

223
00:15:27,112 --> 00:15:31,626
then those models would have your overall
embedding of the data itself, as well as

224
00:15:31,650 --> 00:15:34,722
the weights for individual
nodes on the architecture itself,

225
00:15:34,746 --> 00:15:39,480
which would sometimes correspond
to massive data sizes.

226
00:15:39,504 --> 00:15:42,612
Sorry, model sizes.

227
00:15:42,640 --> 00:15:46,580
Those model sizes could be 1GB,
could be 10GB

228
00:15:46,600 --> 00:15:52,440
or in case of say-- you know some
models that you see right now like GPT-3,

229
00:15:52,460 --> 00:15:58,760
those could be like of the order
of you know, couple-- you know,

230
00:15:58,779 --> 00:16:01,960
of the order of tens or
twenties of gigabytes as well,

231
00:16:01,984 --> 00:16:04,530
something that does not
even fit in your RAM as well.

232
00:16:04,554 --> 00:16:07,735
So, in those scenarios as well, you
will need distributed machine learning

233
00:16:07,759 --> 00:16:10,727
so that you can infer out
of that model itself, right?

234
00:16:10,751 --> 00:16:14,377
Then comes the third one, which is
efficient computation of your algorithm,

235
00:16:14,401 --> 00:16:20,043
which is say, for example, if
you're running across massive data

236
00:16:20,067 --> 00:16:22,664
and you have a very large architecture,

237
00:16:22,689 --> 00:16:27,425
then you will need a mechanism to be able
to execute your algorithms

238
00:16:27,425 --> 00:16:29,780
in the fastest possible manner, right?

239
00:16:29,780 --> 00:16:30,900
In the most efficient manner--

240
00:16:31,000 --> 00:16:34,860
wherein you are utilizing maximum
of your existing resources without

241
00:16:34,889 --> 00:16:40,408
overwhelming them or without creating
a bottleneck in between two processes.

242
00:16:40,432 --> 00:16:44,277
And that's where efficient computation
of algorithms comes into place.

243
00:16:44,301 --> 00:16:51,021
This can be something as simple as, if
we take our case of balls again, right?

244
00:16:51,045 --> 00:16:54,117
So, say, for example,
you have one box of balls,

245
00:16:54,141 --> 00:16:58,156
and you want to segregate red
and blue into two different boxes,

246
00:16:58,180 --> 00:17:03,878
then one scenario can be that you are the
only one who is segregating those balls.

247
00:17:03,902 --> 00:17:07,619
Whereas in a second scenario,
you can ask a friend of yours.

248
00:17:07,643 --> 00:17:10,943
And you can say,
“Hey man, can you help me?

249
00:17:10,967 --> 00:17:12,354
Can you take half of these balls

250
00:17:12,354 --> 00:17:15,781
and can you separately segregate them
into red and blue?”

251
00:17:15,805 --> 00:17:17,439
Now, what you've effectively done is

252
00:17:17,463 --> 00:17:20,098
you've reduced the overall workload
on yourself,

253
00:17:20,098 --> 00:17:22,228
and you've added another person

254
00:17:22,228 --> 00:17:25,886
to help you out with
the task of segregation.

255
00:17:25,910 --> 00:17:28,419
And that's actually a
Gold mine, right?

256
00:17:28,419 --> 00:17:32,147
Why, because one
you've reduced the load on you,

257
00:17:32,171 --> 00:17:39,540
which means that you will be able to
complete your task in roughly X by 2 times

258
00:17:39,564 --> 00:17:45,126
and at the same time, the other
person is also doing the other half of it.

259
00:17:45,150 --> 00:17:47,925
So, what that means is
you've effectively halved

260
00:17:47,949 --> 00:17:50,208
the time that you were
previously taking, right?

261
00:17:50,233 --> 00:17:51,368
And that gives you speed up.

262
00:17:51,368 --> 00:17:57,108
That's the most simplest case
of distributing a task.

263
00:17:57,108 --> 00:18:02,299
So, that's distributed
machine learning 101 for you.

264
00:18:02,323 --> 00:18:04,650
Moving ahead.

265
00:18:04,674 --> 00:18:11,235
If we go like more detailed

266
00:18:11,259 --> 00:18:15,050
If we go into more details
about how this actually works

267
00:18:15,074 --> 00:18:19,581
is by-- like the example
that we just spoke about.

268
00:18:19,605 --> 00:18:23,271
That's something which
is called Out-of-Core ML.

269
00:18:24,599 --> 00:18:29,909
What Out-of-Core ML does is,
it is basically an algorithm which

270
00:18:29,933 --> 00:18:32,669
uses external storage.

271
00:18:32,693 --> 00:18:35,357
And, you know,
essentially what it would do is

272
00:18:35,357 --> 00:18:39,132
it would train your model
on small chunks of data.

273
00:18:39,156 --> 00:18:42,939
So, let's take a different example now.

274
00:18:42,963 --> 00:18:48,339
Say, for example, you're building a
forecasting model for your stock prices.

275
00:18:48,363 --> 00:18:53,361
Say you want to model the
stock prices of Tesla, right?

276
00:18:53,385 --> 00:18:58,666
Now, you would take past the
couple years of Tesla prices.

277
00:18:58,690 --> 00:19:02,423
Say at a [inaudible], right?

278
00:19:02,423 --> 00:19:06,113
That gives you
like very large data frame, right?

279
00:19:06,161 --> 00:19:11,472
That now you have to train an
overall forecasting model on.

280
00:19:11,496 --> 00:19:13,365
So, what you do is

281
00:19:13,389 --> 00:19:17,638
you divide the overall
dataset into multiple chunks.

282
00:19:17,662 --> 00:19:21,806
You take one year of data first
and you train your model on that.

283
00:19:21,830 --> 00:19:25,863
You persist the weights of that, and
then for the second year of data,

284
00:19:25,887 --> 00:19:31,002
you start training from those
weights and you train on those weights.

285
00:19:31,026 --> 00:19:34,109
Then you repeat this process
for the third year of data, for the

286
00:19:34,134 --> 00:19:37,823
fourth year of the data, for the fifth
year of the data and so on and so forth.

287
00:19:37,847 --> 00:19:45,162
What is helps you do is it does not
overwhelm your overall architecture.

288
00:19:47,668 --> 00:19:52,800
And it would effectively
help you run--

289
00:19:54,544 --> 00:20:00,844
help you run or or help you train models
on very large data sets as well.

290
00:20:00,869 --> 00:20:05,545
And you can easily parallelize
these tasks as well, right?

291
00:20:05,569 --> 00:20:08,486
Because more often than not
these can be independent tasks.

292
00:20:08,510 --> 00:20:14,676
So, you can independently train 12
different models on 12 years of data,

293
00:20:14,700 --> 00:20:18,303
and then you can-- you know,
combine their weights

294
00:20:18,328 --> 00:20:22,045
and come up with some sort of
averaged model on top of that as well.

295
00:20:22,069 --> 00:20:25,019
So that's what Out-of-Core ML is,

296
00:20:25,043 --> 00:20:28,797
which means that effectively like
instead of loading the entire data dataset

297
00:20:28,822 --> 00:20:34,186
into your RAM in one go, you only
load some bit of data first,

298
00:20:34,186 --> 00:20:36,325
and then you divide and conquer.

299
00:20:37,420 --> 00:20:41,935
I hope that gives you good enough
details on what Out-of-Core ML is,

300
00:20:41,959 --> 00:20:44,627
because that's what we're going
to build on in the slides to come.

301
00:20:45,797 --> 00:20:49,382
Now, let's go back to
our drawing board.

302
00:20:49,406 --> 00:20:55,611
We spoke about having a dataset,
having a task, you know,

303
00:20:55,635 --> 00:20:59,590
having a model and then our last
function, and then having some sort

304
00:20:59,614 --> 00:21:02,676
of optimization function
on top of it, right?

305
00:21:02,700 --> 00:21:08,022
Now, let's take that
example and let's add the--

306
00:21:08,046 --> 00:21:12,087
let's add the mix of
distributed ML on that.

307
00:21:12,111 --> 00:21:16,745
So, if we were to distribute
the overall task as we said before

308
00:21:16,769 --> 00:21:20,409
into two different nodes,
how would we do that?

309
00:21:20,433 --> 00:21:23,520
You would, first of all,
at the ground level--

310
00:21:23,544 --> 00:21:26,403
You would partition your
data into two sets.

311
00:21:26,434 --> 00:21:28,381
You would have a data set partition one,

312
00:21:28,405 --> 00:21:31,592
and you would have a
data set partition two, right?

313
00:21:31,616 --> 00:21:35,760
Your task remains the same.
If it's classification, it's the same.

314
00:21:35,785 --> 00:21:41,409
If it's regression, it remains the
same, for both the partitions.

315
00:21:41,433 --> 00:21:45,938
What changes is that, except like
previously you were running that task

316
00:21:45,962 --> 00:21:48,331
or say classification
on the entire set of data,

317
00:21:48,355 --> 00:21:52,470
now you run it on a smaller set--
smaller partition of data.

318
00:21:52,494 --> 00:21:54,626
Your model remains the same again.

319
00:21:54,650 --> 00:21:59,167
So, if you were using a
convolution neural net before,

320
00:21:59,191 --> 00:22:03,752
again, you would use the same CNN, same
architecture across both the partitions.

321
00:22:03,776 --> 00:22:10,236
Your loss function updates.
And this is kind of obvious.

322
00:22:10,236 --> 00:22:17,582
Why, because your loss function earlier
was giving you the value of your loss for

323
00:22:17,606 --> 00:22:20,563
just for the entire dataset.

324
00:22:20,587 --> 00:22:23,431
But now, you're training
your entire model on a partition.

325
00:22:23,455 --> 00:22:28,197
So now you need to have a loss that
is computed on that partition itself,

326
00:22:28,221 --> 00:22:29,660
not on the entire dataset.

327
00:22:29,684 --> 00:22:31,720
So, loss function gets updated.

328
00:22:31,720 --> 00:22:36,564
Also, your overall optimization strategy
would also be updated.

329
00:22:36,564 --> 00:22:39,942
Why, because now you need to-- like,

330
00:22:39,942 --> 00:22:43,049
even though you're training
your models into two silos--

331
00:22:43,073 --> 00:22:46,776
But you still need to optimize
it to get one global minimum.

332
00:22:46,800 --> 00:22:48,481
Right? One global minimum.

333
00:22:48,505 --> 00:22:53,120
And the simplest way of
doing that can be that you

334
00:22:53,144 --> 00:22:59,361
have an F1 of X or like whatever
optimization value that you get,

335
00:22:59,385 --> 00:23:01,613
you add for both the partitions.

336
00:23:03,147 --> 00:23:08,594
And how this will be achieved is
through having some sort of communication

337
00:23:08,619 --> 00:23:12,839
and complexity and--

338
00:23:12,863 --> 00:23:15,876
basically having like asynchronous
communication happening

339
00:23:15,907 --> 00:23:17,372
between the two datasets.

340
00:23:17,396 --> 00:23:21,917
It says that, “Hey, I just finished say
X data points, and these are my weights.

341
00:23:21,942 --> 00:23:23,142
So, why don't you look at this?”

342
00:23:23,142 --> 00:23:25,566
And say that,
“You know, Hey, I just finished this,

343
00:23:25,566 --> 00:23:27,537
and these are my weights,”
and so on and so forth.

344
00:23:27,561 --> 00:23:29,654
So communication keeps
on happening between the two.

345
00:23:29,678 --> 00:23:32,433
between the two.

346
00:23:32,457 --> 00:23:39,829
And that’s the simplest way of building
a distributed machine learning pipeline.

347
00:23:39,854 --> 00:23:45,290
Right? If I was to scale this up, I
can have N number of data partitions

348
00:23:45,314 --> 00:23:48,065
working on N number
of working nodes, right?

349
00:23:48,089 --> 00:23:53,651
Running effectively the same code except
running it on small, small chunks of data.

350
00:23:53,675 --> 00:23:59,505
And then I aggregate their overall losses

351
00:23:59,529 --> 00:24:04,518
to get the best possible manner.

352
00:24:04,542 --> 00:24:08,721
So again, this does the same thing,
right? It reduces the overall time for

353
00:24:08,745 --> 00:24:10,739
running your machine learning model.

354
00:24:10,739 --> 00:24:12,763
Not only that, it also helps you

355
00:24:14,836 --> 00:24:20,088
get from your data all the weight
and insights as soon as possible.

356
00:24:20,112 --> 00:24:23,855
Now, enough theoretical stuff, right?

357
00:24:23,879 --> 00:24:28,638
If we were to do this in
action, how we do this?

358
00:24:28,662 --> 00:24:34,788
In real life, you could do it in two ways

359
00:24:34,812 --> 00:24:37,351
without going into too much details.

360
00:24:37,375 --> 00:24:42,486
The first one would be if-- say
you're familiar with scikit-learn,

361
00:24:42,510 --> 00:24:47,394
the first method would be to use
something known as partial_fit, right?

362
00:24:47,418 --> 00:24:49,748
You can do like a model
toward partial_fit.

363
00:24:49,772 --> 00:24:54,521
What partial_fit does, is it effectively
helps you build incremental models.

364
00:24:54,545 --> 00:24:58,336
So, it isn't parallelizing your task

365
00:24:58,360 --> 00:25:03,533
but what it's doing is it's giving
you the capability to train a model

366
00:25:03,557 --> 00:25:08,814
on larger datasets, larger
than your RAM datasets--

367
00:25:08,838 --> 00:25:14,408
on your laptop or on your virtual
machine, whatever it may be.

368
00:25:14,432 --> 00:25:20,967
And this is particularly useful if you're
already familiar with how Dask-ML works

369
00:25:20,992 --> 00:25:25,214
or how scikit-learn works and
so on and so this would follow

370
00:25:25,239 --> 00:25:26,944
the same kind of methodology.

371
00:25:26,968 --> 00:25:30,912
In fact, scikit-learn has a couple
of models which support partial_fit.

372
00:25:30,936 --> 00:25:34,617
And again, what this would do is it
would take small chunks of your data

373
00:25:34,641 --> 00:25:38,611
and sequentially train your model on
those small chunks, and then aggregate

374
00:25:38,635 --> 00:25:43,136
the overall weights
and aggregate the overall equations,

375
00:25:43,160 --> 00:25:46,496
your final equation
on the basis of that.

376
00:25:46,520 --> 00:25:48,649
For example, is a
stochastic gradient descent,

377
00:25:48,674 --> 00:25:52,514
regressor stochastic gradient
descent classifier in scikit-learn,

378
00:25:52,538 --> 00:25:54,162
which supports partial_fit as well.

379
00:25:54,186 --> 00:25:58,267
So, you can use that and effectively
you can train a machine learning model

380
00:25:58,291 --> 00:26:04,751
on a dataset of size 32 GB
with just a RAM of 8 GB as well,

381
00:26:04,775 --> 00:26:09,000
because it will take only say
4GB chunk in one go,

382
00:26:09,082 --> 00:26:12,437
process them, and then take the
next 4GB chunk,

383
00:26:12,462 --> 00:26:14,318
take the next 4GB chunk and so on.

384
00:26:14,342 --> 00:26:17,953
So, this will never give you
out of memory error.

385
00:26:17,977 --> 00:26:24,229
The second way, which is something
which I personally recommend as well

386
00:26:24,253 --> 00:26:31,407
if you're starting off from scratch is
to do a graph-based model training.

387
00:26:31,431 --> 00:26:33,292
Now, what a graph-based
model training is,

388
00:26:33,316 --> 00:26:39,448
is that you use something say like
Tensorflow, Pytorch, MXNET, Dask,

389
00:26:39,472 --> 00:26:45,769
some sort of a distributed library, okay?

390
00:26:45,793 --> 00:26:52,427
What these libraries do is they
build a Directed Acyclic Graph

391
00:26:52,451 --> 00:26:54,171
off of your entire pipeline, right?

392
00:26:54,195 --> 00:26:59,556
So it would put all your preprocessing
functions, your model functions,

393
00:26:59,581 --> 00:27:02,306
your optimization functions,
your loss calculation functions,

394
00:27:02,330 --> 00:27:04,497
all in like one graph, okay?

395
00:27:04,521 --> 00:27:10,108
And it would help you
get that graph, optimize that graph

396
00:27:10,132 --> 00:27:14,962
for your entire architecture, for
your entire cluster architecture.

397
00:27:14,986 --> 00:27:17,300
And then run your training jobs.

398
00:27:17,324 --> 00:27:24,479
What this effectively does is it would
help you optimize all the nooks and crooks

399
00:27:24,503 --> 00:27:28,079
in your entire architecture
before it actually starts training

400
00:27:28,103 --> 00:27:31,589
and second, this way you can
run it on multiple different machines.

401
00:27:31,613 --> 00:27:35,237
So, you can effectively achieve
parallelism in some tasks as well, right?

402
00:27:35,261 --> 00:27:41,392
And that's where a lot of like
Tensorflow, Pytorch stuff helps, right?

403
00:27:41,416 --> 00:27:43,785
And that's what, if you
remember in the first slide,

404
00:27:43,809 --> 00:27:47,980
that's what we were discussing as well,
when we were running multiple--

405
00:27:48,004 --> 00:27:50,304
the same task on multiple worker nodes.

406
00:27:50,328 --> 00:27:51,931
It was quite easy. Why?

407
00:27:51,932 --> 00:27:59,207
Because it-- the overall code was written
as a directed acyclic graph.

408
00:27:59,232 --> 00:28:04,093
So it was already compiled, and
then like-- all that was happening

409
00:28:04,117 --> 00:28:06,759
was it was executing on
different, different GPUs

410
00:28:06,783 --> 00:28:08,839
and that's why we
were getting a speedup.

411
00:28:08,864 --> 00:28:13,865
But more on this later, and in fact,
I have a working session just discussing

412
00:28:13,889 --> 00:28:15,912
these in detail as well.

413
00:28:17,139 --> 00:28:22,391
Now-- now comes the
million-dollar question, right?

414
00:28:22,415 --> 00:28:25,971
How do we build these
scalable ML workflows, given

415
00:28:25,995 --> 00:28:27,565
whatever knowledge we have?

416
00:28:27,589 --> 00:28:32,283
So, I would say there's
something for everyone.

417
00:28:32,307 --> 00:28:36,345
Let's talk about, like in
this particular area, right?

418
00:28:36,345 --> 00:28:39,238
There are generally
just two scenarios, right?

419
00:28:39,262 --> 00:28:41,849
First scenario is wherein
you as a data scientist

420
00:28:41,874 --> 00:28:46,220
or as a machine learning
engineer, you have written code

421
00:28:46,245 --> 00:28:51,799
in say scikit-learn, Statsmodels
XGBoost, and the likes of it

422
00:28:51,823 --> 00:28:58,251
and you're very well aware about building
machine learning workflows on that.

423
00:28:58,275 --> 00:29:02,588
That will be scenario one.
Scenario two is--

424
00:29:02,612 --> 00:29:06,403
and again, assuming that you
already have a pipeline written in

425
00:29:06,427 --> 00:29:08,033
you know, all of this.

426
00:29:08,058 --> 00:29:11,932
Scenario two is when you're
starting an experiment from scratch.

427
00:29:11,957 --> 00:29:14,954
When you're building a machine
learning model from scratch.

428
00:29:14,978 --> 00:29:19,212
In those scenarios, you're free
to choose a particular framework.

429
00:29:19,236 --> 00:29:22,175
You're free to choose say--
Tensorflow, you're free to choose,

430
00:29:22,199 --> 00:29:23,948
you know, something like that.

431
00:29:23,972 --> 00:29:29,563
What you would typically do in both
of these scenarios is something like this.

432
00:29:29,587 --> 00:29:31,603
Wherein you already have, say
for example--

433
00:29:31,603 --> 00:29:36,734
if you already have an entire pipeline,
like in an existing setup

434
00:29:36,734 --> 00:29:39,707
if you already have a machine learning
pipeline written in scikit-learn

435
00:29:41,224 --> 00:29:47,876
or say in XGBoost or some
sort of a library like that,

436
00:29:47,900 --> 00:29:50,495
then you wouldn't want to like--
recreate that library, right?

437
00:29:50,519 --> 00:29:53,792
You would want to make sure
your entire code is maintainable

438
00:29:53,816 --> 00:29:56,083
and you want to make
minimal changes to it.

439
00:29:56,108 --> 00:29:57,566
In those particular scenarios,

440
00:29:57,566 --> 00:30:00,648
what you could do is something
which is put on the left,

441
00:30:00,648 --> 00:30:04,910
is, you can use Dask
for creating batches of your data

442
00:30:05,096 --> 00:30:08,315
and then building
like incremental pipelines,

443
00:30:08,315 --> 00:30:11,176
either using Dask ML
or using scikit-learn.

444
00:30:11,200 --> 00:30:14,331
Both of them are pretty interoperable

445
00:30:14,331 --> 00:30:18,746
and you can use them
to build your distributed pipeline.

446
00:30:18,770 --> 00:30:24,404
And this way you don't have to make a
lot of changes into your existing workflow

447
00:30:24,428 --> 00:30:30,119
and you would also get some
sort of benefits off of distribution

448
00:30:30,144 --> 00:30:32,742
and out of code ML into your pipeline.

449
00:30:32,766 --> 00:30:37,424
Second is a scenario wherein
you are starting from scratch

450
00:30:37,448 --> 00:30:42,088
and you're like, you know, building
an experiment from the get-go.

451
00:30:42,112 --> 00:30:45,947
In those scenarios, I generally
recommend going with the-- you know,

452
00:30:45,971 --> 00:30:49,893
either Tensorflow or Pytorch or
MXNet, one of these libraries wherein

453
00:30:49,917 --> 00:30:56,829
you can effectively create an end-to-end
architecture and the library itself

454
00:30:56,853 --> 00:31:00,553
would create a dag for you and then you
can, you know, use different, different

455
00:31:00,578 --> 00:31:04,322
distribution strategies across your
cluster to be able to build those models.

456
00:31:04,346 --> 00:31:07,017
And those are, again,
like very easy to maintain.

457
00:31:07,041 --> 00:31:10,433
The only thing is it will take some
bit of time for you

458
00:31:10,433 --> 00:31:12,753
if you're new to it
to get a hang of it, right?

459
00:31:12,777 --> 00:31:15,330
And again, now, like with Tensorflow 2.2,

460
00:31:15,330 --> 00:31:21,522
there are like multiple new APIs
that have been introduced.

461
00:31:21,546 --> 00:31:27,180
Like, for example, df.data,
df.distribution strategy, df.--

462
00:31:27,204 --> 00:31:34,899
it supports both either execution and also
like the lazy load execution, wherein you

463
00:31:34,923 --> 00:31:38,182
create the entire graph and then
run it and, you know,

464
00:31:38,182 --> 00:31:42,073
that's I think the way to go
if you have new experiments to run.

465
00:31:42,097 --> 00:31:45,372
Now, enough talk. Let's code.

466
00:31:45,396 --> 00:31:50,897
So now that we've gone through
such a detailed demo about

467
00:31:50,921 --> 00:31:56,221
both Tensorflow as well as Dask, you
may have a question about which one,

468
00:31:56,246 --> 00:32:02,650
which framework, or which flow should you
use for your next machine learning project

469
00:32:02,675 --> 00:32:10,019
or for, you know, taking
on or sort of maintaining

470
00:32:10,043 --> 00:32:13,369
your existing machine learning
workflow or something like that.

471
00:32:13,393 --> 00:32:20,184
Truth be told, this is one quote by
Thomas Sowell, which I follow to the code,

472
00:32:20,208 --> 00:32:24,939
which is that, “There are no solutions,
there are only tradeoffs.”

473
00:32:24,963 --> 00:32:28,863
For example, if you were to
build something with Dask,

474
00:32:28,887 --> 00:32:33,545
then you're stuck with possibly
less parallelizable code, right?

475
00:32:33,569 --> 00:32:36,375
Whereas if you were to build
something with Tensorflow,

476
00:32:36,399 --> 00:32:40,018
then you're stuck with
a very huge learning curve

477
00:32:40,042 --> 00:32:45,000
in terms of learning Tensorflow,
its numerous APIs, not to mention

478
00:32:45,025 --> 00:32:50,813
different sublibraries
for image, for text, for audio

479
00:32:50,837 --> 00:32:54,709
and all those other that it comes with.

480
00:32:54,764 --> 00:33:01,086
Also, keeping in mind that the overall
Tensorflow, Pytorch, Dask, all of this

481
00:33:01,110 --> 00:33:04,959
this entire family is something
which is rapidly evolving as well, right?

482
00:33:05,014 --> 00:33:07,776
So, you have to keep
yourself up to date on that.

483
00:33:07,800 --> 00:33:14,772
However, one, quick thing that I
would say is, and which we have covered

484
00:33:14,797 --> 00:33:17,650
before as well-- remember
the two scenarios?

485
00:33:17,674 --> 00:33:20,518
And there's something
which I always put out is

486
00:33:20,542 --> 00:33:22,956
if you already have a machine
learning workflow,

487
00:33:22,956 --> 00:33:25,493
which is written say in scikit-Learn
or say in Dask,

488
00:33:25,493 --> 00:33:30,944
and you want to extend it to work on
possibly larger set of data,

489
00:33:30,968 --> 00:33:34,947
then you should possibly
look into using Dask ML.

490
00:33:34,971 --> 00:33:39,996
Because you don't have to
rewrite from scratch.

491
00:33:40,020 --> 00:33:44,359
You can use the existing pipeline,
albeit make some changes here and there,

492
00:33:44,383 --> 00:33:45,831
and you're good to go.

493
00:33:45,831 --> 00:33:50,009
Your pipeline works on
a much larger chunk of data then.

494
00:33:50,010 --> 00:33:54,670
However, if you were to build something
from scratch and you know for a fact

495
00:33:54,695 --> 00:34:00,395
that you'll be using-- big data, again,
relative to whatever cluster

496
00:34:00,395 --> 00:34:03,213
or whatever machine
you're running your data in,

497
00:34:03,213 --> 00:34:06,787
you may want to use Tensorflow from
the scratch or you may want to use

498
00:34:06,811 --> 00:34:08,531
say-- Pytorch from the scratch.

499
00:34:08,556 --> 00:34:11,680
I prefer Tensorflow to be honest--

500
00:34:11,680 --> 00:34:20,027
but that helps you big scalability right
from the scratch into your project.

501
00:34:20,051 --> 00:34:27,802
So, that's what my recommendation
would be, and again, do go through

502
00:34:27,826 --> 00:34:35,093
the links to the Colab notebooks
that I just showed a while back.

503
00:34:35,117 --> 00:34:40,556
You can easily retrofit those
to work with your data sets

504
00:34:40,580 --> 00:34:42,642
and try them out for yourself as well.

505
00:34:43,875 --> 00:34:47,060
With that, thank you so
much for attending my talk.

506
00:34:47,084 --> 00:34:55,002
I hope I added some value
to your knowledge base

507
00:34:55,033 --> 00:34:57,901
and you learned something out of it.

508
00:34:57,925 --> 00:35:04,180
Again, you can reach out to me for
any questions at either @reach_vb.

509
00:35:04,204 --> 00:35:07,477
You can find my previous
talks at vaibhav.blog,

510
00:35:07,501 --> 00:35:10,524
and I look forward to hearing from you.

511
00:35:10,548 --> 00:35:12,176
Thank you so much.



