1
00:00:11,020 --> 00:00:13,160
안녕하십니까? 오성우입니다

2
00:00:14,235 --> 00:00:16,914
이렇게 또 2020년에 파이콘을 통해서

3
00:00:16,939 --> 00:00:22,270
뭔가 오픈소스 커뮤니티에
공헌할 수 있게 돼서

4
00:00:22,420 --> 00:00:23,220
기회가 생겨서 되게 좋구요

5
00:00:26,440 --> 00:00:30,780
코로나로 인해서
현장에서 직접 소통하면서

6
00:00:30,780 --> 00:00:33,744
이야기를 전달하지 못한 점도
좀 아쉬웠네요

7
00:00:35,290 --> 00:00:40,055
오늘 그 진행하는 내용은

8
00:00:40,080 --> 00:00:43,580
최근에 KB국민은행에서 공개했다고 하는

9
00:00:43,580 --> 00:00:46,673
KB-ALBERT에 대해서
좀 얘기를 하려고 하고요

10
00:00:48,120 --> 00:00:51,260
개발을 하게 되면서 많이 사용하게 됐던

11
00:00:51,260 --> 00:00:55,003
Hugging Face라는 곳의
Transformers 라이브러리가 있는데

12
00:00:55,390 --> 00:00:58,858
이거를 좀 집중적으로
소개를 하게 될 거 같습니다

13
00:01:00,070 --> 00:01:02,874
발표자인 저는 오성우이고요

14
00:01:03,610 --> 00:01:06,445
저는 데이터를 되게 좋아해서

15
00:01:06,580 --> 00:01:10,540
다양한 산업군에 있는
데이터를 좀 많이 찾아다니면서

16
00:01:10,565 --> 00:01:15,616
분석이나 머신러닝 모델이나
딥러닝 모델들을 개발을 했었고요

17
00:01:15,812 --> 00:01:20,319
최근에는 이제 자연어 처리 쪽에
집중을 하고 있는 상황입니다

18
00:01:20,890 --> 00:01:27,249
일을 하다 보면 데이터나 알고리즘
이런 것들도 되게 좀 중요했는데

19
00:01:27,940 --> 00:01:30,915
비즈니스 문제 해결이
요새는 더 중요한 게 아닌가

20
00:01:30,940 --> 00:01:32,838
어떻게 하면 비즈니스 문제 해결하고

21
00:01:33,370 --> 00:01:37,620
프로세스 개선이나
ROI에 도움이 될 수 있는 것들

22
00:01:37,620 --> 00:01:41,400
방안을 좀 많이 생각하는 있는 요즘이고요

23
00:01:41,590 --> 00:01:46,035
그리고 또 이제 공유의 힘을 믿어서
공유를 많이 하고

24
00:01:46,600 --> 00:01:52,576
오픈을 자꾸 하다 보면은
더 나은 사회가 되지 않을까?

25
00:01:53,435 --> 00:01:57,545
가능하면 조금이라도
또 공유를 하려고 합니다

26
00:01:58,480 --> 00:02:02,681
최근에는 이제 말씀드린 대로
NLP 쪽에 좀 집중을 하고 있고요

27
00:02:02,706 --> 00:02:08,002
그 중에서도 금융 쪽과 관련된
Domain Adaptation, 그리고

28
00:02:09,670 --> 00:02:12,740
최근의 언어 모델 같은 걸
사용하게 한 Pruning 기법이나

29
00:02:12,740 --> 00:02:18,885
Input adaptive inference 같은
분야에 조금 더 관심을 갖고 있고요

30
00:02:18,910 --> 00:02:24,226
그리고 그 외에도 원래 하고 있었던
정형 데이터에 대한 AutoML 기법들

31
00:02:24,250 --> 00:02:28,460
그리고 Topological Data Analysis
같은 것들도

32
00:02:28,460 --> 00:02:32,400
과제를 하면서
관심을 갖고 있는 부분입니다

33
00:02:32,440 --> 00:02:37,916
네 이 발표를 통해서
제가 기여를 하고 싶은 부분은

34
00:02:39,100 --> 00:02:41,656
이제 NLP의 발전을 위해서

35
00:02:41,680 --> 00:02:45,672
그리고 그중에서도
금융과 한국어 NLP 발전을 위해서

36
00:02:45,705 --> 00:02:47,805
제가 조금이라도 좀 기여를 하고자

37
00:02:47,830 --> 00:02:54,236
제가 실제로 과제를 하거나 할 때도
오픈소스를 많이 사용하다 보니까

38
00:02:54,509 --> 00:02:59,596
오픈소스에 대한 고마움
그리고 사람들이 잘 모르는

39
00:02:59,620 --> 00:03:05,159
제가 사용하는 팁들 같은 것들을 함께
공유하고자 이 자리를 마련했습니다

40
00:03:07,180 --> 00:03:11,664
이 발표를 통해서
도움이 되었으면 하는 분들은

41
00:03:12,100 --> 00:03:18,440
최근 그냥 뉴스나
기타 매거진 등을 통해서

42
00:03:18,440 --> 00:03:22,780
은행이 공개했던 KB-ALBERT가
뭔지 궁금해하시는 분들이 많아서

43
00:03:22,810 --> 00:03:26,427
그런 것들에 대한 궁금증이
해소가 될 수 있었으면 하고요

44
00:03:26,837 --> 00:03:32,000
그리고 두 번째로는
최신 NLP 기술들을 어떻게 하면 좀

45
00:03:32,010 --> 00:03:37,665
코딩을 많이 하지 않고도 손쉽게
사용해보고 싶다라고 하시는 분들에게

46
00:03:37,690 --> 00:03:40,438
그 출발점이 됐으면 좋겠고요
그리고

47
00:03:40,871 --> 00:03:44,480
마지막으로는 금융 텍스트
같은 것들을 갖고 계시다면

48
00:03:44,920 --> 00:03:47,700
핸즈온으로 직접
분석을 해볼 수도 있는

49
00:03:47,700 --> 00:03:50,021
그 출발점이 되었으면 하는 마음입니다

50
00:03:50,650 --> 00:03:55,048
발표는 크게 세 가지를 주제로
얘기를 하게 될 것 같은데요

51
00:03:55,660 --> 00:03:59,831
먼저 KB-ALBERT 금융 도메인에
특화됐다는 언어 모델인

52
00:04:01,390 --> 00:04:04,200
KB-ALBERT에 대해
소개하게 될 것 같고요

53
00:04:04,225 --> 00:04:11,553
그리고 개발을 하고
실제 NLP 쪽을 하면서

54
00:04:11,678 --> 00:04:15,990
Hugging Face라는 회사의
Transformers 라이브러리를

55
00:04:16,029 --> 00:04:18,303
많이 사용을 하고
정말 많은 도움이 되고 있는데요

56
00:04:18,466 --> 00:04:22,366
아는 분들은 다 알고
이미 사용을 하고 계시는데

57
00:04:22,390 --> 00:04:27,600
이제 막 시작하시는 분들에게는
익숙하지 않을 수 있습니다

58
00:04:28,000 --> 00:04:31,007
그래서 이 부분에 대해서
소개를 해드리려고 하고요

59
00:04:31,101 --> 00:04:36,663
마지막으로는 실제 이제
그 공개된 언어모델을 가지고

60
00:04:36,688 --> 00:04:43,430
Transformers로 Fine-tuning 해 보는
방법들에 대해서 나눠보려고 합니다

61
00:04:44,560 --> 00:04:49,724
최근 GPT3라는 언어 모델이

62
00:04:49,802 --> 00:04:54,556
사람처럼 직접 뉴스 기사를 작성하고
가짜 뉴스도 만들어내면서

63
00:04:54,580 --> 00:05:00,181
사람들이 아 이게 인공지능이 한 거야
라고 물어볼 정도로 되게 수준 높은

64
00:05:00,640 --> 00:05:05,265
성능을 보여주고 있으면서
사람들을 놀라게 하고 있는데요

65
00:05:05,672 --> 00:05:08,356
GPT3라는 걸 검색하다 보면

66
00:05:08,380 --> 00:05:14,208
BERT나 Transfer Learning, Transformers,
Pretrained Language Model,

67
00:05:14,233 --> 00:05:19,231
Fine-tuning과 같은
생소한 용어들이 많이 나오게 됩니다

68
00:05:19,720 --> 00:05:26,830
그래서 이런 것들이 어떤 건지
먼저 짚어보고 진행을 할 예정입니다

69
00:05:27,400 --> 00:05:32,340
먼저 금융 도메인에 특화된 언어 모델
KB-ALBERT에 대해서 조금 소개를

70
00:05:32,340 --> 00:05:34,025
같이 하려고 하는데요

71
00:05:34,480 --> 00:05:37,425
언어 모델이라는 게 뭘까요?

72
00:05:37,480 --> 00:05:43,019
저도 처음에 언어 모델이 뭐냐고
물어보게 되면

73
00:05:44,080 --> 00:05:48,166
모델이긴 모델인데 언어적인
특징이 있는 모델입니다

74
00:05:48,190 --> 00:05:53,072
이렇게 얘기를 하게 되는데
그 특징이 뭐냐 하고 물어보다 보면

75
00:05:54,130 --> 00:05:56,140
알겠다고 하면서 끝나긴 하는데

76
00:05:56,140 --> 00:05:58,878
제대로 설명이 안 됐다는
느낌이 많이 들더라고요

77
00:05:58,903 --> 00:06:03,192
그래서 먼저 언어 모델이 뭔지
이야기를 해보고자 합니다

78
00:06:03,625 --> 00:06:06,196
위키피디아에 언어 모델을 검색해보면

79
00:06:06,220 --> 00:06:12,706
“A language model is a probability distribution
over sequences of words.”라고 하는데

80
00:06:12,730 --> 00:06:16,080
일련의 단어의
어떤 확률적 분포라고 하는데

81
00:06:16,080 --> 00:06:18,425
이게 사실 딱 와닿지는 않습니다

82
00:06:18,880 --> 00:06:23,926
그리고 NLP 관련된 블로그나
사이트로 가보면

83
00:06:23,950 --> 00:06:28,140
언어 모델을 언어적 특징을
이해하고 처리할 수 있도록

84
00:06:28,140 --> 00:06:31,168
범용적인 목적으로
학습된 모델이라고 이야기를 하고

85
00:06:31,772 --> 00:06:34,180
학습된 언어 모델을 활용해서

86
00:06:34,180 --> 00:06:37,614
질문 답변 문제나
문서 분류 감성 분석과 같은

87
00:06:37,639 --> 00:06:40,889
세부적인 목적의
자연어 처리 문제에 사용한다

88
00:06:40,914 --> 00:06:44,437
와 같은 아리송한 내용들이 좀 나옵니다

89
00:06:46,660 --> 00:06:51,472
사실 이 언어 모델이라는 게
인공지능 기반의 언어모델은

90
00:06:51,527 --> 00:06:54,620
사람이 학습하는 방식이랑
굉장히 유사합니다

91
00:06:54,645 --> 00:07:00,565
그래서 좀 더 캐주얼하게 방식을
어떻게 설명할지 고민을 하다가

92
00:07:01,030 --> 00:07:02,606
이렇게 설명을 하게 됩니다

93
00:07:02,631 --> 00:07:07,995
우리가 영어라는 언어를 공부를
하게 되는 상황을 생각해볼 수 있는데요

94
00:07:08,020 --> 00:07:10,700
일반적으로 우리가 이제
영어 공부를 하게 되면

95
00:07:10,700 --> 00:07:14,026
영작을 하거나 빈칸 채우기
같은 거를 하면서

96
00:07:14,051 --> 00:07:18,960
계속 문제를 풀어보고
글을 써보고 읽어보고 하게 되는데

97
00:07:19,015 --> 00:07:22,696
이런 것들을 통해서 우리가 기본적인
영어 실력을 계속 향상시켜나가잖아요

98
00:07:22,720 --> 00:07:25,921
그래서 다음과 같이 빈칸 문제가
주어졌다고 합시다

99
00:07:25,946 --> 00:07:31,696
쭉 읽어보면 이제 삼성의 어떤
좀 잘 나가는 텔레비전 모델이

100
00:07:31,720 --> 00:07:37,579
어떠한 건지 어떤 스크린을 가졌는지에
대한 문제인데 이 정답은 large 이죠

101
00:07:38,532 --> 00:07:42,625
좀 더 큰 시각적인 경험을
사람들이 선호하기 때문에

102
00:07:43,060 --> 00:07:50,145
삼성의 어떤 잘 나가는 텔레비전 모델은
큰 스크린 그래서 정답은 large (3)이고

103
00:07:50,830 --> 00:07:56,837
이렇게 빈칸에 들어가는 걸
우리는 계속 이런식으로 공부를 하잖아요

104
00:07:56,876 --> 00:08:02,540
이게 이제 실제 인공지능 언어 모델이
학습하는 방식이랑 되게 유사합니다

105
00:08:02,710 --> 00:08:07,636
사람의 영작을 하고 빈칸이 올 단어를
계속 이렇게 추론해가면서 퀴즈도 풀면서

106
00:08:07,678 --> 00:08:14,092
아 이게 어휘가 어떤 건지 그 어휘가
문장 안에서 어떠한 위치에 오는게 좋은지

107
00:08:14,117 --> 00:08:19,396
이런 것들을 영어의 어휘나 문법들을
학습하는 방법으로를 많이 사용을 하잖아요

108
00:08:19,420 --> 00:08:22,500
그런데 그거랑 거의 비슷하게
인공지능 언어 모델도

109
00:08:22,500 --> 00:08:24,418
사람처럼 유사하게 학습을 합니다

110
00:08:24,443 --> 00:08:30,144
수십억 개의 수백억 개의 문장으로부터
빈칸에 어떤 변화가 올지 예측을 하면서

111
00:08:30,169 --> 00:08:33,466
언어가 가지고 있는 어휘가
어떤 게 있고 구조적으로 어떻게 했는지

112
00:08:33,490 --> 00:08:36,544
그런 특징들을
피쳐를 학습하는 방식이고

113
00:08:36,569 --> 00:08:42,186
이렇게 그 언어 모델이 학습하는 방식을
Pre-training이라고도

114
00:08:42,211 --> 00:08:44,031
보통 많이 얘기를 하게 됩니다

115
00:08:44,275 --> 00:08:49,396
그래서 Pre-trained Language Model
약어로는 PLM 같은 것들이

116
00:08:49,420 --> 00:08:52,580
언어 모델에 언어 자체를 학습하는

117
00:08:52,580 --> 00:08:57,302
그런 사전학습하는 개념으로써
많이 사용되게 됩니다

118
00:08:57,490 --> 00:09:00,600
그래서 우리가 문제를 그 정답을 찾는데

119
00:09:00,610 --> 00:09:04,156
그 정답을 찾는 과정들을
생각해 볼 필요가 있습니다

120
00:09:04,180 --> 00:09:10,469
우리가 문제를 풀면 문장에서
그 빈칸에 어떤 단어가 올지

121
00:09:10,494 --> 00:09:14,430
우리가 풀이할 때 그 주변에 올 수 있는
단어들을 생각하잖아요

122
00:09:14,877 --> 00:09:18,056
실제로 풀이집 같은 거 보면은

123
00:09:18,081 --> 00:09:25,133
중요하다고 생각되는 단어나 힌트 부분에
밑줄을 긋고 별표도 치는 것처럼

124
00:09:25,158 --> 00:09:28,740
우리는 이제 문제를 풀었을 때
주변의 단어들을 통해서

125
00:09:28,740 --> 00:09:32,476
빈칸에 올 수 있는
정답 단어를 추론을 하게 되죠

126
00:09:32,501 --> 00:09:38,040
인공지능 언어 모델도 이렇게 사람과
비슷한 방식을 사용하게 됩니다

127
00:09:38,680 --> 00:09:43,396
빈칸에 오는 단어를 예측하기 위해서
주변 단어들을 보고

128
00:09:43,420 --> 00:09:50,826
주변 단어들 중에서 빈칸을 예측하는데
중요한 단어들을 집중을 하게 됩니다

129
00:09:51,310 --> 00:09:55,300
그래서 아래 그림에 보면은
좀 더 집중을 한다고

130
00:09:55,300 --> 00:09:59,427
생각되는 부분에
좀 더 강조표시가 되어 있는데요

131
00:09:59,950 --> 00:10:05,114
이렇게 집중하는 것에 대해선 최근에는
Attention Mechanism이다 이라고 해서

132
00:10:05,260 --> 00:10:09,220
인공지능도
Attention Mechanism을 통해서

133
00:10:09,245 --> 00:10:15,854
좀 더 빈칸에 올 수 있는 부분을 더 잘
예측할 수 있다 이런 컨셉이 있습니다

134
00:10:16,300 --> 00:10:20,534
이러한 그 어텐션 메카니즘
집중을 하는 메카니즘을 사용하는

135
00:10:21,730 --> 00:10:24,815
아키텍처가 Transformer 아키텍처이구요

136
00:10:25,150 --> 00:10:30,526
최근엔 2, 3년 동안
이 Transformer 아키텍처를 이용한

137
00:10:30,550 --> 00:10:35,384
인공지능의 언어 모델 학습이
지금 주류를 이루고 있는 상황입니다

138
00:10:36,340 --> 00:10:40,574
보통 트랜스포머라고 하면
이제 그 오토봇

139
00:10:40,599 --> 00:10:44,004
변신하는 로봇을 상상하게 됐는데

140
00:10:44,715 --> 00:10:48,683
이제 저 같은 경우는 검색을 하게 되면
맨날 이런 걸 찾다 보니까

141
00:10:50,800 --> 00:10:54,753
인공지능 모델들이
검색 결과로 나오게 됩니다

142
00:10:55,592 --> 00:10:59,460
관심이 좀 더 있으신 분들은
"Attention Is All You Need"라는

143
00:10:59,460 --> 00:11:02,779
17년도 논문을 보시면
도움이 될 것 같습니다

144
00:11:03,175 --> 00:11:09,449
요즘 프로젝트를 하다 보면 언어 모델에
대한 지식이 있으신 분은 되게 많습니다

145
00:11:09,474 --> 00:11:15,472
근데 많이들 물어보시는 게
Word2Vec이랑 차이가 뭐냐

146
00:11:15,636 --> 00:11:21,016
Word2Vec도 단어를 벡터와 같은
확률분포 형태로 표현을 하는데

147
00:11:24,520 --> 00:11:28,442
Word2Vec이랑 다른 게 뭐냐 라는
이야기를 많이 듣곤 합니다

148
00:11:30,228 --> 00:11:35,716
가장 큰 차이점은 최근 BERT와 같이
이런 Transformer 아키텍처를

149
00:11:35,740 --> 00:11:37,846
사용하는 언어 모델들은

150
00:11:37,870 --> 00:11:43,726
이제 좀 더 문맥, 맥락에 따른 단어의

151
00:11:43,750 --> 00:11:47,236
표현 방법이 달라진다는 큰 차이가 있습니다

152
00:11:47,260 --> 00:11:51,000
Word2Vec 같은 경우는 주변 단어를 통해서

153
00:11:51,010 --> 00:11:55,944
중심에 있는 단어 또는 주변에 있는
단어들을 예측하는 방식을 통해서

154
00:11:55,990 --> 00:12:03,310
해당 단어에 대한 표현, 벡터를
학습하게 되는데 확률을 학습하게 되는데

155
00:12:04,840 --> 00:12:10,846
이게 한번 학습을 한 이후에는
스태틱한 형태로 그냥 고정된

156
00:12:10,870 --> 00:12:14,656
그 컨텍스트 상관없이
계속 그 값을 사용하게 됐는데

157
00:12:14,680 --> 00:12:20,416
이제 BERT나
그 계열들은 주변의 단어나

158
00:12:20,440 --> 00:12:22,456
문맥이 바뀌게 되면은

159
00:12:22,480 --> 00:12:26,656
같은 단어라고 하더라도
다른 형태로 표현하게 됩니다

160
00:12:26,680 --> 00:12:31,500
조금만 더 예시를 들어보면
왼쪽은 Context-Independent

161
00:12:31,500 --> 00:12:35,100
Word2Vec 같은 걸 말하는데

162
00:12:35,170 --> 00:12:39,365
배나 사과 같은 것들이
고정된 값으로 표현이 되는데

163
00:12:40,330 --> 00:12:48,298
최근의 언어 모델들은 배라는 단어가
먹는 과일이 될 수도 있지만

164
00:12:48,340 --> 00:12:54,496
배를 타고 떠난다 같이 이동을 위한

165
00:12:54,520 --> 00:12:57,104
모빌리티로서
표현이 될 수도 있잖아요

166
00:12:57,129 --> 00:13:01,546
그래서 같은 단어라고 하더라도
주변에 있는 단어

167
00:13:01,570 --> 00:13:09,790
그 맥락 등을 참고해서 표현되는 방식입니다

168
00:13:09,850 --> 00:13:15,037
이후에 이런 Contextual Embedded Vector에
대한 얘기가 좀 더 나올 예정입니다

169
00:13:15,357 --> 00:13:22,263
그럼 언어 모델에 대해서는
좀 이해가 되었다고 생각되는데요

170
00:13:22,750 --> 00:13:27,185
KB-ALBERT라는 게 뭔가라는
질문도 많이 왔었습니다

171
00:13:27,210 --> 00:13:30,967
ALBERT는 뭔가요라고
물어보시는 분들도 있었는데

172
00:13:31,600 --> 00:13:34,912
ALBERT는 A Lite BERT의 약자입니다

173
00:13:35,770 --> 00:13:41,707
BERT 이후로는 사실 BERT의
어떤 변형된 형태 개선된 형태의

174
00:13:42,610 --> 00:13:45,024
Transformer 아키텍처들이
나오게 되는데요

175
00:13:45,280 --> 00:13:50,740
그중에서 ALBERT는
이제 BERT의 경량화된 모델입니다

176
00:13:51,280 --> 00:13:54,676
2019년 9월에
구글 리서치에서 공개를 했고요

177
00:13:54,701 --> 00:13:58,943
2018년에 BERT가 공개된 이후
1년 만이죠

178
00:14:00,430 --> 00:14:03,672
Cross-layer parameter sharing이라는
방법을 통해서

179
00:14:03,976 --> 00:14:07,421
모델 파라미터 수가
감소가 되는 모델입니다

180
00:14:07,446 --> 00:14:13,456
그래서 기존 모델 사이즈가
수백 메가바이트 이상인데

181
00:14:13,480 --> 00:14:17,855
ALBERT의 경우에는
10배에서 20배 가까이 줄어드는

182
00:14:18,580 --> 00:14:20,603
좀 경량화된 형태의 모델입니다

183
00:14:21,040 --> 00:14:24,740
또 동일하게 Transformer Block을
사용을 하게 되는데

184
00:14:24,740 --> 00:14:27,856
기존 BERT처럼 이 Transformer Block을

185
00:14:27,880 --> 00:14:31,060
계속 중첩해나가고
쌓아가는 형태가 아니라

186
00:14:31,060 --> 00:14:36,036
동일한 Transformer 블록을
순환하는 형태로 사용을 하기 때문에

187
00:14:36,544 --> 00:14:42,526
앞에서 얘기했던 언어 모델 학습을
생각해보면 하나의 뇌를 하나의 사람이

188
00:14:42,550 --> 00:14:45,760
계속 집중을 여러 번 반복을 하는 걸
생각해보실 수 있습니다

189
00:14:46,003 --> 00:14:50,315
자 문제를 풀이할 때 한 번만 생각해보고
한 번만 집중해보는 게 아니라

190
00:14:50,596 --> 00:14:54,635
계속 반복적으로 여러 번 집중을 하면서

191
00:14:54,850 --> 00:14:59,600
문제를 풀이하는 모델이라고
생각을 하시면 좋을 것 같습니다

192
00:14:59,920 --> 00:15:07,927
문제는 이 언어 모델이 일반 사람들은
학습해서 사용하기 어렵다는 문제가 있습니다

193
00:15:08,590 --> 00:15:10,906
특히 크게 세 가지로
이제 얘기할 수 있는데요

194
00:15:10,930 --> 00:15:15,140
하나는 대용량 학습 데이터 수집과
정제가 필요합니다

195
00:15:15,850 --> 00:15:19,771
구글이 공식적으로 공개한 논문에서도

196
00:15:19,796 --> 00:15:23,656
수십 기가의 데이터를 사용을 하고요

197
00:15:23,680 --> 00:15:26,820
그 실무에서 실제 사용하고자 한다면

198
00:15:26,820 --> 00:15:31,555
수십 기가 수백 기가에 가까운
텍스트 데이터가 최소한 필요하게 됩니다

199
00:15:31,844 --> 00:15:36,286
이런 큰 텍스트 데이터를
개인 단위로 수집하기도

200
00:15:36,310 --> 00:15:38,388
좀 사실 많이 어렵긴 합니다

201
00:15:40,030 --> 00:15:43,246
그리고 이제 이런 ALBERT라는
알고리즘이 나왔을 때

202
00:15:43,270 --> 00:15:48,074
이 알고리즘을 구현하기 위한 소스코딩
프로그램이 필요하게 되고

203
00:15:48,099 --> 00:15:50,926
학습하기 위한 어떤 세팅이 필요한데

204
00:15:50,950 --> 00:15:57,450
그런 것들도 사실 좀 버든이 되고요
그리고 가장 큰 문제가 아닌가 싶습니다

205
00:15:57,535 --> 00:16:00,256
학습을 위한 자원이 필요하게 되는데

206
00:16:00,280 --> 00:16:03,178
이렇게 큰 언어 모델을 학습을 하려면

207
00:16:03,490 --> 00:16:10,990
하루에 백만원 이만원 수백 만원의 비용이
들어가는 컴퓨터를 사용하게 되는데

208
00:16:10,990 --> 00:16:14,356
이전의 Word2Vec 같은
언어 모델들에 비해서

209
00:16:14,380 --> 00:16:19,786
개인 단위로 학습해서 사용하기가 상당히
어려운 측면이 없지 않아 있게 됩니다

210
00:16:26,209 --> 00:16:32,396
하지만 다행히 이제 구글이나
페이스북 같은 해외 아이티의 기업들이

211
00:16:32,421 --> 00:16:37,996
자신들의 이런 연구 성과나
결과물들 그리고 학습했던 모델들을

212
00:16:38,020 --> 00:16:40,121
오피셜하게 제공을 하고 있습니다

213
00:16:40,570 --> 00:16:42,466
GPT 같은 것도 마찬가지고요

214
00:16:42,490 --> 00:16:45,794
그리고 한국어에 대한
일반적인 언어 모델들도

215
00:16:45,888 --> 00:16:51,586
ETRI의 KorBERT나
SKT의 KoBERT, KoGPT2 같은

216
00:16:51,611 --> 00:16:52,950
언어 모델들도 공개되고 있어서

217
00:16:52,951 --> 00:16:55,860
많은 사람들이 이런 것들을 받아서

218
00:16:55,860 --> 00:16:58,515
필요에 따라 사용을 하고 있는 걸로
저는 알고 있습니다

219
00:16:58,540 --> 00:17:00,700
저도 많은 도움을 받았구요

220
00:17:00,700 --> 00:17:08,206
하지만 이런 일반화 된 언어 모델들은
실제 프랙티걸하게 사용을 하시다 보면은

221
00:17:08,230 --> 00:17:12,500
성능저하가 좀 일어나게 되고
기대했던 수준만큼

222
00:17:12,500 --> 00:17:15,230
성능이 안 나오게 되는
상황이 발생하곤 하는데요

223
00:17:15,761 --> 00:17:19,605
이게 도메인의 특수성이
고려돼야 됐기 때문입니다

224
00:17:20,170 --> 00:17:26,920
일반적인 그 아래 그림의 오른쪽 부분에
일반적인 도메인에서 학습된 모델은

225
00:17:27,010 --> 00:17:28,460
이 모델을 가지고 사용하려고 하는

226
00:17:28,460 --> 00:17:32,291
테스크 도메인에서는
데이터의 분포가 달라지기 때문에

227
00:17:32,890 --> 00:17:35,896
물론 이제 성능은 어느 정도 보장되지만

228
00:17:35,920 --> 00:17:41,380
이제 그 분포에 따라 생길 수 있는
성능 저하 문제는 발생을 하게 됩니다

229
00:17:42,340 --> 00:17:47,308
예를 통해서 좀
생각을 해볼 수 있는데요

230
00:17:48,040 --> 00:17:54,976
실제로 좀 있었던 예문인데
지난 프로젝트에서 여신 담당자와 함께

231
00:17:55,000 --> 00:17:58,164
프로토타입 모형을 개발했다 라는
문장이 있다고 합시다

232
00:17:58,660 --> 00:18:04,666
일반적으로 저도 사실 이제 금융과 관련된
도메인에 대한 이해가 좀 부족할 때는

233
00:18:04,690 --> 00:18:09,520
이 얘기를 듣고는 무슨 얘기인지
어떤 사람이랑 프로토타입

234
00:18:09,545 --> 00:18:11,260
시제품을 만들었다는 건가

235
00:18:11,260 --> 00:18:15,226
이 문장 자체가
잘 이해가 안 됐었는데

236
00:18:15,250 --> 00:18:17,460
이것을 제 주변에 있는 동료들에게

237
00:18:17,460 --> 00:18:22,756
금융 쪽에 좀 더 스페셜티가 있는
 동료들에게 물어봤을 때

238
00:18:22,780 --> 00:18:25,060
여신이라는 게 이제 대출과 관련된 상품

239
00:18:25,060 --> 00:18:29,063
돈을 빌려주는 업무와
관련된 거라는 얘기를 듣고 나서

240
00:18:29,110 --> 00:18:34,320
아 이게 대출 관련 서비스를 만들었다는
얘기구나 라는 걸 이해를 하게 되었습니다

241
00:18:34,360 --> 00:18:39,581
그런데 이제 어디나 마찬가지지만
병원을 가든

242
00:18:39,606 --> 00:18:41,392
새로운 도메인을 가게 되면

243
00:18:41,417 --> 00:18:46,336
그 도메인을 사용하고 있는 용어나
어떤 대화 방식들이 많이 달라서

244
00:18:46,360 --> 00:18:52,570
좀 애를 먹는 것들을 다른 분들도 많이
느끼시지 않으실까 생각을 하게 되는데요

245
00:18:52,780 --> 00:18:56,584
우리가 영어를 공부를 해서
실력이 늘었다고 할지라도

246
00:18:56,615 --> 00:18:58,620
빈칸 추론을 하고 영작을 해서

247
00:18:58,620 --> 00:19:01,966
기본적인 어떤 영어에 대한 문법도
이해가 되고 어휘도 늘었다고 할지라도

248
00:19:04,029 --> 00:19:09,645
만약에 전공서적 영문으로 된
경제나 금융 관련된 전문서적을 보게 되면

249
00:19:09,670 --> 00:19:15,123
또한 이해하기 되게 어렵고 공부하기
힘들었던 경험들이 있으실 텐데요

250
00:19:15,193 --> 00:19:19,083
이런 것처럼 도메인에 대한
특화된 언어 모델에 대한

251
00:19:19,108 --> 00:19:22,605
필요성이 좀 많이 필요하게 되고

252
00:19:22,630 --> 00:19:27,526
실제 그 과제를 진행하게 됐을 때도
이런 도메인에 대한

253
00:19:27,550 --> 00:19:31,745
특수성 때문에 성능 저하 같은 것도
발생하곤 했었습니다

254
00:19:32,890 --> 00:19:37,426
그래서 도메인에 특화된 언어 모델에 대한
필요성을 많이 느끼게 됐고요

255
00:19:37,457 --> 00:19:43,489
그래서 이제 금융도메인 특화가 뭘까 라는
질문을 스스로에게도 많이 던지곤 했습니다

256
00:19:44,110 --> 00:19:49,180
먼저 Domain Adaptation이라는
접근을 통해서

257
00:19:49,205 --> 00:19:51,976
이 문제를 해결을 시도하려고 했었고

258
00:19:52,000 --> 00:19:56,836
그러한 노력들이 KB-ALBERT에
녹아들어가 있는 상황입니다

259
00:19:56,860 --> 00:20:01,786
금융도메인 특화라고 하더라도 일반적인
언어 모델의 성능을 유지를 하면서도

260
00:20:01,810 --> 00:20:05,903
도메인에 특화된 특수성을 고려한
언어 모델 학습을 하고자 했고요

261
00:20:07,060 --> 00:20:13,156
Pre-training을 한 번에 크게 데이터를
다 넣어서 학습을 하는 방식보다는

262
00:20:13,180 --> 00:20:18,383
Pre-training의 페이즈 자체를 여러 단계로
나눠서 학습하는 전략을 좀 구성울 했었고

263
00:20:20,203 --> 00:20:23,776
도메인이라는 거를 이제

264
00:20:23,800 --> 00:20:30,471
일반 금융, 메디컬 이런 식으로
이제 크게 다 잡기도 했지만

265
00:20:30,850 --> 00:20:35,600
그 금융안에서도
은행이나 카드, 증권, 보험

266
00:20:36,000 --> 00:20:42,887
또 세부 금융 계열별 그리고 그 안에서도
업무별로 도메인이 많이 나눠지는데

267
00:20:42,912 --> 00:20:48,857
이러한 하위 도메인을 고려한
Continual learning 방식을 고려를 했습니다

268
00:20:50,080 --> 00:20:53,500
아울러 이런 도메인이나 테스크에 대한 거는

269
00:20:53,500 --> 00:20:57,369
사실 정의하는 방식에 따라
좀 많이 다르다고 하더라고요

270
00:20:57,790 --> 00:21:01,246
그런 것들은 저기에 좀 필요에 맞게
정리를 할 수 있을 거 같고요

271
00:21:01,270 --> 00:21:06,316
그래서 언어 모델을 개발을 하게 되면
이제 보케블러리나

272
00:21:06,340 --> 00:21:09,363
tokenizer 같은 것들을
개발을 하게 되는데

273
00:21:10,360 --> 00:21:15,031
개발했던 예시인데 이 사전에는

274
00:21:15,640 --> 00:21:19,296
실제 주요 금융기관 같은 것들이
많이 포함이 돼 있었고

275
00:21:20,020 --> 00:21:25,559
해외에 있는 금융기관 같은 것들도
좀 사전에 등록이 좀 되어 있었습니다

276
00:21:25,990 --> 00:21:31,247
그리고 증권 리포트나 이런 분석된
데이터들도 활용을 했기 때문에

277
00:21:31,810 --> 00:21:35,786
증권 관련된 주요 지수 같은 것들이
포함이 되어 있고요

278
00:21:36,340 --> 00:21:41,113
그리고 회사 내부적으로 피로를 하다 보니까

279
00:21:42,430 --> 00:21:46,672
그런 외부에 공개된 데이터들을 사용하면서도

280
00:21:48,250 --> 00:21:53,296
내부에 쓸 수 있도록
KB 관련된 자주 사용되는 단어들이

281
00:21:53,320 --> 00:21:55,515
사전에 포함되어 있는 상황입니다

282
00:21:55,750 --> 00:22:01,516
이 사전이 실제 Transformer 관련된 언어
모델를 만들 때에도 되게 많이 중요합니다

283
00:22:01,540 --> 00:22:06,360
사전을 어떻게 짜느냐가
성능에도 많이 이슈가 되었고요

284
00:22:06,910 --> 00:22:12,597
사전에 구축된 것에 따라서
tokenizing 하는 결과도 달라지게 되는데요

285
00:22:13,240 --> 00:22:17,599
방카슈랑스라는 단어는 은행에서 보험 관련

286
00:22:17,624 --> 00:22:21,740
상품 판매하기 위해 사용되는 판매상품인데

287
00:22:22,051 --> 00:22:26,005
일반적인 tokenizing 방식을 쓰면
그 가운데 있는 예시처럼

288
00:22:26,030 --> 00:22:31,066
방카슈랑스가 방,카,슈,랑스와 같이
끊어지게 됩니다

289
00:22:31,090 --> 00:22:33,205
랑스 같은 경우에는 프랑스나

290
00:22:33,230 --> 00:22:38,572
이런 국가에서도 나오는
subword이기 때문에

291
00:22:38,597 --> 00:22:40,156
이런 식으로 쪼개졌고

292
00:22:40,180 --> 00:22:45,736
저희의 이제 금융에 특화되게 학습을 시켜다보니

293
00:22:45,760 --> 00:22:50,776
이런 방카슈랑스 같은 것들도
하나의 온전한 키워드로 토큰으로서

294
00:22:50,800 --> 00:22:53,354
토크라이징 되었되었습니다

295
00:22:55,121 --> 00:23:00,511
비슷한 이제 금융 말뭉치를 보게 되면

296
00:23:01,000 --> 00:23:06,736
되게 약어들이 되게 많이 나옵니다 특히
영문으로 된 약어들이 많이 나오게 되는데

297
00:23:06,760 --> 00:23:11,017
이러한 약어들도 이제 tokenizing을
했을 때 온전하게 추출이 되었구요

298
00:23:11,852 --> 00:23:17,391
실제 사전과 tokenizer에 있어서
또 금융특화를 많이 고려를 했습니다

299
00:23:20,950 --> 00:23:25,606
그러면 이제 이거 가지고 뭘 할 수 있을 거냐
라는 질문을 많이 들었고요

300
00:23:26,020 --> 00:23:31,936
예를 들어 마켓 인텔리전스나 리스크 관리,
커스터머 서비스, 널리지 익스트랙션

301
00:23:31,960 --> 00:23:35,350
이런데 좀 많이 활용을 할 수 있습니다

302
00:23:39,160 --> 00:23:42,722
그런데 이제 우리가 영어 실력을
공부를 했다하더라도

303
00:23:42,761 --> 00:23:49,276
실제 이런 방금 말씀드렸던 마켓 인텔리전스나
이런 세부 과제를 종료 해야 하는데

304
00:23:49,840 --> 00:23:55,238
이게 학습할 때랑 공부했을 때랑
토익시험 공부하게 되면 약간의 이제

305
00:23:55,263 --> 00:23:59,418
우리 지식을 좀 더 조정을 해야 되잖아요

306
00:24:01,090 --> 00:24:04,816
그래서 이런 토의 문제 바로 풀 수 있을까라고 했을 때

307
00:24:04,840 --> 00:24:08,527
우리가 고득점을 위해서는 조금 더 문제를 풀어보고

308
00:24:08,552 --> 00:24:11,540
각 문제 유형별로 문제를 풀어보게 됐는데

309
00:24:12,773 --> 00:24:15,577
이제 이런 방식을

310
00:24:18,220 --> 00:24:20,086
전통적인 기계 학습에서는

311
00:24:20,110 --> 00:24:26,805
각 유형별로 문제를 이제 만들어서
그 문제를 풀 수 있는 모델을 만드는 방식을 사용했습니다

312
00:24:27,700 --> 00:24:32,051
하지만 최근에는 방금 계속 이야기했던
이런 언어 모델들을 사용해서

313
00:24:32,320 --> 00:24:34,913
그 언어 모델을 갖고 있는 언어 지식을 통해서

314
00:24:34,938 --> 00:24:40,516
각 유형들을 이제 풀게 되고
미세 조정하는 과정을 거치게 됩니다

315
00:24:40,540 --> 00:24:43,733
이렇게 했을 때 좀 더 적은 데이터를 통해서

316
00:24:43,758 --> 00:24:49,246
더 빠르게 학습하고 더 적은 데이터로도
안정적인 성능을 나타내게 보여주게 됩니다

317
00:24:50,500 --> 00:24:55,515
이러한 것들을 이제 NLP에도
컴퓨터 비전에서도 사용되는데

318
00:24:55,570 --> 00:24:59,015
Transfer Learning 전이 학습이라는
말로 사용이 됩니다

319
00:24:59,590 --> 00:25:03,832
이 전이 학습이라는 거는 언어 내부적으로
공유되는 특징이 있어서

320
00:25:04,330 --> 00:25:08,812
소스 태스크로부터 언어에 대한 지식을
잘 학습을 하게 되면

321
00:25:08,837 --> 00:25:13,846
이 지식을 이용해서 다른 세부 태스크
타깃 태스크에서도

322
00:25:13,870 --> 00:25:18,245
이 지식을 활용할 수 있다 라는
가정에서 출발한 거고요

323
00:25:18,693 --> 00:25:23,866
이런 전이 학습이 정의를 하면서

324
00:25:23,890 --> 00:25:28,802
그 언어 모델에 있는 웨이트들을
조정하는 과정이 있고

325
00:25:28,827 --> 00:25:34,209
이 조정하는 거를 Fine-tuning
미세 조정이라고 표현을 하게 됩니다

326
00:25:34,840 --> 00:25:37,560
그런데 이 언어 모델을 사용하게 되면은
프렉티컬 문제가 아닌데

327
00:25:37,560 --> 00:25:41,527
Fine-tuning을 할 때
모델을 조정해야 하다 보니

328
00:25:42,018 --> 00:25:44,260
학습자원이 많이 필요하게 되고

329
00:25:44,260 --> 00:25:49,018
그리고 이 모델링을 하는 과정에 대한
코드가 계속 연속적으로 유지가 돼야 해서

330
00:25:50,020 --> 00:25:54,220
연구를 하거나 개발하고 배포하는
일련의 과정에 대한 연속성 확보가 필요해서

331
00:25:54,220 --> 00:25:57,800
이 때 이제 테크니컬 갭이 생기게 되고

332
00:25:57,820 --> 00:26:02,596
그리고 사실 우리는 이 언어 모델을 통해서
다양한 문제들을 해결하고 싶은데

333
00:26:02,620 --> 00:26:06,838
이 문제를 해결하는 Fine-tuning 과정을
계속 테스트하고 만드는게

334
00:26:06,863 --> 00:26:09,128
계속 버든이 생기게 됩니다

335
00:26:09,284 --> 00:26:16,229
근데 최근에 Hugging Face라는 곳에서
Transformers 라이브러리를 공개했는데

336
00:26:17,560 --> 00:26:23,926
NLP의 대중화 democratization이라는
이제 자기만의 슬로건을 걸고

337
00:26:23,950 --> 00:26:29,176
NLP 누구나 쉽게 할 수 있도록 최신의
기술도 쓸 수 있도록 하는 라이브러리입니다

338
00:26:29,200 --> 00:26:33,715
이 라이브러리를 통해서 그 프렉티컬
문제들이 많이 해결이 되었는데요

339
00:26:34,060 --> 00:26:40,966
이 Transformers 라이브러리가 최근 정말
떠오르고 있는 핫한 라이브러리인데

340
00:26:41,920 --> 00:26:49,146
최근에 그동안 전통적으로 좀 많이 사용했던
nltk나 gensim과 같은 알고리즘들이

341
00:26:49,171 --> 00:26:54,166
상승된 Github 스타를 받은 것에 대비해서

342
00:26:54,190 --> 00:27:00,796
Hugging Face Transformers는
1년 만에 3만 스타가 넘는 정말

343
00:27:00,820 --> 00:27:07,077
압도적인 떠오르고 있는 인기를 얻고 있는데요

344
00:27:07,102 --> 00:27:10,586
이 Transformers는
세 가지 특징이 있습니다

345
00:27:10,611 --> 00:27:13,998
하나는 딥러닝을 하는 연구자뿐만 아니라

346
00:27:14,023 --> 00:27:17,566
그 행정 프랙티셔너를 위해서
많이 고안이 되어 있고

347
00:27:17,590 --> 00:27:22,420
그리고 처음 시작하는 에듀케이터
학생들에게도 굉장히 좀

348
00:27:22,420 --> 00:27:24,785
쉽게 사용할 수 있는 API를 제공하고 있습니다

349
00:27:26,080 --> 00:27:32,056
최근에 있는 NLU 언어 이해 모델이나
언어 생성 모델들에서도

350
00:27:32,080 --> 00:27:38,525
고성능으로도 쉽고 빠르게 쓸 수 있도록
많은 것들을 지원하고 있고요

351
00:27:39,040 --> 00:27:44,476
그리고 파인튜닝을 하거나 모델을
배포하는 과정에서도 간단한 API로

352
00:27:44,500 --> 00:27:51,601
사용할 수 있게
계속 협업하는 라이브러리입니다

353
00:27:52,060 --> 00:27:57,646
그래서 왜 근데 Transformers를 꼭 써야
되나요라고 이렇게 많이 물어보시는데

354
00:27:57,670 --> 00:28:04,271
사용성이 정말 좋구요 그리고 PyTorch나
TensorFlow 간에 호환성이 있어서

355
00:28:04,990 --> 00:28:09,646
어딘가에 디팬던시를 가져도
쉽게 사용할 수 있고

356
00:28:09,670 --> 00:28:17,443
그리고 fine-tuning이나 모델 배포와 같은
디플로이먼트에도 많이 집중을 하고 있어서

357
00:28:17,500 --> 00:28:21,007
실제 프랙티컬하게 사용하는데
굉장히 많은 도움이 됩니다

358
00:28:22,270 --> 00:28:30,455
가운데 있는 코드처럼 세 줄의 코드만 입력해도
최신 BERT나 ALBERT, 엘렉트라 같은

359
00:28:30,480 --> 00:28:33,460
최신 Transformer 모델을
쉽게 불러와서 사용하고

360
00:28:33,460 --> 00:28:36,300
자기 필요에 맞게
fine-tuning이 가능합니다

361
00:28:36,340 --> 00:28:43,433
fine-tuning 과정은 내부에 있는 3개의 블록만
이용을 해도 굉장히 빠르게 사용할 수 있는데

362
00:28:43,930 --> 00:28:49,306
먼저 tokenizer를 이용해서 텍스트를 이제

363
00:28:49,330 --> 00:28:55,726
어떤 그 앞에서 Contextual embedded
vector로 변환하기 위한

364
00:28:55,750 --> 00:28:59,539
매핑하기 위한 아이디 값들로 변환해주고요

365
00:28:59,710 --> 00:29:05,746
Transformers라는 블록을 통해서
실제 매핑을 통해 벡터

366
00:29:05,770 --> 00:29:08,566
Contextual embedded vector로 변환시켜줍니다

367
00:29:08,590 --> 00:29:13,696
그리고 Heads라는 블록을 통해서
내가 해결하고 싶은 문제에 대해서

368
00:29:13,720 --> 00:29:20,946
output vector을 반환을 해주는 세 개의
블록을 통해서 손쉽게 파인튜닝이 가능합니다

369
00:29:21,820 --> 00:29:30,710
tokenizer도 API가 기존 학습된 사전이나
모델을 불러와서 사용하는 형태이고요

370
00:29:30,760 --> 00:29:35,025
다음과 같은 코드 형태로
간단하게 변환을 할 수가 있습니다

371
00:29:35,230 --> 00:29:40,696
현재 Hugging Face Transformers를
통해서 제공되는 모델은

372
00:29:40,720 --> 00:29:43,071
공식적인 모델은 24개 정도가 있고요

373
00:29:43,900 --> 00:29:47,900
Autoencoding model, Autoregressive
model, BERT나 GPT와 같은

374
00:29:48,204 --> 00:29:53,009
기본적인 모델부터 Seq-to-Seq,
Long-sequence, Efficient한

375
00:29:53,034 --> 00:29:57,180
특성을 갖고 있는 모델들에 대해서도
현재 제공을 하고 있습니다

376
00:29:57,970 --> 00:30:02,915
그래서 이런 모델들을 직접
코딩을 하는 게 아니라

377
00:30:03,430 --> 00:30:10,437
앞에서 본 것 처럼 사전 정의된 형태의 모델들을
불러올 수 있는 형태로 사용이 가능합니다

378
00:30:10,484 --> 00:30:15,106
앞에서 말씀드린 것처럼 이 토큰나이저를 통해서

379
00:30:15,130 --> 00:30:21,606
아이디 값들로 텍스트가 변환된 것들을
이제 Transformer 모델에 넣으면

380
00:30:23,231 --> 00:30:28,996
예시에 있는 아웃풋 처럼 벡터화된 형태로
contextual embedded vector 반환이 됩니다

381
00:30:29,020 --> 00:30:31,380
그리고 이 헤드는 Transformer 모델 위에

382
00:30:31,380 --> 00:30:35,441
보통 우리가 해결하고자 하는 목적에 따라
아웃풋 레이어를 만들게 되는데

383
00:30:35,860 --> 00:30:40,766
Transformer 모델과 아웃풋 레이어를
함께 묶어서 보통 지칭을 하게 됩니다

384
00:30:41,110 --> 00:30:47,056
그래서 헤더 내부적으로는
사전 학습된 Transformer 모델을 불러오고

385
00:30:47,080 --> 00:30:54,017
아웃풋 레이어는 fine-tuning을 통해서
Transformer 모델과 함께 학습이 된 형태입니다

386
00:30:54,850 --> 00:30:59,817
그리고 필요에 따라서 이 헤드를
커스터마이징하기가 편리하게 되어 있어서

387
00:30:59,842 --> 00:31:04,996
필요한 경우에는 API를
수정을 해서 사용할 수 있습니다

388
00:31:05,020 --> 00:31:08,596
다음과 같이 커스터마이즈 된 헤드를 만들어서

389
00:31:08,620 --> 00:31:12,340
Pre-trained 모델을 레이어로 좀 더 쌓는다거나

390
00:31:12,340 --> 00:31:17,346
다른 로스 계산을 다르게 한다거나 하는 
것들을 구현을 할 수도 있습니다

391
00:31:17,463 --> 00:31:23,119
그리고 또 큰 장점인데 PyTorch와
Tensorflow가 높은 호환성이 있어서

392
00:31:23,165 --> 00:31:29,462
PyTorch로 모델을 학습했다 하더라도
Tensorflow로 모델을 불러와서 사용할 수 있습니다

393
00:31:29,782 --> 00:31:32,506
그래서 사람들과 업무를 하게 되면

394
00:31:32,530 --> 00:31:37,405
누군가는 Tensorflow가 좀 더 익숙할 수 있고
PyTorch가 좀 더 익숙한 사람도 있을 수 있는데

395
00:31:37,810 --> 00:31:41,919
이 Hugging Face Transformers를
통해서 작업을 하게 되면

396
00:31:42,430 --> 00:31:50,187
Transformer 안에서 이렇게 변환이
자유로워서 실제 업무를 할 때 편안하게 됩니다

397
00:31:50,620 --> 00:31:55,573
그리고 이게 제가 생각하는
가장 큰 Transformers 장점인데

398
00:31:56,260 --> 00:31:59,250
실제 이 자기네가 만든
Transformers 라이브러리를 통해서

399
00:31:59,275 --> 00:32:03,197
프로덕션 레벨로 개발하는 것에 대한
고민을 되게 많이 했습니다

400
00:32:03,222 --> 00:32:09,663
그래서 지금 실제 다른 프로덕션 레벨에 있는
팀들과의 협업을 많이하고 있는 걸로 알고 있고

401
00:32:09,688 --> 00:32:12,883
그 실험 결과들이 계속 공유가 되고 있는 상황입니다

402
00:32:13,090 --> 00:32:18,376
서빙을 하는 곳과는
TorchServing과 뭔가 실험을 하고 있고

403
00:32:18,402 --> 00:32:24,416
인퍼런스를 좀 더 추적하는 것에
ONNX팀이나 TVM팀과 같은 곳들과

404
00:32:24,441 --> 00:32:30,615
하드웨어 차원에서의 인퍼런스 속도를 
높이는 것들을 연구하고 있는 걸로 알고 있고요

405
00:32:30,640 --> 00:32:35,416
그리고 엣지 디바이스에 대해서도
고려가 많이 되어 있는 상황이고

406
00:32:35,440 --> 00:32:42,196
그리고 Optuna나 Ray Tune 같이
실제 사용해 봤던 것들인데

407
00:32:42,227 --> 00:32:46,149
Hyperparameter Optimization과
관련된 것들도 최근 추가가 되었습니다

408
00:32:46,960 --> 00:32:50,300
그 밖에도 PyTorch lightning이나
Weight & Biases 같은

409
00:32:50,300 --> 00:32:54,374
다른 머신러닝 툴이나 라이브러리들과
 연계가 잘 되고 있어서

410
00:32:55,240 --> 00:32:59,927
굉장히 사용할 때 도움이 많이 되고
직접 구현했던 부분들도

411
00:32:59,952 --> 00:33:04,068
어느새 공식적인 기능으로 통합이 되기 때문에

412
00:33:04,960 --> 00:33:09,522
이미 짜던 코드들을 변환해야 되는
상황도 발생하곤 했습니다

413
00:33:11,634 --> 00:33:18,813
개발을 엄청 빨리 하고 있어서
이런 것들은 API 있으면 좋겠다 싶은 것들은

414
00:33:19,329 --> 00:33:23,086
이미 뭔가 다른 브랜치에서 실험을 하고
있거나 이미 올라와 있거나

415
00:33:23,110 --> 00:33:30,891
PR 중이거나 그런 상황입니다
되게 많은 열정적인 개발자 분들의

416
00:33:31,810 --> 00:33:34,513
빠른 업데이트가 이뤄지고 있는
라이브러리이기도 합니다

417
00:33:36,400 --> 00:33:38,896
그리고 이게 또 다른 정말 큰 장점인데요

418
00:33:38,920 --> 00:33:44,271
커뮤니티를 통해서
학습한 모델들이 공유가 되고 있고

419
00:33:44,623 --> 00:33:47,529
자기가 만든 모델도
쉽게 공유할 수 있는 상황입니다

420
00:33:47,799 --> 00:33:50,660
구글이나 페이스북도 이제 공식적인 모델들을

421
00:33:50,660 --> 00:33:54,245
Hugging Face Transformer 커뮤니티
허브를 통해서 제공을 하고 있습니다

422
00:33:54,346 --> 00:33:58,340
그러면 이제 Transformers를 사용해서
KB-ALBERT를 어떻게

423
00:33:58,340 --> 00:34:03,813
파인튜닝을 해볼 수 있을까 라는
질문이 있을 수 있는데요

424
00:34:03,931 --> 00:34:07,156
우리가 PyTorch나
Transformers 코드를 사용하게 되면

425
00:34:08,000 --> 00:34:11,250
그 학습을 하기 위한 코드나 모델링을
위한 코드나 tokenizing 하는 코드

426
00:34:11,289 --> 00:34:18,405
이런 것들을 짜게 되면서 나이브하게 코딩을
하다 보면 굉장히 많은 프로그램이 필요하게 되는데

427
00:34:18,429 --> 00:34:23,956
이 Transformers 라이브러리를 사용하게
되면은 정말 적은 코드만으로도

428
00:34:23,980 --> 00:34:26,698
fine-tuning의 모델을 짤 수가 있습니다

429
00:34:27,190 --> 00:34:33,256
이번 예시에는 네이버 영화 리뷰에 대한
감성 분석 관련 예제를 통해서 소개를 하려고 하는데

430
00:34:34,622 --> 00:34:37,286
이 예제에 대한 파인튜닝을 하기 위해서

431
00:34:37,318 --> 00:34:43,551
그리고 인퍼런스 파이프라인까지 만드는데
단1 0줄의 코드만으로 개발이 가능했습니다

432
00:34:43,960 --> 00:34:49,786
그래서 데이터로 준비하는 과정을 거친 다음에
또 tokenizer로 학습용 데이터 셋을 만들고

433
00:34:49,810 --> 00:34:55,726
학습용 데이터 셋을 가지고
헤드를 통해서 모델 파인 튜닝을 하는데

434
00:34:55,750 --> 00:35:02,601
이 모든 일련의 과정들이 정말 빠르게 테스트나
학습이나 그리고 나중에 인퍼런스용 코드까지

435
00:35:02,626 --> 00:35:05,746
정말 심플하고 하지만 파워풀하게 만들 수 있습니다

436
00:35:05,770 --> 00:35:09,100
또 네이버 영화 리뷰는 총 20만 개의 리뷰 중에서

437
00:35:09,100 --> 00:35:13,149
학습 데이터 15만개,
테스트 데이터 5만개로 구성되어 있습니다

438
00:35:13,300 --> 00:35:19,401
긍정적인 리뷰에는 1로 표기가 돼있고
부정적인 리뷰에는 0으로 표시되어 있는 데이터입니다

439
00:35:19,600 --> 00:35:22,654
그 앞에서 말씀드린 것처럼
tokenizer를 이용하면은

440
00:35:23,200 --> 00:35:28,606
이렇게 사전 학습된 tokenizer를 불러오고
그 tokenizer를 통해서

441
00:35:28,630 --> 00:35:32,028
그 아이디 값으로 텍스트로 변환하게 해줍니다

442
00:35:32,710 --> 00:35:35,536
그 이후에는 PyTorch 같은 경우에는 데이터 셋으로

443
00:35:35,560 --> 00:35:41,278
그리고 Tensorflow 같은 경우에는
Tensor Slice같은 걸로 변환하게 해주면 됩니다

444
00:35:41,465 --> 00:35:46,220
그리고 학습을 하기 위해서
학습 epoch를 몇으로 설정 할 건지

445
00:35:46,220 --> 00:35:52,605
미니 배치의 배치 사이즈를 어떻게 할건지와
같은 하이퍼파라미터를 설정을 해주고

446
00:35:53,425 --> 00:35:56,089
그리고 이제 헤드를 불러 불러야 됩니다

447
00:35:56,740 --> 00:36:00,526
네이버 영화 리뷰 같은 경우에는
Sequence Classification의 문제기 때문에

448
00:36:00,550 --> 00:36:06,737
이제 AlbertForSequenceClassification을
통해서 사전 학습된 언어모델를 불러오고

449
00:36:06,762 --> 00:36:13,301
이 언어 모델을 Hugging Face에서 제공해주는
트레이너라는 API를 통해서 학습을 해 주면됩니다

450
00:36:13,840 --> 00:36:19,965
그리고 학습된 이 모델은 tokenizer 모델을
파이프라인이라는 하나의 API를 통해서 쉽게 할 수 있는데

451
00:36:20,380 --> 00:36:25,348
이렇게 그 무대가 tokenizer를 사전 학습된 것을 파인튜닝 해준 걸 넣어주면

452
00:36:25,450 --> 00:36:29,598
다음처럼 리뷰를 넘었을 때 결과가 나오게 됩니다

453
00:36:30,160 --> 00:36:34,486
지금 이렇게 쭉 설명하는 그 코드들이 10줄 정도밖에 되지 않았는데

454
00:36:34,510 --> 00:36:39,533
10줄의 코드만 파이썬 프로그램에서 하실 수 있으면
쉽게 파인튜닝을 할 수가 있습니다

455
00:36:40,180 --> 00:36:44,836
이 Transformers가 굉장히 파워풀하고 되게 편리한데

456
00:36:44,860 --> 00:36:51,719
사용할 때 좀 주의해야 될 점이 버전이 하나씩
하나씩 올라갈 때마다 그 효과가 생각보다 큽니다

457
00:36:51,899 --> 00:36:56,656
사용자들이 어떤 니즈나 필요들을 계속 반영하기 때문에

458
00:36:56,680 --> 00:36:58,906
이 업그레이드할때 마다 많이 변화가 되구요

459
00:36:59,290 --> 00:37:02,547
그래서 그 변화하는 거에 따라서 기능이 점점 좋아지긴 하는데

460
00:37:02,586 --> 00:37:07,546
그 가끔은 제가 직접 만들었던 기능들이 이걸 한데 충돌이 나와서

461
00:37:07,570 --> 00:37:11,386
기존의 작성했던 코드를 수정하는 경우도 종종 있었습니다

462
00:37:11,410 --> 00:37:15,283
그리고 많은 것들이 추상화되어 있기 때문에

463
00:37:15,580 --> 00:37:19,126
때로는 좀 더 세부적인 Optimization을 위해서는

464
00:37:19,450 --> 00:37:23,504
기본에 API 구조를 따르면서
커스터마이제이션 하는 게 좋을 수 있습니다

465
00:37:25,090 --> 00:37:30,199
또는 필요에 따라 일부 API 사용하는 것도
좋은 방법일 수도 있습니다

466
00:37:31,150 --> 00:37:38,173
그데 이거는 하나의 좀 제한점인데
Transformers 팀은 Fine-tuning이나 Deployment에 집중하고 있다보니까

467
00:37:38,440 --> 00:37:42,518
그 ALBERT나 BERT 같은 언어 모델에 대해서
Pre-training을 하고 싶다면은

468
00:37:43,630 --> 00:37:46,546
직접 좀 개발을 많이 해야 되는 상황입니다

469
00:37:46,570 --> 00:37:52,249
네 저는 처음에 준비했을 때 40분이면 시간이 길 거라고 생각했는데

470
00:37:52,750 --> 00:37:57,984
이렇게 말을 하다 보니 좀 시간이 많이 빠르게 지나왔고

471
00:37:58,660 --> 00:38:00,730
정신없이 좀 온 것 같은데

472
00:38:02,140 --> 00:38:07,999
저는 이제 Hugging Face의
Transformers를 사용하면서

473
00:38:08,980 --> 00:38:11,970
비즈니스 문제와 데이터에 좀 더 집중할 수 있었고

474
00:38:11,995 --> 00:38:18,860
어떻게 하면 좀 더 데이터를 확보하고
언어 모델을 잘 미세 조정할 수 있을까?

475
00:38:18,885 --> 00:38:21,588
이런 것들을 좀 고민할 수 있어서
정말 도움이 많이 됐습니다

476
00:38:22,150 --> 00:38:24,406
그리고 API가 워낙 좀 잘 짜져 있다 보니까

477
00:38:24,430 --> 00:38:30,346
커스터 마이제이션으로옵티마이제이션 했을 때도
쉽고 빠르게 테스트를 해볼 수도 있었다는 점도 있었고

478
00:38:30,370 --> 00:38:39,924
새로운 모델들이 나왔을 때도 부담 없이 바로 갈아끼는
형태로 플러그인 플레이를 할 수 있어서 굉장히 좋았습니다.

479
00:38:40,060 --> 00:38:48,060
아쉬운 점은 일본어나 프랑스와 아랍어 같은
언어와 같은 것들이 현재 Transformers 쪽의

480
00:38:48,490 --> 00:38:54,755
컨트리미션이 많아서 tokenizer 같은 것들도 공식적으로 사용을 할 수 있는데

481
00:38:54,970 --> 00:38:59,776
한국어 같은 경우에는 공식 tokenizer가 등록이 되어 있지 않고

482
00:38:59,800 --> 00:39:05,376
등록된 모델의 수도
다른 언어에 비해서 적은 편입니다

483
00:39:05,401 --> 00:39:10,565
관심 있거나 개발을 이미 하고 계신 분들은
적극적인 공유를 통해서

484
00:39:11,350 --> 00:39:16,559
한국어 쪽에서도 많은 기회가
있었으면 하는 바람이 있고

485
00:39:16,584 --> 00:39:22,559
저도 시간과 기회가 허락한다면
Transformers나 NLP 쪽에 조금더

486
00:39:22,584 --> 00:39:25,130
공헌할 수 있으면 하는 바람이 있습니다

487
00:39:25,990 --> 00:39:29,686
지금까지 이제 준비한 내용이고요

488
00:39:29,710 --> 00:39:37,710
이렇게 발표를 통해서 미흡했던 부분들은 발표자료나
예제 소스코드를 GitHub에 올려서 계속 추가해나가면서

489
00:39:38,110 --> 00:39:43,063
이제 발표에서 끝나는 게 아니라 지속적인 정보 공유나

490
00:39:43,088 --> 00:39:50,002
아니면 다른 분들의 어떤 의견들을
얻을 수 있는 창구로 사용을 할 예정입니다

491
00:39:51,100 --> 00:39:57,496
지금까지 금융 언어 이해를 위해 개발된
ALBERT 톺아보기 with Transformers

492
00:39:57,521 --> 00:39:59,528
발표를 마치도록 하겠습니다

493
00:40:03,385 --> 00:40:04,435
감사합니다



