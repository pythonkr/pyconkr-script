1
00:00:11,010 --> 00:00:14,700
안녕하세요. 파이써니스타 여러분 저는 김성현이라고 합니다

2
00:00:14,730 --> 00:00:20,860
제가 오늘 발표할 주제는 파이썬으로 구현하는
신경세포 기반의 인공뇌 시뮬레이터입니다

3
00:00:21,420 --> 00:00:24,480
발표에 앞서 저를 간단히 먼저 소개해 드리겠습니다

4
00:00:24,510 --> 00:00:28,440
저는 현재 솔트룩스에서 자연어 임베딩 분야를 연구하고 있습니다

5
00:00:28,470 --> 00:00:34,110
저의 가장 큰 관심사는 인간의 학습원리
즉 뇌에서 발생하는 지식의 임베딩 원리와

6
00:00:34,140 --> 00:00:39,740
최신 딥러닝 기술을 융합하여 AGI 범용
인공지능을 개발하고자 하는 꿈을 가지고 있습니다

7
00:00:40,050 --> 00:00:43,050
앞서 오늘 발표 주제에 대해서 설명을 드렸었는데요

8
00:00:43,080 --> 00:00:48,760
바로 이 꿈을 이루기 위해 공부했던 내용들을
공유하고자 파이콘에 발표 신청을 하게 됐습니다

9
00:00:49,050 --> 00:00:51,510
오늘 발표 순서는 다음과 같습니다

10
00:00:51,540 --> 00:00:57,040
먼저 우리 모두가 가지고 있지만 아직도
미지의 영역인 뇌에 대해서 살펴보도록 하겠습니다

11
00:00:57,360 --> 00:01:02,430
그리고 신경세포와 스파이크 기반의 신경망 모델인 스파이킹 뉴런 네트워크

12
00:01:02,460 --> 00:01:05,340
즉 SNN에 대해서 알아보도록 하겠습니다

13
00:01:05,670 --> 00:01:09,560
마지막으로 SNN에 활용도에 대해 살펴보도록 하겠습니다

14
00:01:09,930 --> 00:01:12,330
그러면 먼저 뇌에 대해서 살펴볼까요?

15
00:01:12,660 --> 00:01:15,730
뇌는 약 1.5킬로의 조직으로 이루어져 있습니다

16
00:01:16,050 --> 00:01:20,460
우리의 신체 전체에서 뇌가 차지하는 비율은 굉장히 작음에도 불구하고

17
00:01:20,490 --> 00:01:24,430
뇌는 모든 장기를 컨트롤하는 가장 주된 역할을 수행하고 있죠

18
00:01:24,950 --> 00:01:29,220
또한 학습을 하고 기억을 하고 기억에 대한 retrieval 도 가능합니다

19
00:01:29,580 --> 00:01:33,090
감정을 느끼고 표현하고 주변 상황을 인지하고

20
00:01:33,120 --> 00:01:37,650
언어를 학습하고 감각을 느끼며 생각과 상상도 할 수 있습니다

21
00:01:38,190 --> 00:01:44,590
대처 뇌가 어떻게 생겨 먹었길래 이렇게 말도 안
되게 복잡한 연산들을 동시에 처리할 수 있을까요?

22
00:01:44,970 --> 00:01:50,820
바로 이 궁금증 때문에 신경과 즉 Neuroscience 라고 하는 학문이 발전하게 됩니다

23
00:01:51,080 --> 00:01:55,900
1865년 오토는 처음으로 뇌조직을 현미경으로 관찰하고

24
00:01:55,930 --> 00:01:58,860
뇌세포  즉 neuron의 존재를 밝혀냈습니다

25
00:01:59,250 --> 00:02:05,520
그리고 수많은 neuron들이 서로 긴 나뭇가지와 같은
줄기를 통해 연결돼 있음을 관찰할 수 있었습니다

26
00:02:06,030 --> 00:02:11,460
셀링턴과 카할은 이러한 연결을 통해 Neural signal 의 전파가 일어날 것이라고 가정하고

27
00:02:11,490 --> 00:02:14,750
전파가 일어나는 공간을 시냅스라고 명명하였습니다

28
00:02:14,880 --> 00:02:17,340
그런 neuron의 구조를 한번 살펴볼까요?

29
00:02:17,370 --> 00:02:19,890
neuron은 크게 세 부분으로 이루어져 있습니다

30
00:02:19,920 --> 00:02:25,360
먼저 Cell body 라고 할 수 있는 soma 여기에는
세포에게는 Nucleus 가 포함되어 있습니다

31
00:02:25,380 --> 00:02:31,060
그리고 나뭇가지와 같은 Dendrite 에서는 다른 neuron부터 Neural signal input 을 받게 됩니다

32
00:02:31,500 --> 00:02:36,230
Dendrite 가 input 을 받는 곳이라면 output 을 담당하는 곳은 Axon 입니다

33
00:02:36,780 --> 00:02:41,940
neuron의 input 은 합산되어 절연체인 Myelin sheath 로 감싸지는 Axon 을 지나고

34
00:02:41,970 --> 00:02:47,040
Axon terminal 에서 Neuro-Transmitter 를 분비함으로써
Neural signal 을 전파하게 되죠

35
00:02:47,410 --> 00:02:50,960
이 Neural signal이라고 하는 것은 전기적 신호로 이루어져 있습니다

36
00:02:51,270 --> 00:02:57,050
1945년 Hodgkin과 Huxley는 오징어의 Giant axon 에 Electrode 라고 하는

37
00:02:57,080 --> 00:03:02,320
전압 변화 측정 장치를 삽입하고 neuron의 멤브레인 전압 변화를 측정하였습니다

38
00:03:02,730 --> 00:03:07,460
이 실험을 통해 그들은 neuron이 전기적 신호를 발생시키는 걸 확인하였습니다

39
00:03:07,920 --> 00:03:10,830
이를 Action Potential 혹은 Spike 라고 합니다

40
00:03:11,430 --> 00:03:16,820
neuron의 멤브레인 내부에는 음전하를 많이
가지고 있어서 마이너스 전압 값을 가집니다

41
00:03:17,280 --> 00:03:22,530
이때 양전하 Neuro-Transmitter 를 Dendrite를 통해 유입이 되면

42
00:03:22,560 --> 00:03:25,190
맴브레인은 탈분극이 발생하게 됩니다

43
00:03:25,620 --> 00:03:30,640
여기서 멤브레인 포텐셜이 특정 threshold 를 넘게 되면
Action Potential이 발생하게 되죠

44
00:03:30,840 --> 00:03:36,600
즉 neuron은 입력에 대해 특정 threshold에 따라 발화가 되지 않는 0

45
00:03:36,630 --> 00:03:42,180
아니면 발화가 발생하는 1, 0과 1의 신호으로 Neural signal에 전파하게 됩니다

46
00:03:42,210 --> 00:03:46,760
이러한 신경신호는 시냅스를 통해 다음 neuron으로 전파가 일어나게 됩니다

47
00:03:46,790 --> 00:03:50,700
그래서 시냅스 전 neuron는 Presynaptic neuron 시냅스 후

48
00:03:50,730 --> 00:03:53,310
neuron은 Postsynaptic neuron이라고 합니다

49
00:03:53,640 --> 00:03:55,770
시냅스는 크게 두 종류가 있습니다

50
00:03:55,800 --> 00:03:59,730
먼저 Excitatory Synapse는 양전하를 분비함으로써

51
00:04:00,080 --> 00:04:03,800
시냅스후 뉴런의 Depolarisation 을 야기하게 됩니다

52
00:04:04,110 --> 00:04:07,950
반대로 이니 Inhibitory Synapse는 음전하를 분비함으로써

53
00:04:07,970 --> 00:04:11,670
시냅스후 neuron의 Hyperpolarisation을 야기하게 됩니다

54
00:04:11,910 --> 00:04:15,410
재미있는 것은 이렇게 전파되는 Neural signal의 강도가

55
00:04:15,430 --> 00:04:18,440
상황에 따라 조절되는 것이 관찰 됐다는 것입니다

56
00:04:18,900 --> 00:04:24,530
1958년 휴즈는 Presynaptic neuron의
지속적인 전기적 자극을 주었을 때

57
00:04:24,560 --> 00:04:29,970
Postsynaptic neuron에서 측정되는 neural signal 이
더 크게 측정되는 것을 관찰하였습니다

58
00:04:30,000 --> 00:04:34,260
즉 neuron과 neuron을 연결하는 Synapse weight 가 더 커지게 된 것이죠

59
00:04:34,620 --> 00:04:40,940
이러한 현상을 시냅스가 마치 플라스틱처럼 유연하게
변화한다는 의미는 Synaptic Plasticity라고 합니다

60
00:04:41,190 --> 00:04:45,510
뇌라고 하는 것은 바로 이 세 가지 컴포넌트에 의해서 구성되어 있습니다

61
00:04:45,840 --> 00:04:51,030
86억개에 해당하는 neuron과 한 neuron당 약 7000개씩 붙어 있는 시냅스

62
00:04:51,160 --> 00:04:54,800
그리고 시냅스에 웨이트가 조절되는 Synaptic Plasticity입니다

63
00:04:55,380 --> 00:04:58,540
현대기계학습은 신경과학을 통해 밝혀진

64
00:04:58,560 --> 00:05:03,180
neuron의 이 세 가지 특성을 이용해 만들어진 인공신경망을 기반으로 하고 있습니다

65
00:05:03,330 --> 00:05:08,550
그리고 이러한 인공신경망은 현재의 딥러닝으로까지 엄청난 발전을 이룩하며

66
00:05:08,580 --> 00:05:11,330
자연어 뿐만 아니라 실생활에 많은 분야에서

67
00:05:11,350 --> 00:05:14,770
휴먼 퍼포먼스를 뛰어넘는 놀라운 성능을 보여주고 있습니다

68
00:05:15,000 --> 00:05:18,430
하지만 그럼에도 불구하고 현재의 인공신경망은

69
00:05:18,450 --> 00:05:20,610
분명한 한계점을 가지고 있습니다

70
00:05:20,920 --> 00:05:23,910
인공신경망은 기본적으로 오차 역전파

71
00:05:23,940 --> 00:05:26,790
즉 에러 back propagation을 기반으로 학습을 수행합니다

72
00:05:26,820 --> 00:05:31,180
하지만 오차 역전파의 경우 최종 ouput layer의 출력과

73
00:05:31,210 --> 00:05:35,670
타겟의 차이점 하나만을 가지고 모델 전체의 학습이 이루어지게 됩니다

74
00:05:36,060 --> 00:05:42,100
따라서 신경망이 무한대로 커지고 복잡해진다면
output layer의 에러는 갈수록 희석될 것입니다

75
00:05:42,600 --> 00:05:46,930
또한 back propagation의 알고리즘
특성상 local minimum에 빠질 수도 있죠

76
00:05:47,020 --> 00:05:51,820
물론 이를 극복하기 위해 다양한 Optimizer 와
Activation function들이 제시되고 있지만

77
00:05:52,020 --> 00:05:55,220
back propagation의 태생적인 한계는 분명히 존재합니다

78
00:05:55,240 --> 00:05:59,760
또한 인공신경망은 prediction을 위한 Feed forward와 Training을 위한

79
00:05:59,790 --> 00:06:03,450
Feedback network가 서로 같은 weight를 사용하고 있다는 점입니다

80
00:06:03,480 --> 00:06:08,590
심지어 Feed forward 와 Feedback network 가
서로 다른 수식으로 개선됨에도 불구하고 말이죠

81
00:06:08,620 --> 00:06:11,750
결국 Error gradient와 Neuron activation은

82
00:06:11,770 --> 00:06:15,250
별개로 메모리에 저장에 대해서 학습이 이루어지게 됩니다

83
00:06:15,270 --> 00:06:20,610
이러한 학습원리는 생물학적으로 발생하기 어렵고 물리학적인 근거가 부족합니다

84
00:06:20,700 --> 00:06:26,560
일반적으로 neuron과 neuron 사이의 Neural signal 은
시냅스를 통해 단 방향으로 전파가 이루어지게 됩니다

85
00:06:26,880 --> 00:06:30,160
물론 서로 recurrent 하게 연결되어 있는 이론도 존재하지만

86
00:06:30,180 --> 00:06:33,720
이런 neuron이 뇌 전체를 구성하고 있다고 말할 수 있는 것은 아닙니다

87
00:06:33,750 --> 00:06:40,260
나아가 학습과 예측이 명확하게 분리되어 있기
때문에 back propagation에서 발생한 에러값은

88
00:06:40,290 --> 00:06:45,090
Prediction 과정에 Feed forward network 활성에 어떠한 영향도 미치지 않습니다

89
00:06:45,120 --> 00:06:49,390
또한 인공신경망의 출력값은 continuous floating value 를 사용합니다

90
00:06:49,710 --> 00:06:54,360
따라서 미분이 불가능한 연산에 대해서는
back propagation이 동작하지 않습니다

91
00:06:54,660 --> 00:06:58,860
실제 neuron은 Spike 로 결정되는 1과 0의 값을 이용하게 되죠

92
00:06:58,890 --> 00:07:04,640
추가로 back propagation의 알고리즘에 의해
loss function 는 0 으로 수렴하도록 이루어지며

93
00:07:04,670 --> 00:07:08,700
이 경우엔 학습에 더 진행되지 않고 over fitting 을 야기할 수도 있습니다

94
00:07:08,730 --> 00:07:12,840
자 그러면 현재 ANN이 이런 다양한 한계점을 지니고 있는데요

95
00:07:12,870 --> 00:07:16,950
그렇다면 뇌는 어떨까요? 뇌는 대체 어떻게 학습을 할 수 있을까요?

96
00:07:16,980 --> 00:07:20,940
바로 이 궁금증들 때문에 neuron은 실제와 같이 모사하는

97
00:07:20,970 --> 00:07:25,980
Neuromorphic Spiking neuron network model이라고 하는
신경망이 탄생하게 됩니다

98
00:07:26,010 --> 00:07:29,820
이제 본격적으로 SNN에 대해서 살펴보도록 하겠습니다

99
00:07:29,850 --> 00:07:35,880
기존 기계학습을 공부하셨던 분들이라면 SNN의
동작이 크게 안와 닿을 수도 있을 것 같습니다

100
00:07:35,910 --> 00:07:40,360
SNN의 컴포넌트는 앞에서 설명 들었다는던 뇌 컴포넌트와 동일합니다

101
00:07:40,590 --> 00:07:43,610
바로 neuron의 역할을 할 수 있는 Spiking neuron model

102
00:07:43,630 --> 00:07:46,230
그리고 각 neuron 을 연결하는 시냅스와

103
00:07:46,260 --> 00:07:50,070
시냅스에 적용되어 있는 학습규칙인 Synaptic Plasticity 입니다

104
00:07:50,100 --> 00:07:53,950
왼쪽 그림의 그래프에서 X축은 Time-serise가 되고

105
00:07:53,970 --> 00:07:56,600
파란색 막대가 Spike train 이 됩니다

106
00:07:57,000 --> 00:08:01,890
여기서 한 neuron의 입력은 Presynaptic neuron 에서
발생한 Action Potential 이 됩니다

107
00:08:01,920 --> 00:08:06,170
Postsynaptic neuron 은 Presynaptic neuron 의
Neural signal 정보를 합산하여

108
00:08:06,200 --> 00:08:09,860
특정 threshold 를 넘게 되면
역시 Action Potential 로 출력하게 되죠

109
00:08:09,880 --> 00:08:13,100
이때 기존 신경망 모델과의 가장 큰 차이점은

110
00:08:13,130 --> 00:08:16,770
이러한 시뮬레이션은 시간의 흐름에 따라 동작한다는 것입니다

111
00:08:16,800 --> 00:08:20,340
자 그럼 Spiking neuron model 은 어떻게 만들 수 있을까요?

112
00:08:20,370 --> 00:08:23,580
Spiking neuron model 은 그 종류가 아주 다양합니다

113
00:08:23,910 --> 00:08:29,220
오늘 소개해 드릴 모델은 LIF model 혹은 IAF model 이라고요

114
00:08:29,240 --> 00:08:34,500
이외에 Hodgkin-Huxley model, Izhikevich model 등
매우 다양한 방식의 모델이 존재합니다

115
00:08:34,530 --> 00:08:39,600
짧은 발표 시간 관계상 오늘은 LIF neuron model 만
소개해 드리도록 하겠습니다

116
00:08:39,630 --> 00:08:45,090
LIF neuron 은 1907년에 처음 제시가
되었으며 세 가지의 특성을 가지고 있습니다

117
00:08:45,430 --> 00:08:48,320
먼저 Sub-threshold 의 입력이 뉴런으로 들어오면

118
00:08:48,340 --> 00:08:51,130
이를 지속적으로 Integration 을 수행하게 됩니다

119
00:08:51,280 --> 00:08:56,850
그리고 Integration 된 Neural signal 이 threshold 를 넘으면
발화로 이어지게 됩니다

120
00:08:56,880 --> 00:09:00,410
발화 이후에는 입력을 받아도 일정 기간 무시하며

121
00:09:00,430 --> 00:09:02,450
다시 Resting 상태로 돌아가게 됩니다

122
00:09:03,060 --> 00:09:06,850
이러한 특성은 RC circuit 의 간단한 구현될 수 있습니다

123
00:09:07,080 --> 00:09:10,840
RC circuit 은 Register 와 Capacitor 로 구성되어 있습니다

124
00:09:11,070 --> 00:09:13,680
Capacitor 는 에너지를 저장할 수 있고요

125
00:09:13,710 --> 00:09:18,810
그리고 직렬의 Register 는 시간의 흐름에 따른
충전과 방전을 제어할 수 있습니다

126
00:09:19,320 --> 00:09:24,210
회로에 전류가 흐르기 시작하면 Capacitor 에 전류가 점차 저장될 것입니다

127
00:09:24,540 --> 00:09:30,340
그리고 Capacitor 에 저장 한도를 초과하게 되면
저항에 의해 전압이 천천히 증가하게 되겠죠

128
00:09:30,540 --> 00:09:36,810
전류의 흐름에 멈추게 되면 Capacitor 에 저장되었던
전류가 흐르며 전압이 천천히 감소하게 됩니다

129
00:09:37,350 --> 00:09:40,550
이러한 RC circuit 에 current 입력을 추가하여

130
00:09:40,570 --> 00:09:43,610
LIF model 을 위한 circuit 을 설계할 수 있습니다

131
00:09:44,040 --> 00:09:49,550
자세한 내용은 아래 Appendix 로 추가되어
있는 유튜브 영상들을 봐두시면 좋을 것 같습니다

132
00:09:49,710 --> 00:09:53,240
자 이제 Neuron model 이 만들어졌으니 시냅스 모델은 살펴볼까요?

133
00:09:53,580 --> 00:09:58,820
시냅스 모델은 Neuron model 로 입력을
Input current 로서 조절을 하게 됩니다

134
00:09:59,040 --> 00:10:03,030
그래서 Presynaptic neuron 들을 활성들을 모두 합산을 하고

135
00:10:03,050 --> 00:10:07,200
그 합산된 정보를 Postsynaptic neuron 의 input current 로써

136
00:10:07,220 --> 00:10:09,830
전달하는 것이 시냅스 모델의 역할입니다

137
00:10:09,860 --> 00:10:15,450
여기서 Excitatory 는 흥분성 신호로 Weight 를 양수 값으로 표현을 하고

138
00:10:15,480 --> 00:10:20,410
Inhibitory neuron 은 억제성 신호로 Weight 를 음수 값으로 표현을 하게 됩니다

139
00:10:20,640 --> 00:10:24,590
이제 시냅스의 Weight 를 조절하기 위한 규칙도 정의가 필요합니다

140
00:10:24,840 --> 00:10:28,650
학습이라고 하는 것은 기본적으로 Punishment 를 줄이거나

141
00:10:28,680 --> 00:10:33,060
혹은 Reward 를 최대로 하기위해 네트워크를 업데이트하는 과정을 말합니다

142
00:10:33,090 --> 00:10:36,590
그래서 현재 ANN에는 back propagation을 사용하고 있죠

143
00:10:37,230 --> 00:10:40,330
SNN에서는 두 가지의 학습 규칙이 적용됩니다

144
00:10:40,650 --> 00:10:43,090
하나는 Fire-together, Wire-together

145
00:10:43,120 --> 00:10:46,430
Heavy 한 learning rule 기반의 unsupervised learning rule 입니다

146
00:10:46,860 --> 00:10:52,170
다른 하나는 출력과 타겟 간에 에러를 감소시키는
supervised learning rule 이 있습니다

147
00:10:52,200 --> 00:10:56,160
대표적인 Heavy한 learning rule 중 하나가 STDP learning rule 입니다

148
00:10:56,480 --> 00:11:00,170
이 룰은 Presynaptic neuron 과 Postsynaptic neuron 은

149
00:11:00,200 --> 00:11:04,270
발화시간 차이에 의해서 학습이 조절되는 것을 의미합니다

150
00:11:04,950 --> 00:11:09,660
그래서 Presynaptic neuron 이 발화한 후에 Postsynaptic neuron 의 발화가 일어나면

151
00:11:09,690 --> 00:11:14,010
Synaptic weight 가 강화가 일어나고 반대의 경우엔 약화가 일어납니다

152
00:11:14,220 --> 00:11:18,260
실제로 1998년에 생물학적 실험을 통해 밝혀졌습니다

153
00:11:18,630 --> 00:11:22,880
이처럼 SNN 에서는 기본적으로 뉴런과 뉴런 사이의

154
00:11:22,900 --> 00:11:25,700
local-learning 방식이 적용되어 있습니다

155
00:11:25,880 --> 00:11:31,180
또 다른 학습규칙으로는 1982년에 밝혀진 BCM learning rule 이 있습니다

156
00:11:31,530 --> 00:11:35,350
이 룰은 Presynaptic neuron 에 Firing-rate 에 따라

157
00:11:35,380 --> 00:11:37,950
Weight 가 조절되는 것을 의미합니다

158
00:11:38,280 --> 00:11:43,630
그래서 Presynaptic neuron 의 Firing-rate 가 낮은 Hz로 동작을 하게 되고

159
00:11:43,650 --> 00:11:46,940
그때 Postsynaptic 에 발화를 야기하게 되면

160
00:11:46,960 --> 00:11:49,160
이때는 Weight 가 약화가 일어납니다

161
00:11:49,440 --> 00:11:52,510
반대 경우에는 Weight 가 강화가 일어나게 됩니다

162
00:11:52,540 --> 00:11:57,150
뇌에서의 supervised learning 은 아직까지 밝혀지지 않은 미지의 영역입니다

163
00:11:57,330 --> 00:12:02,340
그래서 뇌에서의 학습방식을 연구하는 다양한 가설들이 제시되어지고 있습니다

164
00:12:02,370 --> 00:12:05,230
이러한 가설들은 다음의 공통점이 있습니다

165
00:12:05,420 --> 00:12:08,970
먼저 Feed-forward network 와 Feedback-network 가

166
00:12:09,000 --> 00:12:13,540
서로 다른 Weight 를 가지며 서로 다른 네트워크 구조를 가지고 있습니다

167
00:12:13,580 --> 00:12:18,180
또한 에러의 값이 네트워크 전체를 걸쳐 전파되는 것이 아니라

168
00:12:18,210 --> 00:12:23,450
local error 를 표현해 local error 를 감소시키는 방향으로
학습이 이루어지게 됩니다

169
00:12:23,490 --> 00:12:26,250
대표적인 알고리즘이 바로 TP learning 입니다

170
00:12:26,280 --> 00:12:31,800
TP learning rule 은 Feed-forward 를 구성하는 네트워크 와
Feedback 을 구성하는 네트워크가

171
00:12:31,830 --> 00:12:34,670
서로 독립되어 있는 네트워크로 구성이 되어 있습니다

172
00:12:34,700 --> 00:12:39,690
그리고 에러를 계산할 때는 각각의 layer에 있는 local error 를 계산하고

173
00:12:39,710 --> 00:12:43,350
그 local error 를 감소시키는 방향으로 학습이 이루어지게 됩니다

174
00:12:43,380 --> 00:12:46,930
이와 비슷한 개념으로 PES learning rule 이 제시되었습니다

175
00:12:47,220 --> 00:12:51,080
이 학습방식은 실제 뉴런의 구조를 바탕으로 만들어졌습니다

176
00:12:51,390 --> 00:12:55,850
Hippocampus 에 있는 Pyramidal neuron은 Inhibitory 신호를 전달하는

177
00:12:55,880 --> 00:13:00,990
Interneuron 과 연결이 되어 있어 피드백 신호를 전달받는 것으로 알려져 있습니다

178
00:13:01,290 --> 00:13:04,740
그래서 PES learning rule 은 layer 의 출력값을

179
00:13:04,740 --> 00:13:04,750
Inhibitory neuron 을 통해 음수값으로 전달을 받음으로써
그래서 PES learning rule 은 layer 의 출력값을

180
00:13:04,750 --> 00:13:08,610
Inhibitory neuron 을 통해 음수값으로 전달을 받음으로써

181
00:13:08,640 --> 00:13:12,920
해당 layer 에 에러를 최소화하는 방향으로 학습이 이루어지게 됩니다

182
00:13:13,080 --> 00:13:18,300
그럼 이제 본격적으로 파이썬으로 구현하는 SNN에 대해서 알아보도록 하겠습니다

183
00:13:18,780 --> 00:13:23,400
SNN엔 분야에서는 Tensorflow, Keras 혹은 Pytorch 와 같이

184
00:13:23,430 --> 00:13:27,110
정말로 많이 사용되고 있는 Nengo라고 하는 라이브러리가 있습니다

185
00:13:27,390 --> 00:13:30,980
이 라이브러리는 Spiking neuron 을 만드는 데 특화되어 있습니다

186
00:13:31,130 --> 00:13:35,850
뿐만 아니라 실험의 진행을 GUI를 통해 실시간으로 모니터링도 가능합니다

187
00:13:36,450 --> 00:13:39,170
그리고 다른 딥러닝 라이브러리 못지않게

188
00:13:39,200 --> 00:13:42,830
문서와 예제코드에 대한 내용이 매우 잘 정리되어 있습니다

189
00:13:43,170 --> 00:13:46,230
현재 모두의 연구소 DNL이라고 하는 랩에서

190
00:13:46,250 --> 00:13:49,990
이 Nengo 라이브러리에 대한 한국어 번역작업을 진행하고 있습니다

191
00:13:50,280 --> 00:13:56,350
이번 발표에서는 Nengo 라이브러리를 이용해
SNN을 구현하는 방식들을 소개해 드리도록 하겠습니다

192
00:13:56,850 --> 00:13:58,650
Nengo의 설치는 매우 간단합니다

193
00:13:58,680 --> 00:14:01,360
그냥 pip 인스톨을 통해서 설치해줄면 되고요

194
00:14:01,770 --> 00:14:05,200
Nengo 의 구현은 마치 초기에 Tensorflow 와 유사합니다

195
00:14:05,370 --> 00:14:08,490
그래프를 생성 하듯이 먼저 네트워크를 생성해주고

196
00:14:08,520 --> 00:14:11,450
그 네트워크 내부를 object 로 체워주게 됩니다

197
00:14:11,940 --> 00:14:16,980
현재 화면에 보여지는 예제는 Sine wave 를 출력하는 Node 를 먼저 생성을 했고요

198
00:14:17,010 --> 00:14:21,820
그 Node 를 입력으로 받는 네 개의 Spiking neuron 을 생성을 해주었습니다

199
00:14:22,140 --> 00:14:27,460
이때 두 neuron 인 Connection() 이라고 하는 함수를 통해서
시냅스로 연결을 해주게 됩니다

200
00:14:28,440 --> 00:14:34,320
일반적으로 neuron 은 neuron 으로 들어오는
입력에 대해서 그 발화 패턴이 매우 다양합니다

201
00:14:34,350 --> 00:14:38,640
심지어 같은 발화 패턴을 가진 neuron 이 거의 없다고 해도 과언이 아니죠

202
00:14:39,180 --> 00:14:43,680
Nengo 에서는 neuron 을 그냥 생성을 하게 되면 오른쪽 그림과 같이

203
00:14:43,720 --> 00:14:48,140
Input value 대비 Firing rate 를 random 하게 선택해서 만들어지게 됩니다

204
00:14:48,360 --> 00:14:52,140
물론 이러한 Characteristic 를 코드를 통해서 수정할 수 있습니다

205
00:14:52,530 --> 00:14:58,640
intercepts 라고 하는 변수는 neuron 의 발화가 어느
value 에서 시작이 되는지를 지정해줄 수 있습니다

206
00:14:58,980 --> 00:15:02,130
그래서 가운데는 0으로 지정이 되어 있는데요

207
00:15:02,160 --> 00:15:04,160
이를 5로 설정을 하게 되면

208
00:15:04,180 --> 00:15:06,230
Input value 가 0.5일 때부터

209
00:15:06,260 --> 00:15:09,560
Firing rate 가 Maximum firing rate 까지 증가하게 됩니다

210
00:15:10,110 --> 00:15:13,470
또한 Encoder 라는 변수를 지정을 해줄 수가 있는데요

211
00:15:13,500 --> 00:15:16,980
Encoder 라는 변수를 -1로 지정을 해주게 되면

212
00:15:17,000 --> 00:15:19,890
음수의 값에서 발화가 일어나게 됩니다

213
00:15:19,920 --> 00:15:22,680
그리고 max-rates 라고 하는 변수가 있는데요

214
00:15:22,710 --> 00:15:25,740
이 변수를 지정하게 되면 Input value 대비

215
00:15:25,760 --> 00:15:29,960
Maximum firing rate 가 몇 가지 올라갈 것인지를 지정해줄 수가 있습니다

216
00:15:30,300 --> 00:15:32,430
또한 radius 라고 하는 변수가 있는데요

217
00:15:32,460 --> 00:15:36,810
이 radius 변수는 neuron 이 처음 지정된 Maximum firing rate 까지

218
00:15:36,830 --> 00:15:39,880
어떤 기울기로 올라가는지를 지정해줄 수가 있습니다

219
00:15:40,140 --> 00:15:46,530
그래서 radius 값이 크게 되면 Maximum firing rate 까지
더 낮은 기울기로 증가를 하게 되는 것이죠

220
00:15:47,700 --> 00:15:50,870
다음으로는 Neuron decoding 에 대해서 알아보도록 하겠습니다

221
00:15:51,050 --> 00:15:55,460
Neuron decoding 이란 입력된 신호를 뉴런의 발화 패턴을 통해서

222
00:15:55,490 --> 00:15:58,020
반대로 Decoding 하는 과정을 말합니다

223
00:15:58,050 --> 00:16:01,200
이를 위해서 두 개의 neuron 을 선언을 해보도록 하겠습니다

224
00:16:01,370 --> 00:16:06,780
하나의 neuron 은 Input value 가 양수일 때
그 Firing rate 가 증가하게 만들었고

225
00:16:06,810 --> 00:16:10,350
또 다른 뉴런은 Input value 가 음수일 때

226
00:16:10,380 --> 00:16:13,560
Firing rate 가 증가하도록 선언을 해주었습니다

227
00:16:13,920 --> 00:16:19,260
그리고 처음 설계됐던 네트워크와 마찬가지로 sine 입력을 input 으로 주었습니다

228
00:16:19,290 --> 00:16:23,550
따라서 한 뉴런은 sine 이 음수값을 나타낼 때 발화가 일어나고

229
00:16:23,580 --> 00:16:27,790
또 다른 뉴런은 sine 값이 양수값으로 나타낼 때 발화가 일어날 것입니다

230
00:16:28,200 --> 00:16:31,690
Nengo simulator 는 시간의 흐름에 따라 진행이 되기 때문에

231
00:16:31,710 --> 00:16:36,090
시뮬네이션의 결과 정보를 어떤 object 에 저장을 해줘야 됩니다

232
00:16:36,120 --> 00:16:39,050
그 object 이 바로 Probe() 라고 하는 것입니다

233
00:16:39,870 --> 00:16:44,660
위와 같이 Probe 를 선언을 해주면
시뮬레이션의 동작하는 동안 나온 결과들을

234
00:16:44,690 --> 00:16:47,370
Probe object 에 저장을 하게 됩니다

235
00:16:47,400 --> 00:16:50,350
그래서 다음과 같이 그림을 확인할 수가 있는데요

236
00:16:50,380 --> 00:16:53,820
On neuron 같은 경우에는 양수값을 가질 때 발화하는 neuron

237
00:16:53,850 --> 00:16:56,990
Off neuron 은 음수값을 가질 때 발화하는 neuron 입니다

238
00:16:57,330 --> 00:16:59,870
이 단 두 neuron 의 활성을 바탕으로

239
00:16:59,900 --> 00:17:04,250
오른쪽과 같이 Input signal 에 대한 정보를 decoding 할 수가 있습니다

240
00:17:04,590 --> 00:17:09,410
하지만 단 2개의 neuron 으로는 그 해상도가 좋지 않은 것을 확인할 수가 있습니다

241
00:17:09,630 --> 00:17:12,920
그래서 다음과 같이 백 개의 neuron 을 선언해주면

242
00:17:12,950 --> 00:17:17,370
Input value 에 대한 Decoding 이 성공적으로 잘 이루어진 것을 확인할 수가 있었습니다

243
00:17:17,730 --> 00:17:23,800
SNN으로 들어가는 Input node 를
Input function custom 을 통해서 지정을 해줄 수가 있습니다

244
00:17:23,840 --> 00:17:28,840
이 예제는 MNIST 이미지를 neuron 의 input 으로 넣어주는 건데요

245
00:17:29,030 --> 00:17:33,020
Input function 의 경우 T라고 하는 시간 정보를 입력으로 받게 됩니다

246
00:17:33,220 --> 00:17:39,120
시뮬레이션이 진행되면서 해당 T로는 second 단위의
시간 정보가 입력으로 들어가게 됩니다

247
00:17:39,450 --> 00:17:45,810
그림에서는 1초가 지날 때마다 새로운
이미지를 뱉어내도록 function 에 지정을 해주었는데요

248
00:17:45,840 --> 00:17:48,400
그래서 다음과 같은 결과를 확인할 수가 있습니다

249
00:17:48,810 --> 00:17:53,200
MNIST 데이터는 28곱한 28의 픽셀 데이터로 되어 있기 때문에

250
00:17:53,220 --> 00:17:56,790
저도 28 곱한 28개 뉴런을 선언을 해주었고요

251
00:17:56,820 --> 00:18:01,890
오른쪽 그림을 보시면 Y축은 그 28 곱하기 28개의 뉴런 개수고

252
00:18:01,920 --> 00:18:04,140
오른쪽은 시간을 의미하고 있습니다

253
00:18:04,530 --> 00:18:09,240
각각의 점들은 어떤 특정 neuron 의 발언을 했던 것을 의미하고 있는데요

254
00:18:09,670 --> 00:18:12,890
1초마다 서로 다른 입력이 들어오는 것을 확인할 수가 있습니다

255
00:18:13,350 --> 00:18:16,630
이미지뿐만 아니라 음성신호에 대한 입력도 가능합니다

256
00:18:16,890 --> 00:18:22,990
이 예제는 Mel Spectrogram 으로 변환된 음성신호를
프레임 단위로 전달을 해주는 코드입니다

257
00:18:23,310 --> 00:18:28,560
이 코드를 이용하면 Mel Spectrogram 이미지가
아래와 같이 네트워크로 입력이 들어가며

258
00:18:28,590 --> 00:18:31,150
발화에 영향을 미치는 것을 확인할 수가 있습니다

259
00:18:31,440 --> 00:18:34,710
지금까지는 Nengo 를 활용한 입출력에 대해서 알아보았고

260
00:18:34,740 --> 00:18:38,690
다음으로는 Unsupervised learning 을 어떻게 적용하는지 알아보도록 하겠습니다

261
00:18:38,920 --> 00:18:42,450
Unsupervised learning 을 적용하는 방식은 매우 간단합니다

262
00:18:42,480 --> 00:18:48,040
Nengo 에서는 입력과 출력 layer 에 대한 것을
Connection 이라고 하는 함수를 통해서 연결할 수가 있습니다

263
00:18:48,270 --> 00:18:52,550
이 Connection 함수의 learning rule 타입이라고 하는 변수로 지정을 해주면

264
00:18:52,580 --> 00:18:55,690
해당 시냅스는 특정한 learning rule 을 가지게 됩니다

265
00:18:55,950 --> 00:18:58,780
위 예제는 BCM 러닝으로 작용한 결과고요

266
00:18:59,580 --> 00:19:04,830
시간의 흐름에 따라서 Postsynaptic neuron 의
활성이 조절되는 것을 확인할 수가 있었습니다

267
00:19:05,370 --> 00:19:08,940
마찬가지로 supervised learning 도 쉽게 적용이 가능합니다

268
00:19:08,970 --> 00:19:12,180
먼저 PES learning 이 적용되지 않았을 때 예시입니다

269
00:19:12,780 --> 00:19:16,700
네트워크의 구성은 white noise  를 뱉는 node 와

270
00:19:16,710 --> 00:19:18,820
이 node 에 입력을 받는 input node

271
00:19:18,860 --> 00:19:22,440
그리고 input node 의 결과를 직접 받는 output node 가 있습니다

272
00:19:22,950 --> 00:19:27,630
이때 input neuron 과 output neuron 은 서로
의미 없는 값으로 연결이 되어 있기 때문에

273
00:19:27,660 --> 00:19:32,000
output neuron 에서 input neuron 에 대한 반응을 제대로 확인할 수가 없었습니다

274
00:19:32,760 --> 00:19:38,880
이제 목적은 output neuron 이 input neuron 과
동일한 활성을 보이는 것을 목적으로 두고 있는데요

275
00:19:38,910 --> 00:19:43,410
이를 구현하기 위해서 앞서서 설명드렸던 PES learning 을 활용할 예정입니다

276
00:19:43,590 --> 00:19:46,380
그래서 error neuron 을 따로 설정을 해두었구요

277
00:19:46,620 --> 00:19:50,820
error neuron 의 경우 input 과 output 으로부터 입력을 받게 됩니다

278
00:19:50,850 --> 00:19:55,980
그리고 그 input 과 output 의 값의 차이를 이용해서 에러값을 계산을 하고

279
00:19:56,010 --> 00:20:02,170
그 에러를 최소화하는 방향으로 input 과 output neuron 사이의
Synapse weight 를 조절하게 됩니다

280
00:20:02,700 --> 00:20:06,840
이렇게 PES learning 에 적용이 되었을 때
시간의 흐름에 따라

281
00:20:06,860 --> 00:20:11,730
output neuron 의 활성이 더 input neuron 과 가까워 지는 것을 확인할 수 있었습니다

282
00:20:12,210 --> 00:20:15,470
그리고 두 neuron 출력에 대한 에러 값을 비교해보면

283
00:20:15,500 --> 00:20:20,260
시간의 흐름에 따라서 그 에러가 점점 0로 수렴하는 것을 관찰할 수가 있었습니다

284
00:20:21,080 --> 00:20:26,930
Nengo 의 가장 큰 장점 중 하나는 기존의 Keras 와 같은 딥러닝 라이브러리를

285
00:20:26,970 --> 00:20:30,340
SNN로 컨버팅하는 게 매우 용이하다는 장점이 있습니다

286
00:20:30,930 --> 00:20:36,450
이를 위해서 MNIST 숫자 데이터를 분류하던 Keras 모델을 구축을 해보았고요

287
00:20:36,930 --> 00:20:42,930
다음의 코드와 같이 Keras 모델을 Nengo 의
모델로 컨버팅하는 것이 아주 쉽게 가능합니다

288
00:20:43,350 --> 00:20:47,590
이때 중요한 것은 Keras 모델을 가지고 있는 Activation function인데요

289
00:20:48,000 --> 00:20:51,990
Keras model 의 Activation function 을 ReLU 함수로 만들게 되면

290
00:20:52,020 --> 00:20:56,370
neuron 의 dynamics 역시 ReLU 함수골로 활성 이루어져야 됩니다

291
00:20:56,400 --> 00:21:01,060
즉 input value 에 따라 firing-rate 가 변화하는 neuron dynamics 가

292
00:21:01,080 --> 00:21:04,120
ReLU와 같은 꼴을 가지게 된다는 것이죠

293
00:21:04,910 --> 00:21:08,340
그리고 케라스에서 사용하는 fit 함수를 통해서

294
00:21:08,360 --> 00:21:11,340
컨버팅된 네트워크를 학습할 수가 있습니다

295
00:21:11,850 --> 00:21:14,820
다음의 결과를 보시면 input 이미지에 대해서

296
00:21:14,850 --> 00:21:17,440
위쪽에 있는 layer 가 상위 layer고요

297
00:21:17,460 --> 00:21:21,230
아래쪽에 있는 layer가 끝쪽에 있는 출력 layer와 같습니다

298
00:21:21,450 --> 00:21:25,830
그래서 시간의 흐름에 따라서 layer별로 전파되는 것을 관찰할 수가 있고

299
00:21:25,860 --> 00:21:27,910
마지막 layer에 도달했을 때는

300
00:21:27,920 --> 00:21:31,680
그 input 이미지가 어떤 것으로 분류가 되는지 표현이 됩니다

301
00:21:31,940 --> 00:21:36,100
그렇다면 이런 SNN를 활용한 실제 어플리케이션은 무엇이 있을까요?

302
00:21:36,540 --> 00:21:40,100
2020년에 공개된 사이언스지에 논문에 따르면

303
00:21:40,140 --> 00:21:45,190
기존의 perceptron 혹은 인공신경망에서 해결할 수 없었던  XOR 문제를

304
00:21:45,510 --> 00:21:50,150
단순히 node 의 activation function 만 spike
형태로 변환을 해주었더니

305
00:21:50,180 --> 00:21:52,070
XOR 문제를 해결했다고 합니다

306
00:21:52,590 --> 00:21:57,480
또한 MNIST 데이타 중에는 Permuted sequential MNIST 라는 데이터가 있는데요

307
00:21:57,770 --> 00:22:01,310
이 데이터는 시간의 흐름에 따라서 MNIST에 데이터를

308
00:22:01,330 --> 00:22:06,210
어떻게 쓰게 되는지 그 pathway 정보가 담겨 있는 데이터입니다

309
00:22:06,720 --> 00:22:10,070
따라서 이 데이터는 시간 정보를 포함하고 있는데요

310
00:22:10,440 --> 00:22:14,440
시간에 대한 정보를 처리할 수 있는 RNN이나 LSTM보다

311
00:22:14,470 --> 00:22:18,940
LMU라고 하는 sequential unit이 훨씬 더 좋은 성능을 보였다고 합니다

312
00:22:19,170 --> 00:22:24,660
그리고 LMU를 Spiking neuron network 를
구성을 하였더니 SotA(state-of-the-art)의 성능이 나왔다고 합니다

313
00:22:25,080 --> 00:22:28,370
다음으로 소개해드릴 모델은 SPAUN이라고 하는 모델입니다

314
00:22:28,650 --> 00:22:34,760
이 모델은 총 250만 개나 LIF neuron 을 이용해서 모델링을 진행을 했고요

315
00:22:34,900 --> 00:22:39,600
뇌에 존재하는 7개 구역에 뇌 정보처리기관들을

316
00:22:39,630 --> 00:22:42,120
모사를 해서 시뮬레이션의 진행을 하였습니다

317
00:22:42,390 --> 00:22:46,780
이렇게 시뮬레이션을 했을 때 오른쪽에 있는 8개
테스크에 대해서

318
00:22:46,800 --> 00:22:51,460
동일한 네트워크 구조를 이용을 해서 모두 성공적으로 수행했다고 합니다

319
00:22:53,070 --> 00:22:56,660
앞으로의 Spiking neuron network 는 어떤 의미를 가지고 있을까요?

320
00:22:57,180 --> 00:23:01,470
신경망이 Neuromorphic 하다는 것은 다양한 장점을 가질 수가 있습니다

321
00:23:01,500 --> 00:23:06,070
Spiking neuron 의 neuron model 들은 각각의 sparse 하게 동작을 합니다

322
00:23:06,420 --> 00:23:11,880
즉 각 neuron 의 커뮤니케이션 자체가
기존에 ANN에 비해서 더 적다는 걸 의미합니다

323
00:23:12,180 --> 00:23:16,220
그 말은 lookup 메모리의 양이 점점 더 줄어든다는 장점이 있고

324
00:23:16,380 --> 00:23:19,850
그리고 이 네트워크는 단순히 Integration 을 하고

325
00:23:19,880 --> 00:23:22,140
Fire 하는 방향으로 만들어지기 때문에

326
00:23:22,170 --> 00:23:26,200
컴퓨터이션에 대한 파워가 적게 필요하다는 장점이 있습니다

327
00:23:26,790 --> 00:23:29,490
또한 최근 많은 하드웨어 기업들에서

328
00:23:29,510 --> 00:23:32,780
이 Neuromorphic Spiking neuron network 를 더 빠른 속도로

329
00:23:32,810 --> 00:23:38,000
더 효과적으로 처리할 수 있는 Neuromorphic processing unit 들을 만들어내고 있습니다

330
00:23:38,430 --> 00:23:42,030
특히 인텔의 Loihi 칩이라고 하는 것이 대표적입니다

331
00:23:42,060 --> 00:23:46,500
이 칩은 Nengo 로 만들어진 코드와의 융합이 매우 용이하게 이루어집니다

332
00:23:47,010 --> 00:23:51,040
이런 Neuromorphic chip 은 병렬화를 극대화했다는 장점이 있습니다

333
00:23:51,090 --> 00:23:54,480
한 예로 앞서서 소개드렸던 SPAUN 모델의 경우

334
00:23:54,510 --> 00:23:59,830
CPU를 사용할 때는 1초를 시뮬레이션하는데
약이 2.5시간의 시간이 걸렸다고 합니다

335
00:24:00,150 --> 00:24:02,750
반면에 인텔의 Loihi 칩을 사용을 하면

336
00:24:02,770 --> 00:24:06,960
실시간으로 그 반응성을 확인할 수 있을 정도로 매우 빠르다고 합니다

337
00:24:07,200 --> 00:24:09,750
또한 인텔에서 공개한 자료를 확인해보면

338
00:24:09,780 --> 00:24:13,160
엔비디아와 같은 GPU 장비를 사용했을 때 대비

339
00:24:13,180 --> 00:24:17,130
훨씬 더 에너지 효율적인 결과를 확인할 수 있었다고 합니다

340
00:24:17,520 --> 00:24:20,390
SNN를 활용을 하게 되면 더 적은 메모리

341
00:24:20,420 --> 00:24:24,420
더 적은 에너지를 사용해서 정보처리를 할 수 있다는 장점뿐만 아니라

342
00:24:24,450 --> 00:24:29,160
실제 생물학적으로 테스트하기 힘든 실험도 진행할 수 있다는 장점이 있습니다

343
00:24:29,700 --> 00:24:34,720
예를 들어서 왼쪽의 그림 같은 경우에는 SPAUN 모델에 있는 LIF neuron 을

344
00:24:34,770 --> 00:24:40,170
실제 3D로 모델링된 neural model 을 활용을 해서 시뮬레이션을 한 결과입니다

345
00:24:40,200 --> 00:24:43,980
이 모델의 경우 neuron 의 이온 채널들도 다 붙어 있고요

346
00:24:44,010 --> 00:24:50,430
그 neuron cable theory 도 다 적용이 돼서
neuron 의 활성들을 실제와 같이 모사할 수가 있습니다

347
00:24:50,460 --> 00:24:56,100
예를 들어서서 sodium ion channel 이 학습에 어떠한 영향을 미치는지 연구하기 위해서

348
00:24:56,130 --> 00:25:01,770
이런 인공뇌 시뮬레이터를 개발하고 컴퓨터로 이온 채널을 차단을 함으로써

349
00:25:01,800 --> 00:25:05,190
그 학습이 어떻게 변화되는지를 관찰할 수가 있습니다

350
00:25:05,260 --> 00:25:11,860
또한 현재 기술로는 수많은 neuron들의
활성을 일일이다 관찰하는 것은 불가능합니다

351
00:25:12,550 --> 00:25:17,520
그래서 오른쪽의 그림과 같이 각 neuron의 비율에 따른 활성 변화라던가

352
00:25:17,550 --> 00:25:19,800
혹은 어떤 약물을 주입했을 때 변화되는

353
00:25:19,830 --> 00:25:26,010
발화 패턴 등을 이렇게 시뮬레이션을 통해서 관찰할 수가 있습니다

354
00:25:26,210 --> 00:25:30,350
이처럼 SNN을 사용하면 생물학적으로 실험을 불가능한 것도

355
00:25:30,370 --> 00:25:33,510
컴퓨터 모델링을 통해서 실험을 할 수가 있습니다

356
00:25:33,540 --> 00:25:38,880
그래서 SNN을 만드는 아이디어는 실제 생물학적 실험을 바탕으로 나오고

357
00:25:38,910 --> 00:25:43,260
실제 생물학적으로 실험이 불가능한 걸 SNN로 실험을 함으로써

358
00:25:43,350 --> 00:25:49,500
선순환적인 구조가 이뤄지며 더욱 더
Neuroscience 가 발전을 할 수가 있을 것입니다

359
00:25:49,520 --> 00:25:52,870
이상으로 발표를 마치겠습니다

360
00:25:52,910 --> 00:25:55,950
SNN과 관련된 대화는 언제나 환영합니다

361
00:25:55,980 --> 00:25:58,710
아래 메일을 통해서 연락 부탁드릴게요 감사합니다

