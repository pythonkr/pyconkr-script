1
00:00:10,031 --> 00:00:14,420
Hello, thank you for having
me at PyCon Korea 2020.
2
00:00:09.945   -->   00:00:13.396
It's truly my honor to
present the following topic; 

3
00:00:13.421   -->   00:00:16.974
Your Escape Plan
from Numpy and Cython. 

4
00:00:16.999   -->   00:00:21.098
Here is some of my information.
My name is Chen-Lin Yang 

5
00:00:21.123   -->   00:00:24.879
and you can find me on
GitHub C-L-Y-A-N-G. 

6
00:00:24.904   -->   00:00:27.605
I'm a Taiwanese
and live in Taipei now 

7
00:00:27.630   -->   00:00:32.300
Currently working for a cyber security
company called CyCraft Japan, 

8
00:00:32.325   -->   00:00:35.214
and I'm also a member of
the Machine Learning Team. 

9
00:00:35.239   -->   00:00:40.479
So basically, all my daily life is dealing
with millions of logs, generated by 

10
00:00:40.504   -->   00:00:44.257
a lot of endpoints
in different countries. 

11
00:00:44.282   -->   00:00:48.853
Okay, before we start, here's
one very quick question for you. 

12
00:00:48.878   -->   00:00:54.469
Let's say you have a very large Numpy
array, which contains a lot of floating 

13
00:00:54.494   -->   00:00:59.717
points and you want each element
to multiply itself eight times, 

14
00:00:59.742   -->   00:01:02.849
which of the following code do
you think will run faster? 

15
00:01:02.849   -->   00:01:09.735
Is it A, the power function of Numpy,
or B, X times times 8, 

16
00:01:09.760   -->   00:01:15.210
or C, the most naive one X times X,
and then repeat four times? 

17
00:01:15.249   -->   00:01:18.070
I'll give you three seconds
to think about it. 

18
00:01:21.771   -->   00:01:25.925
The answer might surprise you, is C, the
most naive one. 

19
00:01:25.945   -->   00:01:29.561
You just use X times X,
and then repeat four times. 

20
00:01:29.561   -->   00:01:34.444
In order to prove it, here's the result
of my benchmark code. 

21
00:01:34.444   -->   00:01:39.409
As you can see the power function
of Numpy and X**8. 

22
00:01:39.409   -->   00:01:42.640
They both took about two seconds
 

23
00:01:42.640   -->   00:01:48.616
and therefore the most naive one
 X*X repeat four times, 

24
00:01:48.617   -->   00:01:52.556
and then you can see
it only takes 0.4 seconds. 

25
00:01:52.581   -->   00:01:57.835
So, it's about five times faster
than previous two, but there is a case. 

26
00:01:57.860   -->   00:02:01.444
So, if, unless you are doing
the scientific computing, 

27
00:02:01.469   -->   00:02:07.147
which require very, very high accuracy,
otherwise the result of the last one 

28
00:02:07.172   -->   00:02:11.160
will be a very small difference
than the previous two. 

29
00:02:11.185   -->   00:02:16.860
The difference is like 10
to number of power, negative 20. 

30
00:02:16.885   -->   00:02:22.118
So, I believe in most case you will
just choose the final-- last solution, 

31
00:02:22.143   -->   00:02:26.064
X*X, and then repeat four times.
 

32
00:02:26.089   -->   00:02:30.086
Okay. Let's move on.
Why not Cython? 

33
00:02:30.111   -->   00:02:34.007
If you ever have
the Numpy code performance issue 

34
00:02:34.032   -->   00:02:38.046
and ask your friends, colleagues,
or teachers, how to improve it, 

35
00:02:38.071   -->   00:02:43.210
the most likely the answer you will get
is like, “Why don't you try on Cython?” 

36
00:02:43.235   -->   00:02:47.586
So, in the slide, I will give some
pros and cons regarding to it. 

37
00:02:47.611   -->   00:02:52.761
First, let's take a look at
advantage. If you are using Cython 

38
00:02:52.786   -->   00:02:57.401
and of course you can utilize
a lot of 3rd party C libraries, 

39
00:02:57.426   -->   00:03:03.299
and normally C code runs much,
much faster than the Python code. 

40
00:03:03.324   -->   00:03:10.075
And also, Cython allows you to release
the Global Interpreter lock or code GIL, 

41
00:03:10.100   -->   00:03:13.366
which means that your market-rated program
 

42
00:03:13.391   -->   00:03:17.638
can be run much, much
faster than the existing one. 

43
00:03:17.663   -->   00:03:21.818
And also you can still have the
run-time check for the common problem 

44
00:03:21.843   -->   00:03:27.083
like the boundary check or some
error handling, and which will be handled 

45
00:03:27.108   -->   00:03:29.121
by the upper layer Python.
 

46
00:03:29.146   -->   00:03:33.816
And then last, the Cython
syntax is very similar to Python, 

47
00:03:33.841   -->   00:03:37.424
meaning that you can learn it
in a very short amount of time, 

48
00:03:37.449   -->   00:03:40.338
but also it has some disadvantage.
 

49
00:03:40.363   -->   00:03:45.188
The first thing I can think is that you
have to handle the memory by yourself. 

50
00:03:45.213   -->   00:03:49.000
So, for example, if you are
using malloc in your Cython code, 

51
00:03:49.025   -->   00:03:55.004
and then you have to use this memory
by yourself and freely after all, 

52
00:03:55.029   -->   00:03:57.614
and you have to do it all by your own.
 

53
00:03:57.614   -->   00:04:03.066
Otherwise it might cause some very
serious memory leak. 

54
00:04:03.091   -->   00:04:09.557
And also, to get the ultimate performance
with Cython, to write a C code 

55
00:04:09.582   -->   00:04:12.730
and the low level intrinsic cannot
be avoided. 

56
00:04:12.730   -->   00:04:14.674
So, no matter what,
 

57
00:04:14.674   -->   00:04:18.868
if you want to get the best
performance, you have to write a C code. 

58
00:04:19.722   -->   00:04:23.876
And trust me, it's really
painful. It's just something like this. 

59
00:04:23.901   -->   00:04:29.215
And why do I know it? Because
the code you see is written by myself. 

60
00:04:29.240   -->   00:04:33.422
Great. Let's have a look
to today's example. 

61
00:04:33.447   -->   00:04:37.664
This example we'll use as a
benchmark for the rest of the talk. 

62
00:04:37.689   -->   00:04:39.007
So, let's have a look.
 

63
00:04:39.032   -->   00:04:45.329
If you are not new to machine learning,
you must know what SoftMax function is. 

64
00:04:45.354   -->   00:04:49.772
The formula is just like the one
on the screen, and don't worry, 

65
00:04:49.797   -->   00:04:52.920
I'm not going to explain
it in a mathematical way. 

66
00:04:52.945   -->   00:04:59.334
All you have to know is that it's a number
divided by the sum of a lot of numbers 

67
00:04:59.359   -->   00:05:00.841
and that's enough.
 

68
00:05:00.866   -->   00:05:05.739
And you might think the formula
itself looks very easy and it shouldn't be 

69
00:05:05.764   -->   00:05:09.949
very hard to implement in
Python. And yes, you are correct. 

70
00:05:09.974   -->   00:05:15.112
But if you try to implement with the
formula on this screen, you will easily 

71
00:05:15.137   -->   00:05:20.019
encounter a numerical problem.
In Computer Science, we call it 

72
00:05:20.044   -->   00:05:22.566
an underflow or overflow issue.
 

73
00:05:22.591   -->   00:05:27.723
And on the next slide, I will
give you an underflow example. 

74
00:05:27.748   -->   00:05:30.637
So, to avoid this kind of situation,
 

75
00:05:30.662   -->   00:05:35.527
a trick called the log-sum
exponential will be used. 

76
00:05:35.552   -->   00:05:39.903
Now here's the example of underflow.
 

77
00:05:39.928   -->   00:05:46.899
Let's say the sum of this power
of the denominator is 134217728 

78
00:05:46.924   -->   00:05:50.297
and the last part is its reciprocal.
 

79
00:05:50.322   -->   00:05:54.922
And what will happen if we
are trying to add them together? 

80
00:05:54.947   -->   00:06:02.625
Here's the benchmark code. As you
can see, I set variable A to 134217728 

81
00:06:02.650   -->   00:06:08.624
and then try to plus it with its
reciprocal and here's the result. 

82
00:06:08.649   -->   00:06:14.177
As you can see, it's exactly the
same as A. It's quite shocking, right? 

83
00:06:14.202   -->   00:06:17.885
This effect is called underflow.
 

84
00:06:17.910   -->   00:06:20.736
You might think this
is not a very big deal, 

85
00:06:20.761   -->   00:06:25.305
but if you're doing a scientific
calculation, like Markoff Chan the input 

86
00:06:25.330   -->   00:06:31.667
data is a list of very small probability.
And just like my example, on the slide, 

87
00:06:31.724   -->   00:06:36.762
after a specific position, the
probability will not be counted, 

88
00:06:36.787   -->   00:06:42.384
which will cause a very large
error in your final result. 

89
00:06:42.409   -->   00:06:47.345
So, as I mentioned in the previous
page, this problem can be solved 

90
00:06:47.370   -->   00:06:50.242
by a simple trick called
LogSum Exponential, 

91
00:06:50.267   -->   00:06:54.985
and the formula is just like
the one I show on the slide. 

92
00:06:55.010   -->   00:06:59.304
And the result of this formula
will mathematically equivalent 

93
00:06:59.329   -->   00:07:01.164
to the softmax function.
 

94
00:07:01.189   -->   00:07:05.242
And don't worry again, because
I'm not going to prove it in the talk, 

95
00:07:05.267   -->   00:07:09.297
but the answer can be
easily found on the internet. 

96
00:07:09.322   -->   00:07:15.054
So, in order to prove it-- it really
works, here is another example code. 

97
00:07:15.079   -->   00:07:18.139
As you can see the final result over here,
 

98
00:07:18.164   -->   00:07:24.004
you can see the very little difference
between this one and the previous one. 

99
00:07:24.029   -->   00:07:31.622
So, the smaller value is not cut or
avoided, which will result in a more, 

100
00:07:31.647   -->   00:07:36.525
more accurate result
to your final answers. 

101
00:07:36.550   -->   00:07:41.485
Some of you might ask,
SciPy already has this function, 

102
00:07:41.510   -->   00:07:45.370
Why would you rebuild your own wheel?
The answer to that 

103
00:07:45.395   -->   00:07:49.229
is SciPy’s function is built
for general purpose usage, 

104
00:07:49.254   -->   00:07:53.986
meaning it will perform a lot of checks
to make sure input and output data 

105
00:07:54.011   -->   00:07:59.816
is well handled, but in a real-world
scenario, you know what your data is. 

106
00:07:59.841   -->   00:08:03.900
So, you can rewrite the
function based on your data 

107
00:08:03.925   -->   00:08:09.165
Not only you can remove a lot
of checks but also it gives you 

108
00:08:09.190   -->   00:08:13.470
an opportunity to apply
third party boosting solution 

109
00:08:13.495   -->   00:08:16.149
to improve your code performance.
 

110
00:08:16.174   -->   00:08:21.248
Today's case, I will assume
there's the input data will be only 

111
00:08:21.273   -->   00:08:26.591
one-dimensional array, and then I will
try to apply a three different solution 

112
00:08:26.616   -->   00:08:30.769
to the example I just mentioned before.
 

113
00:08:30.794   -->   00:08:32.730
Okay, let's moving on.
 

114
00:08:32.755   -->   00:08:36.886
How do we implement
LogSum Exponential in Numpy? 

115
00:08:36.911   -->   00:08:42.015
As I mentioned before, all my input
data will just be one dimensional array. 

116
00:08:42.040   -->   00:08:46.273
So, LogSum Exponential
can be implemented as follows. 

117
00:08:46.298   -->   00:08:52.538
As you can see, it only took three lines
of code to finish. It's quite easy right? 

118
00:08:52.563   -->   00:08:56.477
Here. I'm going to leave five
seconds to you to view the code. 

119
00:09:01.752   -->   00:09:03.234
Time is up.
 

120
00:09:03.259   -->   00:09:06.687
So, after finish our
own version in NumPy, 

121
00:09:06.712   -->   00:09:11.007
I wrote a very simple benchmark
code to compare our own version versus 

122
00:09:11.032   -->   00:09:15.335
SciPy's original function.
The result is on the right. 

123
00:09:15.360   -->   00:09:19.773
As you can see, our version
is about 0.5 second faster 

124
00:09:19.798   -->   00:09:22.328
than the original SciPy's function.
 

125
00:09:22.353   -->   00:09:26.809
You might think it's only 0.4 second,
it's not a lot. 

126
00:09:26.809   -->   00:09:30.881
But if you need to code
this function a million times a day, 

127
00:09:30.881   -->   00:09:34.106
 it will save you 
about half million seconds. 

128
00:09:34.131   -->   00:09:35.990
That's quite a lot right?
 

129
00:09:37.699   -->   00:09:41.590
Starting from here, I'm going
to introduce three different types 

130
00:09:41.615   -->   00:09:45.277
of solutions that might improve
your Numpy performance. 

131
00:09:45.302   -->   00:09:48.027
The first solution will be CuPy.
 

132
00:09:48.052   -->   00:09:53.064
CuPy is an open source project
developed by a Japanese company. 

133
00:09:53.089   -->   00:09:56.720
It provides NumPy compatible
ND-array on CUDA. 

134
00:09:56.745   -->   00:09:59.843
So, you can utilize all your GPU powers.
 

135
00:09:59.868   -->   00:10:05.272
And also it is compatible with existing
CUDA kernel, meaning that you don't have 

136
00:10:05.297   -->   00:10:08.970
to waste your existing file, you just
need to import it 

137
00:10:08.970   -->   00:10:11.701
and then CUDA will run it for you.
 

138
00:10:11.726   -->   00:10:15.640
It also provides many NumPy
equivalent functions. 

139
00:10:15.640   -->   00:10:18.692
So you don't have
to change your code a lot 

140
00:10:18.717   -->   00:10:22.106
and minimize the code refactoring effect.
 

141
00:10:22.131   -->   00:10:28.651
But still, please remember CuPy
and NumPy has some difference. 

142
00:10:28.676   -->   00:10:32.785
Please check on CuPy's website
to find the details of difference 

143
00:10:32.810   -->   00:10:34.940
between CuPy and NumPy.
 

144
00:10:34.965   -->   00:10:40.744
And one last thing, moving the data
between CPU and GPU is very expensive. 

145
00:10:40.769   -->   00:10:45.506
Please use it wisely. Otherwise, it
might not improve your performance, 

146
00:10:45.531   -->   00:10:48.727
but drag your performance a lot.
 

147
00:10:48.752   -->   00:10:53.719
So, how do we implement
logsum exponential in CuPy? 

148
00:10:53.744   -->   00:10:58.235
It's quite easy, all you have
to do is import CuPy as CP 

149
00:10:58.260   -->   00:11:04.945
and then use the function
underlined here, cp.array to convert the 

150
00:11:04.970   -->   00:11:12.271
existing NumPy array to CuPy array,
and then CuPy does the metric for you. 

151
00:11:12.296   -->   00:11:17.654
And here's the final result,
as you can see our CuPy version, 

152
00:11:17.679   -->   00:11:22.101
the running time of our CuPy
version is just 1.6 seconds. 

153
00:11:22.126   -->   00:11:29.499
So, it's from like 6.5 seconds to 1.6
seconds. It's pretty amazing, right? 

154
00:11:29.524   -->   00:11:33.025
Now let's move on to the
next solution, Numba. 

155
00:11:33.050   -->   00:11:36.273
Numba is a very popular open source
project 

156
00:11:36.273   -->   00:11:39.715
that can boost your NumPy
and Python performance. 

157
00:11:39.715   -->   00:11:43.871
It is also backed
by many large companies and organizations. 

158
00:11:43.871   -->   00:11:48.533
The approach Numba uses
is called Just-In-Time technology. 

159
00:11:48.558   -->   00:11:53.587
Basically, it’s just translates a
subset of Python and NumPy code into 

160
00:11:53.612   -->   00:11:56.570
low-level machine code and runs faster.
 

161
00:11:56.595   -->   00:12:00.406
It also utilizes both
CPU and GPU power. 

162
00:12:00.431   -->   00:12:05.742
One of the best things Numba has
is that it supports OpenMP, which means 

163
00:12:05.767   -->   00:12:11.920
that you can improve your market
ready program much, much faster. 

164
00:12:11.945   -->   00:12:16.705
The highlight of Numba is that it
provides near zero code modification. 

165
00:12:16.730   -->   00:12:22.820
All you have to do is to put a decorator,
@jit before the function you want to do 

166
00:12:22.845   -->   00:12:26.882
you want it to speed up
and Numba will take care of it. 

167
00:12:26.907   -->   00:12:32.780
And currently Numba only works
best with the functions not classes. 

168
00:12:32.805   -->   00:12:38.193
So, if you want to accelerate your
classes please do not use Numba. 

169
00:12:38.218   -->   00:12:41.844
Otherwise, you might
have a lot of issues with that. 

170
00:12:41.869   -->   00:12:47.657
And Numba also has a very large user
community, which means that if you have 

171
00:12:47.682   -->   00:12:52.256
any problems, you can find
the answer on the stack overflow. 

172
00:12:52.281   -->   00:12:56.323
When talking about Numba, there
are two modes you need to know. 

173
00:12:56.348   -->   00:12:58.439
The first one is nopython mode.
 

174
00:12:58.464   -->   00:13:03.311
So in this mode Numba allows you
to get rid of Python’s GIL 

175
00:13:03.311   -->   00:13:07.943
 so you can get
the most performance from it. 

176
00:13:07.968   -->   00:13:12.075
But not every function
is supported in nopython mode. 

177
00:13:12.114   -->   00:13:16.372
So sometimes you have to
switch back to the object mode. 

178
00:13:16.397   -->   00:13:20.104
In order to know which function is
supporting which mode, 

179
00:13:20.104   -->   00:13:23.309
 you have to look up
the document in Numba’s website. 

180
00:13:23.334   -->   00:13:29.051
Luckily, the document itself is
very thorough, you shouldn't have 

181
00:13:29.076   -->   00:13:33.506
any problem to find out
which one works in which mode. 

182
00:13:33.539   -->   00:13:39.592
Finally, if you want to use OpenMP,
you must use it with the no JIT mode. 

183
00:13:39.617   -->   00:13:43.302
So basically, you only can
use it in the nopython mode. 

184
00:13:43.327   -->   00:13:46.794
And with that, here's
the sample code of the 

185
00:13:46.819   -->   00:13:49.497
OpenMP code, as you can see,
 

186
00:13:49.522   -->   00:13:55.318
all I have to do is just
change the prange to prange, 

187
00:13:55.343   -->   00:14:00.843
and then Numba will recognize this
and then try to parallelize my code. 

188
00:14:00.868   -->   00:14:03.459
So, there's just one character change
 

189
00:14:03.484   -->   00:14:08.030
and then I can get the most
performance out of Numba. 

190
00:14:08.055   -->   00:14:13.244
Now let's see how to implement
logsum exponential by Numba. 

191
00:14:13.269   -->   00:14:15.826
Here's the example I wrote.
 

192
00:14:15.851   -->   00:14:20.128
As you can remember, when
you're looking at the code, 

193
00:14:20.153   -->   00:14:24.780
it's basically almost the same as
the one I implemented in NumPy. 

194
00:14:24.805   -->   00:14:29.413
Yes, that's correct, they
are exactly the same. 

195
00:14:29.438   -->   00:14:32.701
The only difference between
this one and the previous one 

196
00:14:32.726   -->   00:14:35.826
is that there are two
different decorators I put. 

197
00:14:35.859   -->   00:14:42.349
So, the first one is @jit and the
second one is @njit and 

198
00:14:42.374   -->   00:14:46.786
everything else is almost the
same, and here's the result. 

199
00:14:46.811   -->   00:14:51.504
As you can see, when using
the nopython mode with Numba, 

200
00:14:51.529   -->   00:14:56.872
the performance I get is about six seconds
but when switching back to the JIT mode, 

201
00:14:56.897   -->   00:15:02.332
it’s about 6.7 seconds.
I think it's about 0.2 second 

202
00:15:02.357   -->   00:15:05.473
slower than the original SciPy functions.
 

203
00:15:05.498   -->   00:15:07.934
So, here's the very big difference.
 

204
00:15:07.959   -->   00:15:12.941
If your function cannot
accelerate by Numba, sometimes 

205
00:15:12.966   -->   00:15:17.652
it will give you a worse result.
So be careful with that. 

206
00:15:17.677   -->   00:15:21.418
Now let's move on to the
last solution, Pythran. 

207
00:15:21.443   -->   00:15:27.491
Pythran is a relativity new open source
project written by a French developer. 

208
00:15:27.516   -->   00:15:33.648
It's under very active development as
it has a very fast-growing community. 

209
00:15:33.673   -->   00:15:39.289
Unlike Numba, which uses
just-in-time technology, Pythran uses 

210
00:15:39.314   -->   00:15:41.811
ahead-of-time compiling approach.
 

211
00:15:41.836   -->   00:15:47.709
Basically, it means that it will
translate a part of your Python and NumPy 

212
00:15:47.734   -->   00:15:55.263
code into C++ and then utilize modern
compiler to compile and optimize your code 

213
00:15:55.288   -->   00:16:01.250
into a very efficient low-level
machine code, and it runs faster. 

214
00:16:01.275   -->   00:16:05.934
It also supports a subset of
Python and NumPy functions 

215
00:16:05.959   -->   00:16:12.363
and works on Python 2.7
and Python 3.6 to 3.8. 

216
00:16:12.388   -->   00:16:17.214
Just like Numba, all you have to do
is put a very special decorator 

217
00:16:17.239   -->   00:16:19.752
before the function you want to boost
 

218
00:16:19.777   -->   00:16:23.366
and Pythran will take
the rest work for you. 

219
00:16:23.391   -->   00:16:26.037
It also supports OpenMP,
 

220
00:16:26.037   -->   00:16:32.903
so you can utilize the full power
of a market-rated in your code. 

221
00:16:32.928   -->   00:16:37.731
So, how do we implement
logsum exponential in Pythran? 

222
00:16:37.756   -->   00:16:40.605
To use Pythran, normally
you need two steps. 

223
00:16:40.630   -->   00:16:46.725
First, you just write a Python code
as usual, so here is my sample code. 

224
00:16:46.750   -->   00:16:51.918
The only thing you have to do is put
a very special decorator over here 

225
00:16:51.943   -->   00:16:58.058
is #pythran export and then
your function now, and then 

226
00:16:58.083   -->   00:17:01.926
the data type of the input data.
 

227
00:17:01.951   -->   00:17:04.953
And that's pretty much everything
you have to do, 

228
00:17:04.953   -->   00:17:09.144
and then the rest
is just like the one in NumPy. 

229
00:17:09.169   -->   00:17:13.355
After you finish editing the
file, all you have to do is compile. 

230
00:17:13.380   -->   00:17:18.379
You can use this very, very
long  code over here. 

231
00:17:18.404   -->   00:17:25.608
Or you can take an easier one. You
just use Pythran and then-- which part? 

232
00:17:25.633   -->   00:17:29.217
Over here, Pythran
and then your file name 

233
00:17:29.242   -->   00:17:34.490
and you will use all the default
argument to compile your code. 

234
00:17:34.515   -->   00:17:38.801
So, once you finish your code
and compile your code without any 

235
00:17:38.826   -->   00:17:44.097
compiling error roll, then
the second step is to import 

236
00:17:44.122   -->   00:17:48.838
the compiled module to the file.
So, you just create another file 

237
00:17:48.863   -->   00:17:53.851
and then import, just compile the
module over here, so here's the example. 

238
00:17:53.876   -->   00:17:59.826
And then all you have to do is just code
a function like you used to do in Python 

239
00:17:59.851   -->   00:18:02.364
so here is the code.
 

240
00:18:02.389   -->   00:18:09.009
And the code will successful executed.
Here is the result of the Pythran. 

241
00:18:09.034   -->   00:18:14.795
As you can see, the result is much, much
faster than the NumPy’s nopython mode. 

242
00:18:14.820   -->   00:18:18.522
Sorry, it’s Numba’s nopython mode.
 

243
00:18:18.547   -->   00:18:26.336
It only took 5.45 seconds to finish, it's
about one and half seconds faster than 

244
00:18:26.361   -->   00:18:28.254
Numba’s nopython mode.
 

245
00:18:28.279   -->   00:18:33.504
So, if you want to get the full power
of your CPU, 

246
00:18:33.504   -->   00:18:39.145
please consider to use
Pythran to get the ultimate performance. 

247
00:18:39.170   -->   00:18:44.214
So, that’s it, I just introduced three
different approaches to improve your 

248
00:18:44.239   -->   00:18:47.060
Python and NumPy’s code performance.
 

249
00:18:47.060   -->   00:18:50.838
So, right now, you might want to ask,
which is better? 

250
00:18:50.863   -->   00:18:56.211
Before we compare, I have to
tell you all the benchmark I ran 

251
00:18:56.236   -->   00:19:01.333
in the previous line is on a bare metal
machine with the following specifications. 

252
00:19:01.358   -->   00:19:05.263
The CPU I use is an Intel Xeon CPU.
 

253
00:19:05.288   -->   00:19:09.373
And it comes with 256GB of RAM.
 

254
00:19:09.398   -->   00:19:15.122
It also has a GeForce GTX 1080 Ti GPU.
 

255
00:19:15.147   -->   00:19:22.908
The Python version I used is 3.6.9 on
Ubuntu and all the other libraries 

256
00:19:22.933   -->   00:19:27.134
and version information you
can find in this slide. 

257
00:19:27.159   -->   00:19:33.267
In this slide, I just put all previous
benchmark results into the same bar chart. 

258
00:19:33.292   -->   00:19:36.376
So, you can compare
the results by yourself. 

259
00:19:36.401   -->   00:19:39.267
The answer itself is quite clear.
 

260
00:19:39.292   -->   00:19:45.477
If you have single or multiple
GPU, CuPy will be your best choice 

261
00:19:45.502   -->   00:19:51.839
because when you see the original function
in SciPy took about 6.7 seconds to finish. 

262
00:19:51.839   -->   00:19:55.075
CuPy only needed 1.6 seconds.
 

263
00:19:55.100   -->   00:20:00.130
So, that's about five times
faster. It's just a no-brainer. 

264
00:20:00.155   -->   00:20:04.002
However, if you don't have any GPU,
don't worry. 

265
00:20:04.002   -->   00:20:05.965
You still have the other options.
 

266
00:20:06.117   -->   00:20:11.404
So, as I mentioned before,
original SciPy function took 6.7. 

267
00:20:11.429   -->   00:20:15.710
In this scenario, I will
suggest you to try Numba first. 

268
00:20:15.735   -->   00:20:21.718
Because Numba supports more
NumPy solutions than Pythran. 

269
00:20:21.743   -->   00:20:27.672
So, after you give your Numba a try,
you will find that in no JIT mode, 

270
00:20:27.697   -->   00:20:35.652
Numba can successfully compile an SQ, and
it will give you about 0.7 seconds faster 

271
00:20:35.679   -->   00:20:40.049
and from now on you can
give the Pythran another try. 

272
00:20:40.292   -->   00:20:44.599
So if the same function also
is supported in Pythran, 

273
00:20:44.599   -->   00:20:48.444
then it can give you 
extra performance boost. 

274
00:20:48.469   -->   00:20:53.162
In my case, Pythran can
work in my benchmark code. 

275
00:20:53.187   -->   00:20:58.688
So, the total execution time
of Pythran is 5.4 seconds. 

276
00:20:58.713   -->   00:21:02.282
So, by comparing to 
the original SciPy's version, 

277
00:21:02.282   -->   00:21:08.352
it's about 20% or 25% faster
than original solution. 

278
00:21:08.377   -->   00:21:14.682
That’s not bad for just change a few lines
of code and get extra 20% of performance 

279
00:21:14.707   -->   00:21:18.357
I can call  you it a day.
 

280
00:21:18.382   -->   00:21:23.747
At the end, I would like to show
you my own decision tree to CuPy, 

281
00:21:23.772   -->   00:21:26.091
Numba and Pythran.
 

282
00:21:26.116   -->   00:21:30.709
Here is my decision tree. So,
the first question starting with, 

283
00:21:30.734   -->   00:21:32.660
do you have GPU?
 

284
00:21:32.685   -->   00:21:36.576
If you do have GPU then the
follow up question will be, 

285
00:21:36.601   -->   00:21:42.240
do you need to use CUDA kernel or do you
need to implement your own CUDA kernel? 

286
00:21:42.265   -->   00:21:48.044
If the answer is still yes, then I will
strongly recommend you to use CuPy. 

287
00:21:48.069   -->   00:21:52.278
If the answer is no, then
the follow up question will be, 

288
00:21:52.303   -->   00:21:58.297
do you also have
the CPU computation task with your GPU? 

289
00:21:58.322   -->   00:22:02.951
If the answer is yes, then
Numba will be a better choice. 

290
00:22:02.976   -->   00:22:06.241
Otherwise, please stay with CuPy.
 

291
00:22:06.266   -->   00:22:10.695
If you don't have any GPU,
you still have options, right? 

292
00:22:10.720   -->   00:22:14.407
So, the question itself will
be very, very easy. 

293
00:22:14.407   -->   00:22:17.334
Do you like to deal with compiler?
 

294
00:22:17.335   -->   00:22:22.036
So, if the answer is no, then I
would suggest that you use Numba. 

295
00:22:22.061   -->   00:22:26.605
So, it will only give you
very minimal compiling errors. 

296
00:22:26.630   -->   00:22:32.260
And if you are a geek and you like
to deal with the compiler, compiling flags 

297
00:22:32.285   -->   00:22:37.509
and all the compiling error logs, and
then of course Pythran can give you 

298
00:22:37.534   -->   00:22:39.657
the most of these.
 

299
00:22:39.682   -->   00:22:46.936
And also, if you choose Pythran as in
the example I demo in a previous page 

300
00:22:46.936   -->   00:22:49.227
and also in my real work,
 

301
00:22:49.227   -->   00:22:55.229
Pythran can almost always give me
the most CPU power, all of it. 

302
00:22:55.229   -->   00:22:59.900
So, I will suggest that if you
want to achieve the best performance 

303
00:22:59.925   -->   00:23:04.523
with CPU, Pythran
will be your best choice. 

304
00:23:04.548   -->   00:23:09.773
Finally, at the end of this, I would
like to provide three takeaways for you. 

305
00:23:09.798   -->   00:23:16.366
First, if you have GPU please try CuPy
first. It's really easy to use CuPy. 

306
00:23:16.391   -->   00:23:22.682
Just import CuPy as CP, and then
code NumPy’s functions with CuPy. 

307
00:23:22.707   -->   00:23:24.887
But there's one thing
you have to remember, 

308
00:23:25.885   -->   00:23:29.681
please remember to convert
NumPy’s array to CuPy’s array, 

309
00:23:29.681   -->   00:23:36.396
so CuPy can help you to move the data from
physical memory to graphic card memory. 

310
00:23:36.421   -->   00:23:40.735
Second, if you only have
GPU, that's totally fine 

311
00:23:40.735   -->   00:23:42.433
because you still have options.
 

312
00:23:42.458   -->   00:23:46.230
In this case, I will suggest
you to use Numba first, 

313
00:23:46.255   -->   00:23:50.394
because Numba supports more
NumPy functions than Pythran 

314
00:23:50.419   -->   00:23:57.589
and I would strongly recommend
you to use Numba in its nopython mode. 

315
00:23:57.614   -->   00:24:03.013
If your code runs smoothly
with Numba in nopython mode, 

316
00:24:03.038   -->   00:24:09.585
then I would suggest that you try 
Pythran to get more performance. 

317
00:24:09.617   -->   00:24:16.454
In my company's case, we can get the
most CPU power with Pythran every time. 

318
00:24:16.479   -->   00:24:19.277
So, it's very worth to give it a try.
 

319
00:24:19.302   -->   00:24:25.386
Finally, each solution support
different numbers of NumPy functions. 

320
00:24:25.411   -->   00:24:30.433
Luckily, each solution has a very
well written and maintained website. 

321
00:24:30.458   -->   00:24:36.718
So you can easily find the support
functions this on each solutions website. 

322
00:24:36.743   -->   00:24:38.389
So just give it a look.
 

323
00:24:38.414   -->   00:24:42.901
Also, it's fairly easy to find out which
function doesn't work 

324
00:24:42.901   -->   00:24:45.636
because normally your program will just die
 

325
00:24:45.636   -->   00:24:50.527
 if the function is not supported
by the proposed solution. 

326
00:24:50.921   -->   00:24:55.139
And also, if A doesn’t work, B might
work. 

327
00:24:55.139   -->   00:25:00.922
So, if for example, if Pythran doesn't work,
you can always give Numba a try. 

328
00:25:01.088   -->   00:25:05.758
And in reality, especially during my work,
 

329
00:25:05.783   -->   00:25:12.555
I always try to use a mixture
of these three proposed methods. 

330
00:25:12.580   -->   00:25:18.937
So, sometimes I will use Numba
with Pythran or CuPy with Pythran. 

331
00:25:18.962   -->   00:25:25.752
So, it's all up to you. I will suggest
that you to give a mixture solution a try. 

332
00:25:25.777   -->   00:25:28.742
So, that's pretty much all my talk.
 

333
00:25:28.767   -->   00:25:33.804
Thank you. And I hope you stay
safe during the COVID-19 pandemic.


