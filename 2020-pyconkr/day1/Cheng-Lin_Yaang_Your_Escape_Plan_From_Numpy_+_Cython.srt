1
00:00:10,031 --> 00:00:14,420
Hello, thank you for having
me at PyCon Korea 2020.

2
00:00:14.945   -->   00:00:18.396
It's truly my honor to
present the following topic; 

3
00:00:18.421   -->   00:00:21.974
Your Escape Plan
from Numpy and Cython. 

4
00:00:21.999   -->   00:00:26.098
Here is some of my information.
My name is Chen-Lin Yang 

5
00:00:26.123   -->   00:00:29.879
and you can find me on
GitHub C-L-Y-A-N-G. 

6
00:00:29.904   -->   00:00:32.605
I'm a Taiwanese
and live in Taipei now 

7
00:00:32.630   -->   00:00:37.300
Currently working for a cyber security
company called CyCraft Japan, 

8
00:00:37.325   -->   00:00:40.214
and I'm also a member of
the Machine Learning Team. 

9
00:00:40.239   -->   00:00:45.479
So basically, all my daily life is dealing
with millions of logs, generated by 

10
00:00:45.504   -->   00:00:49.257
a lot of endpoints
in different countries. 

11
00:00:49.282   -->   00:00:53.853
Okay, before we start, here's
one very quick question for you. 

12
00:00:53.878   -->   00:00:59.469
Let's say you have a very large Numpy
array, which contains a lot of floating 

13
00:00:59.494   -->   00:01:04.717
points and you want each element
to multiply itself eight times, 

14
00:01:04.742   -->   00:01:07.849
which of the following code do
you think will run faster? 

15
00:01:07.849   -->   00:01:14.735
Is it A, the power function of Numpy,
or B, X times times 8, 

16
00:01:14.760   -->   00:01:20.210
or C, the most naive one X times X,
and then repeat four times? 

17
00:01:20.249   -->   00:01:23.070
I'll give you three seconds
to think about it. 

18
00:01:23.771   -->   00:01:30.925
The answer might surprise you, is C, the
most naive one. 

19
00:01:30.945   -->   00:01:34.561
You just use X times X,
and then repeat four times. 

20
00:01:34.561   -->   00:01:39.444
In order to prove it, here's the result
of my benchmark code. 

21
00:01:39.444   -->   00:01:44.409
As you can see the power function
of Numpy and X**8. 

22
00:01:44.409   -->   00:01:47.640
They both took about two seconds
 
23
00:01:47.640   -->   00:01:53.616
and therefore the most naive one
 X*X repeat four times, 

24
00:01:53.617   -->   00:01:57.556
and then you can see
it only takes 0.4 seconds. 

25
00:01:57.581   -->   00:02:02.835
So, it's about five times faster
than previous two, but there is a case. 

26
00:02:02.860   -->   00:02:06.444
So, if, unless you are doing
the scientific computing, 

27
00:02:06.469   -->   00:02:12.147
which require very, very high accuracy,
otherwise the result of the last one 

28
00:02:12.172   -->   00:02:16.160
will be a very small difference
than the previous two. 

29
00:02:16.185   -->   00:02:21.860
The difference is like 10
to number of power, negative 20. 

30
00:02:21.885   -->   00:02:27.118
So, I believe in most case you will
just choose the final-- last solution, 

31
00:02:27.143   -->   00:02:31.064
X*X, and then repeat four times.
 
32
00:02:31.089   -->   00:02:35.086
Okay. Let's move on.
Why not Cython? 

33
00:02:35.111   -->   00:02:39.007
If you ever have
the Numpy code performance issue 

34
00:02:39.032   -->   00:02:43.046
and ask your friends, colleagues,
or teachers, how to improve it, 

35
00:02:43.071   -->   00:02:48.210
the most likely the answer you will get
is like, “Why don't you try on Cython?” 

36
00:02:48.235   -->   00:02:52.586
So, in the slide, I will give some
pros and cons regarding to it. 

37
00:02:52.611   -->   00:02:57.761
First, let's take a look at
advantage. If you are using Cython 

38
00:02:57.786   -->   00:03:02.401
and of course you can utilize
a lot of 3rd party C libraries, 

39
00:03:02.426   -->   00:03:07.299
and normally C code runs much,
much faster than the Python code. 

40
00:03:07.324   -->   00:03:15.075
And also, Cython allows you to release
the Global Interpreter lock or code GIL, 

41
00:03:15,100 --> 00:03:18,366
which means that your market-rated program

42
00:03:18,391 --> 00:03:22,638
can be run much, much
faster than the existing one.

43
00:03:22,663 --> 00:03:26,818
And also you can still have the
run-time check for the common problem

44
00:03:26,843 --> 00:03:32,083
like the boundary check or some
error handling, and which will be handled

45
00:03:32,108 --> 00:03:34,121
by the upper layer Python.


46
00:03:34,146 --> 00:03:38,816
And then last, the Cython
syntax is very similar to Python,

47
00:03:38,841 --> 00:03:42,424
meaning that you can learn it
in a very short amount of time,

48
00:03:42,449 --> 00:03:45,338
but also it has some disadvantage.


49
00:03:45,363 --> 00:03:50,188
The first thing I can think is that you
have to handle the memory by yourself.

50
00:03:50,213 --> 00:03:54,000
So, for example, if you are
using malloc in your Cython code,

51
00:03:54,025 --> 00:04:00,004
and then you have to use this memory
by yourself and freely after all,

52
00:04:00,029 --> 00:04:02,614
and you have to do it all by your own.


53
00:04:02,614 --> 00:04:08,066
Otherwise it might cause some very
serious memory leak.

54
00:04:08,091 --> 00:04:14,557
And also, to get the ultimate performance
with Cython, to write a C code

55
00:04:14,582 --> 00:04:17,730
and the low level intrinsic cannot
be avoided.

56
00:04:17,730 --> 00:04:19,674
So, no matter what,
 

57
00:04:19,674 --> 00:04:23,868
if you want to get the best
performance, you have to write a C code.

58
00:04:24,722 --> 00:04:28,876
And trust me, it's really
painful. It's just something like this.

59
00:04:28,901 --> 00:04:34,215
And why do I know it? Because
the code you see is written by myself.

60
00:04:34,240 --> 00:04:38,422
Great. Let's have a look
to today's example.

61
00:04:38,447 --> 00:04:42,664
This example we'll use as a
benchmark for the rest of the talk.

62
00:04:42,689 --> 00:04:44,007
So, let's have a look.


63
00:04:44,032 --> 00:04:50,329
If you are not new to machine learning,
you must know what SoftMax function is.

64
00:04:50,354 --> 00:04:54,772
The formula is just like the one
on the screen, and don't worry,

65
00:04:54,797 --> 00:04:57,920
I'm not going to explain
it in a mathematical way.

66
00:04:57,945 --> 00:05:04,334
All you have to know is that it's a number
divided by the sum of a lot of numbers

67
00:05:04,359 --> 00:05:05,841
and that's enough.


68
00:05:05,866 --> 00:05:10,739
And you might think the formula
itself looks very easy and it shouldn't be

69
00:05:10,764 --> 00:05:14,949
very hard to implement in
Python. And yes, you are correct.

70
00:05:14,974 --> 00:05:20,112
But if you try to implement with the
formula on this screen, you will easily

71
00:05:20,137 --> 00:05:25,019
encounter a numerical problem.
In Computer Science, we call it

72
00:05:25,044 --> 00:05:27,566
an underflow or overflow issue.


73
00:05:27,591 --> 00:05:32,723
And on the next slide, I will
give you an underflow example.

74
00:05:32,748 --> 00:05:35,637
So, to avoid this kind of situation,


75
00:05:35,662 --> 00:05:40,527
a trick called the log-sum
exponential will be used.

76
00:05:40,552 --> 00:05:44,903
Now here's the example of underflow.


77
00:05:44,928 --> 00:05:51,899
Let's say the sum of this power
of the denominator is 134217728

78
00:05:51,924 --> 00:05:55,297
and the last part is its reciprocal.


79
00:05:55,322 --> 00:05:59,922
And what will happen if we
are trying to add them together?

80
00:05:59,947 --> 00:06:07,625
Here's the benchmark code. As you
can see, I set variable A to 134217728

81
00:06:07,650 --> 00:06:13,624
and then try to plus it with its
reciprocal and here's the result.

82
00:06:13,649 --> 00:06:19,177
As you can see, it's exactly the
same as A. It's quite shocking, right?

83
00:06:19,202 --> 00:06:22,885
This effect is called underflow.


84
00:06:22,910 --> 00:06:25,736
You might think this
is not a very big deal,

85
00:06:25,761 --> 00:06:30,305
but if you're doing a scientific
calculation, like Markoff Chan the input

86
00:06:30,330 --> 00:06:36,667
data is a list of very small probability.
And just like my example, on the slide,

87
00:06:36,724 --> 00:06:41,762
after a specific position, the
probability will not be counted,

88
00:06:41,787 --> 00:06:47,384
which will cause a very large
error in your final result.

89
00:06:47,409 --> 00:06:52,345
So, as I mentioned in the previous
page, this problem can be solved

90
00:06:52,370 --> 00:06:55,242
by a simple trick called
LogSum Exponential,

91
00:06:55,267 --> 00:06:59,985
and the formula is just like
the one I show on the slide.

92
00:07:00,010 --> 00:07:04,304
And the result of this formula
will mathematically equivalent

93
00:07:04,329 --> 00:07:06,164
to the softmax function.


94
00:07:06,189 --> 00:07:10,242
And don't worry again, because
I'm not going to prove it in the talk,

95
00:07:10,267 --> 00:07:14,297
but the answer can be
easily found on the internet.

96
00:07:14,322 --> 00:07:20,054
So, in order to prove it-- it really
works, here is another example code.

97
00:07:20,079 --> 00:07:23,139
As you can see the final result over here,


98
00:07:23,164 --> 00:07:29,004
you can see the very little difference
between this one and the previous one.

99
00:07:29,029 --> 00:07:36,622
So, the smaller value is not cut or
avoided, which will result in a more,

100
00:07:36,647 --> 00:07:41,525
more accurate result
to your final answers.

101
00:07:41,550 --> 00:07:46,485
Some of you might ask,
SciPy already has this function,

102
00:07:46,510 --> 00:07:50,370
Why would you rebuild your own wheel?
The answer to that

103
00:07:50,395 --> 00:07:54,229
is SciPy's function is built
for general purpose usage,

104
00:07:54,254 --> 00:07:58,986
meaning it will perform a lot of checks
to make sure input and output data

105
00:07:59,011 --> 00:08:04,816
is well handled, but in a real-world
scenario, you know what your data is.

106
00:08:04,841 --> 00:08:08,900
So, you can rewrite the
function based on your data

107
00:08:08,925 --> 00:08:14,165
Not only you can remove a lot
of checks but also it gives you

108
00:08:14,190 --> 00:08:18,470
an opportunity to apply
third party boosting solution

109
00:08:18,495 --> 00:08:21,149
to improve your code performance.


110
00:08:21,174 --> 00:08:26,248
Today's case, I will assume
there's the input data will be only

111
00:08:26,273 --> 00:08:31,591
one-dimensional array, and then I will
try to apply a three different solution

112
00:08:31,616 --> 00:08:35,769
to the example I just mentioned before.
 

113
00:08:35,794 --> 00:08:37,730
Okay, let's moving on.


114
00:08:37,755 --> 00:08:41,886
How do we implement
LogSum Exponential in Numpy?

115
00:08:41,911 --> 00:08:47,015
As I mentioned before, all my input
data will just be one dimensional array.

116
00:08:47,040 --> 00:08:51,273
So, LogSum Exponential
can be implemented as follows.

117
00:08:51,298 --> 00:08:57,538
As you can see, it only took three lines
of code to finish. It's quite easy right?

118
00:08:57,563 --> 00:09:01,477
Here. I'm going to leave five
seconds to you to view the code.

119
00:09:06,752 --> 00:09:08,234
Time is up.


120
00:09:08,259 --> 00:09:11,687
So, after finish our
own version in NumPy,

121
00:09:11,712 --> 00:09:16,007
I wrote a very simple benchmark
code to compare our own version versus

122
00:09:16,032 --> 00:09:20,335
SciPy's original function.
The result is on the right.

123
00:09:20,360 --> 00:09:24,773
As you can see, our version
is about 0.5 second faster

124
00:09:24,798 --> 00:09:27,328
than the original SciPy's function.


125
00:09:27,353 --> 00:09:31,809
You might think it's only 0.4 second,
it's not a lot.

126
00:09:31,809 --> 00:09:35,881
But if you need to code
this function a million times a day,

127
00:09:35,881 --> 00:09:39,106
 it will save you
about half million seconds.

128
00:09:39,131 --> 00:09:40,990
That's quite a lot right?


129
00:09:42,699 --> 00:09:46,590
Starting from here, I'm going
to introduce three different types

130
00:09:46,615 --> 00:09:50,277
of solutions that might improve
your Numpy performance.

131
00:09:50,302 --> 00:09:53,027
The first solution will be CuPy.


132
00:09:53,052 --> 00:09:58,064
CuPy is an open source project
developed by a Japanese company.

133
00:09:58,089 --> 00:10:01,720
It provides NumPy compatible
ND-array on CUDA.

134
00:10:01,745 --> 00:10:04,843
So, you can utilize all your GPU powers.


135
00:10:04,868 --> 00:10:10,272
And also it is compatible with existing
CUDA kernel, meaning that you don't have

136
00:10:10,297 --> 00:10:13,970
to waste your existing file, you just
need to import it

137
00:10:13,970 --> 00:10:16,701
and then CUDA will run it for you.


138
00:10:16,726 --> 00:10:20,640
It also provides many NumPy
equivalent functions.

139
00:10:20,640 --> 00:10:23,692
So you don't have
to change your code a lot

140
00:10:23,717 --> 00:10:27,106
and minimize the code refactoring effect.


141
00:10:27,131 --> 00:10:33,651
But still, please remember CuPy
and NumPy has some difference.

142
00:10:33,676 --> 00:10:37,785
Please check on CuPy's website
to find the details of difference

143
00:10:37,810 --> 00:10:39,940
between CuPy and NumPy.


144
00:10:39,965 --> 00:10:45,744
And one last thing, moving the data
between CPU and GPU is very expensive.

145
00:10:45,769 --> 00:10:50,506
Please use it wisely. Otherwise, it
might not improve your performance,

146
00:10:50,531 --> 00:10:53,727
but drag your performance a lot.


147
00:10:53,752 --> 00:10:58,719
So, how do we implement
logsum exponential in CuPy?

148
00:10:58,744 --> 00:11:03,235
It's quite easy, all you have
to do is import CuPy as CP

149
00:11:03,260 --> 00:11:09,945
and then use the function
underlined here, cp.array to convert the

150
00:11:09,970 --> 00:11:17,271
existing NumPy array to CuPy array,
and then CuPy does the metric for you.

151
00:11:17,296 --> 00:11:22,654
And here's the final result,
as you can see our CuPy version,

152
00:11:22,679 --> 00:11:27,101
the running time of our CuPy
version is just 1.6 seconds.

153
00:11:27,126 --> 00:11:34,499
So, it's from like 6.5 seconds to 1.6
seconds. It's pretty amazing, right?

154
00:11:34,524 --> 00:11:38,025
Now let's move on to the
next solution, Numba.

155
00:11:38,050 --> 00:11:41,273
Numba is a very popular open source
project

156
00:11:41,273 --> 00:11:44,715
that can boost your NumPy
and Python performance.

157
00:11:44,715 --> 00:11:48,871
It is also backed
by many large companies and organizations.

158
00:11:48,871 --> 00:11:53,533
The approach Numba uses
is called Just-In-Time technology.

159
00:11:53,558 --> 00:11:58,587
Basically, it's just translates a
subset of Python and NumPy code into

160
00:11:58,612 --> 00:12:01,570
low-level machine code and runs faster.


161
00:12:01,595 --> 00:12:05,406
It also utilizes both
CPU and GPU power.

162
00:12:05,431 --> 00:12:10,742
One of the best things Numba has
is that it supports OpenMP, which means

163
00:12:10,767 --> 00:12:16,920
that you can improve your market
ready program much, much faster.

164
00:12:16,945 --> 00:12:21,705
The highlight of Numba is that it
provides near zero code modification.

165
00:12:21,730 --> 00:12:27,820
All you have to do is to put a decorator,
@jit before the function you want to do

166
00:12:27,845 --> 00:12:31,882
you want it to speed up
and Numba will take care of it.

167
00:12:31,907 --> 00:12:37,780
And currently Numba only works
best with the functions not classes.

168
00:12:37,805 --> 00:12:43,193
So, if you want to accelerate your
classes please do not use Numba.

169
00:12:43,218 --> 00:12:46,844
Otherwise, you might
have a lot of issues with that.

170
00:12:46,869 --> 00:12:52,657
And Numba also has a very large user
community, which means that if you have

171
00:12:52,682 --> 00:12:57,256
any problems, you can find
the answer on the stack overflow.

172
00:12:57,281 --> 00:13:01,323
When talking about Numba, there
are two modes you need to know.

173
00:13:01,348 --> 00:13:03,439
The first one is nopython mode.


174
00:13:03,464 --> 00:13:08,311
So in this mode Numba allows you
to get rid of Python's GIL

175
00:13:08,311 --> 00:13:12,943
 so you can get
the most performance from it.

176
00:13:12,968 --> 00:13:17,075
But not every function
is supported in nopython mode.

177
00:13:17,114 --> 00:13:21,372
So sometimes you have to
switch back to the object mode.

178
00:13:21,397 --> 00:13:25,104
In order to know which function is
supporting which mode,

179
00:13:25,104 --> 00:13:28,309
 you have to look up
the document in Numba's website.

180
00:13:28,334 --> 00:13:34,051
Luckily, the document itself is
very thorough, you shouldn't have

181
00:13:34,076 --> 00:13:38,506
any problem to find out
which one works in which mode.

182
00:13:38,539 --> 00:13:44,592
Finally, if you want to use OpenMP,
you must use it with the no JIT mode.

183
00:13:44,617 --> 00:13:48,302
So basically, you only can
use it in the nopython mode.

184
00:13:48,327 --> 00:13:51,794
And with that, here's
the sample code of the

185
00:13:51,819 --> 00:13:54,497
OpenMP code, as you can see,


186
00:13:54,522 --> 00:14:00,318
all I have to do is just
change the prange to prange,

187
00:14:00,343 --> 00:14:05,843
and then Numba will recognize this
and then try to parallelize my code.

188
00:14:05,868 --> 00:14:08,459
So, there's just one character change


189
00:14:08,484 --> 00:14:13,030
and then I can get the most
performance out of Numba.

190
00:14:13,055 --> 00:14:18,244
Now let's see how to implement
logsum exponential by Numba.

191
00:14:18,269 --> 00:14:20,826
Here's the example I wrote.


192
00:14:20,851 --> 00:14:25,128
As you can remember, when
you're looking at the code,

193
00:14:25,153 --> 00:14:29,780
it's basically almost the same as
the one I implemented in NumPy.

194
00:14:29,805 --> 00:14:34,413
Yes, that's correct, they
are exactly the same.

195
00:14:34,438 --> 00:14:37,701
The only difference between
this one and the previous one

196
00:14:37,726 --> 00:14:40,826
is that there are two
different decorators I put.

197
00:14:40,859 --> 00:14:47,349
So, the first one is @jit and the
second one is @njit and

198
00:14:47,374 --> 00:14:51,786
everything else is almost the
same, and here's the result.

199
00:14:51,811 --> 00:14:56,504
As you can see, when using
the nopython mode with Numba,

200
00:14:56,529 --> 00:15:01,872
the performance I get is about six seconds
but when switching back to the JIT mode,

201
00:15:01,897 --> 00:15:07,332
it's about 6.7 seconds.
I think it's about 0.2 second

202
00:15:07,357 --> 00:15:10,473
slower than the original SciPy functions.


203
00:15:10,498 --> 00:15:12,934
So, here's the very big difference.


204
00:15:12,959 --> 00:15:17,941
If your function cannot
accelerate by Numba, sometimes

205
00:15:17,966 --> 00:15:22,652
it will give you a worse result.
So be careful with that.

206
00:15:22,677 --> 00:15:26,418
Now let's move on to the
last solution, Pythran.

207
00:15:26,443 --> 00:15:32,491
Pythran is a relativity new open source
project written by a French developer.

208
00:15:32,516 --> 00:15:38,648
It's under very active development as
it has a very fast-growing community.

209
00:15:38,673 --> 00:15:44,289
Unlike Numba, which uses
just-in-time technology, Pythran uses

210
00:15:44,314 --> 00:15:46,811
ahead-of-time compiling approach.


211
00:15:46,836 --> 00:15:52,709
Basically, it means that it will
translate a part of your Python and NumPy

212
00:15:52,734 --> 00:16:00,263
code into C++ and then utilize modern
compiler to compile and optimize your code

213
00:16:00,288 --> 00:16:06,250
into a very efficient low-level
machine code, and it runs faster.

214
00:16:06,275 --> 00:16:10,934
It also supports a subset of
Python and NumPy functions

215
00:16:10,959 --> 00:16:17,363
and works on Python 2.7
and Python 3.6 to 3.8.

216
00:16:17,388 --> 00:16:22,214
Just like Numba, all you have to do
is put a very special decorator

217
00:16:22,239 --> 00:16:24,752
before the function you want to boost


218
00:16:24,777 --> 00:16:28,366
and Pythran will take
the rest work for you.

219
00:16:28,391 --> 00:16:31,037
It also supports OpenMP,


220
00:16:31,037 --> 00:16:37,903
so you can utilize the full power
of a market-rated in your code.

221
00:16:37,928 --> 00:16:42,731
So, how do we implement
logsum exponential in Pythran?

222
00:16:42,756 --> 00:16:45,605
To use Pythran, normally
you need two steps.

223
00:16:45,630 --> 00:16:51,725
First, you just write a Python code
as usual, so here is my sample code.

224
00:16:51,750 --> 00:16:56,918
The only thing you have to do is put
a very special decorator over here

225
00:16:56,943 --> 00:17:03,058
is #pythran export and then
your function now, and then

226
00:17:03,083 --> 00:17:06,926
the data type of the input data.


227
00:17:06,951 --> 00:17:09,953
And that's pretty much everything
you have to do,

228
00:17:09,953 --> 00:17:14,144
and then the rest
is just like the one in NumPy.

229
00:17:14,169 --> 00:17:18,355
After you finish editing the
file, all you have to do is compile.

230
00:17:18,380 --> 00:17:23,379
You can use this very, very
long  code over here.

231
00:17:23,404 --> 00:17:30,608
Or you can take an easier one. You
just use Pythran and then-- which part?

232
00:17:30,633 --> 00:17:34,217
Over here, Pythran
and then your file name

233
00:17:34,242 --> 00:17:39,490
and you will use all the default
argument to compile your code.

234
00:17:39,515 --> 00:17:43,801
So, once you finish your code
and compile your code without any

235
00:17:43,826 --> 00:17:49,097
compiling error roll, then
the second step is to import

236
00:17:49,122 --> 00:17:53,838
the compiled module to the file.
So, you just create another file

237
00:17:53,863 --> 00:17:58,851
and then import, just compile the
module over here, so here's the example.

238
00:17:58,876 --> 00:18:04,826
And then all you have to do is just code
a function like you used to do in Python

239
00:18:04,851 --> 00:18:07,364
so here is the code.


240
00:18:07,389 --> 00:18:14,009
And the code will successful executed.
Here is the result of the Pythran.

241
00:18:14,034 --> 00:18:19,795
As you can see, the result is much, much
faster than the NumPy's nopython mode.

242
00:18:19,820 --> 00:18:23,522
Sorry, it's Numba's nopython mode.


243
00:18:23,547 --> 00:18:31,336
It only took 5.45 seconds to finish, it's
about one and half seconds faster than

244
00:18:31,361 --> 00:18:33,254
Numba's nopython mode.


245
00:18:33,279 --> 00:18:38,504
So, if you want to get the full power
of your CPU,

246
00:18:38,504 --> 00:18:44,145
please consider to use
Pythran to get the ultimate performance.

247
00:18:44,170 --> 00:18:49,214
So, that's it, I just introduced three
different approaches to improve your

248
00:18:49,239 --> 00:18:52,060
Python and NumPy's code performance.


249
00:18:52,060 --> 00:18:55,838
So, right now, you might want to ask,
which is better?

250
00:18:55,863 --> 00:19:01,211
Before we compare, I have to
tell you all the benchmark I ran

251
00:19:01,236 --> 00:19:06,333
in the previous line is on a bare metal
machine with the following specifications.

252
00:19:06,358 --> 00:19:10,263
The CPU I use is an Intel Xeon CPU.


253
00:19:10,288 --> 00:19:14,373
And it comes with 256GB of RAM.


254
00:19:14,398 --> 00:19:20,122
It also has a GeForce GTX 1080 Ti GPU.


255
00:19:20,147 --> 00:19:27,908
The Python version I used is 3.6.9 on
Ubuntu and all the other libraries

256
00:19:27,933 --> 00:19:32,134
and version information you
can find in this slide.

257
00:19:32,159 --> 00:19:38,267
In this slide, I just put all previous
benchmark results into the same bar chart.

258
00:19:38,292 --> 00:19:41,376
So, you can compare
the results by yourself.

259
00:19:41,401 --> 00:19:44,267
The answer itself is quite clear.


260
00:19:44,292 --> 00:19:50,477
If you have single or multiple
GPU, CuPy will be your best choice

261
00:19:50,502 --> 00:19:56,839
because when you see the original function
in SciPy took about 6.7 seconds to finish.

262
00:19:56,839 --> 00:20:00,075
CuPy only needed 1.6 seconds.


263
00:20:00,100 --> 00:20:05,130
So, that's about five times
faster. It's just a no-brainer.

264
00:20:05,155 --> 00:20:09,002
However, if you don't have any GPU,
don't worry.

265
00:20:09,002 --> 00:20:10,965
You still have the other options.


266
00:20:11,117 --> 00:20:16,404
So, as I mentioned before,
original SciPy function took 6.7.

267
00:20:16,429 --> 00:20:20,710
In this scenario, I will
suggest you to try Numba first.

268
00:20:20,735 --> 00:20:26,718
Because Numba supports more
NumPy solutions than Pythran.

269
00:20:26,743 --> 00:20:32,672
So, after you give your Numba a try,
you will find that in no JIT mode,

270
00:20:32,697 --> 00:20:40,652
Numba can successfully compile an SQ, and
it will give you about 0.7 seconds faster

271
00:20:40,679 --> 00:20:45,049
and from now on you can
give the Pythran another try.

272
00:20:45,292 --> 00:20:49,599
So if the same function also
is supported in Pythran,

273
00:20:49,599 --> 00:20:53,444
then it can give you
extra performance boost.

274
00:20:53,469 --> 00:20:58,162
In my case, Pythran can
work in my benchmark code.

275
00:20:58,187 --> 00:21:03,688
So, the total execution time
of Pythran is 5.4 seconds.

276
00:21:03,713 --> 00:21:07,282
So, by comparing to
the original SciPy's version,

277
00:21:07,282 --> 00:21:13,352
it's about 20% or 25% faster
than original solution.

278
00:21:13,377 --> 00:21:19,682
That's not bad for just change a few lines
of code and get extra 20% of performance

279
00:21:19,707 --> 00:21:23,357
I can call  you it a day.


280
00:21:23,382 --> 00:21:28,747
At the end, I would like to show
you my own decision tree to CuPy,

281
00:21:28,772 --> 00:21:31,091
Numba and Pythran.


282
00:21:31,116 --> 00:21:35,709
Here is my decision tree. So,
the first question starting with,

283
00:21:35,734 --> 00:21:37,660
do you have GPU?


284
00:21:37,685 --> 00:21:41,576
If you do have GPU then the
follow up question will be,

285
00:21:41,601 --> 00:21:47,240
do you need to use CUDA kernel or do you
need to implement your own CUDA kernel?

286
00:21:47,265 --> 00:21:53,044
If the answer is still yes, then I will
strongly recommend you to use CuPy.

287
00:21:53,069 --> 00:21:57,278
If the answer is no, then
the follow up question will be,

288
00:21:57,303 --> 00:22:03,297
do you also have
the CPU computation task with your GPU?

289
00:22:03,322 --> 00:22:07,951
If the answer is yes, then
Numba will be a better choice.

290
00:22:07,976 --> 00:22:11,241
Otherwise, please stay with CuPy.


291
00:22:11,266 --> 00:22:15,695
If you don't have any GPU,
you still have options, right?

292
00:22:15,720 --> 00:22:19,407
So, the question itself will
be very, very easy.

293
00:22:19,407 --> 00:22:22,334
Do you like to deal with compiler?


294
00:22:22,335 --> 00:22:27,036
So, if the answer is no, then I
would suggest that you use Numba.

295
00:22:27,061 --> 00:22:31,605
So, it will only give you
very minimal compiling errors.

296
00:22:31,630 --> 00:22:37,260
And if you are a geek and you like
to deal with the compiler, compiling flags

297
00:22:37,285 --> 00:22:42,509
and all the compiling error logs, and
then of course Pythran can give you

298
00:22:42,534 --> 00:22:44,657
the most of these.


299
00:22:44,682 --> 00:22:51,936
And also, if you choose Pythran as in
the example I demo in a previous page

300
00:22:51,936 --> 00:22:54,227
and also in my real work,


301
00:22:54,227 --> 00:23:00,229
Pythran can almost always give me
the most CPU power, all of it.

302
00:23:00,229 --> 00:23:04,900
So, I will suggest that if you
want to achieve the best performance

303
00:23:04,925 --> 00:23:09,523
with CPU, Pythran
will be your best choice.

304
00:23:09,548 --> 00:23:14,773
Finally, at the end of this, I would
like to provide three takeaways for you.

305
00:23:14,798 --> 00:23:21,366
First, if you have GPU please try CuPy
first. It's really easy to use CuPy.

306
00:23:21,391 --> 00:23:27,682
Just import CuPy as CP, and then
code NumPy's functions with CuPy.

307
00:23:27,707 --> 00:23:29,887
But there's one thing
you have to remember,

308
00:23:30,885 --> 00:23:34,681
please remember to convert
NumPy's array to CuPy's array,

309
00:23:34,681 --> 00:23:41,396
so CuPy can help you to move the data from
physical memory to graphic card memory.

310
00:23:41,421 --> 00:23:45,735
Second, if you only have
GPU, that's totally fine

311
00:23:45,735 --> 00:23:47,433
because you still have options.


312
00:23:47,458 --> 00:23:51,230
In this case, I will suggest
you to use Numba first,

313
00:23:51,255 --> 00:23:55,394
because Numba supports more
NumPy functions than Pythran

314
00:23:55,419 --> 00:24:02,589
and I would strongly recommend
you to use Numba in its nopython mode.

315
00:24:02,614 --> 00:24:08,013
If your code runs smoothly
with Numba in nopython mode,

316
00:24:08,038 --> 00:24:14,585
then I would suggest that you try
Pythran to get more performance.

317
00:24:14,617 --> 00:24:21,454
In my company's case, we can get the
most CPU power with Pythran every time.

318
00:24:21,479 --> 00:24:24,277
So, it's very worth to give it a try.


319
00:24:24,302 --> 00:24:30,386
Finally, each solution support
different numbers of NumPy functions.

320
00:24:30,411 --> 00:24:35,433
Luckily, each solution has a very
well written and maintained website.

321
00:24:35,458 --> 00:24:41,718
So you can easily find the support
functions this on each solutions website.

322
00:24:41,743 --> 00:24:43,389
So just give it a look.


323
00:24:43,414 --> 00:24:47,901
Also, it's fairly easy to find out which
function doesn't work

324
00:24:47,901 --> 00:24:50,636
because normally your program will just die


325
00:24:50,636 --> 00:24:55,527
 if the function is not supported
by the proposed solution.

326
00:24:55,921 --> 00:25:00,139
And also, if A doesn't work, B might
work.

327
00:25:00,139 --> 00:25:05,922
So, if for example, if Pythran doesn't work,
you can always give Numba a try.

328
00:25:06,088 --> 00:25:10,758
And in reality, especially during my work,


329
00:25:10,783 --> 00:25:17,555
I always try to use a mixture
of these three proposed methods.

330
00:25:17,580 --> 00:25:23,937
So, sometimes I will use Numba
with Pythran or CuPy with Pythran.

331
00:25:23,962 --> 00:25:30,752
So, it's all up to you. I will suggest
that you to give a mixture solution a try.

332
00:25:30,777 --> 00:25:33,742
So, that's pretty much all my talk.


333
00:25:33,767 --> 00:25:38,804
Thank you. And I hope you stay
safe during the COVID-19 pandemic.


