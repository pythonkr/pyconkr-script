1
00:00:10,220 --> 00:00:15,790
안녕하세요 청각장애인을 위한 발음교정 앱 개발기를
발표하게 된 희아 유어 보이스입니다

2
00:00:18,340 --> 00:00:23,920
우선 발표순서는 다음과 같습니다 팀원소개 먼저 시작하겠습니다

3
00:00:23,920 --> 00:00:28,560
저는 김희아이고 공주대학교 컴퓨터공학전공 휴학 중에 있습니다

4
00:00:29,710 --> 00:00:34,630
네 안녕하십니까
저는 보이스루 R&D팀 연구원으로 일하고 있는 문영기라고 합니다.

5
00:00:35,360 --> 00:00:39,590
현재 보이스루라고 하는 회사에서 이제 유튜브 음성인식을 통해서

6
00:00:39,600 --> 00:00:43,470
다국어 번역해서 다국어 자막으로 만드는 일을 하고 있고요

7
00:00:43,510 --> 00:00:47,640
그와 동시에 인하대학교 컴퓨터공학과 석사에 재학중입니다

8
00:00:48,220 --> 00:00:53,940
안녕하세요
저는 아주대학교 디지털미디어전공 4학년 박태훈이라고 하고요

9
00:00:54,260 --> 00:01:01,240
저는 프로젝트에서 모바일 어플리케이션 안드로이드 OS 개발을 맡았습니다

10
00:01:01,800 --> 00:01:07,770
안녕하세요
저는 서울대학교 전기정보공학부에서 박사과정 재학 중인 조원익 입니다

11
00:01:08,390 --> 00:01:17,360
저는 음성 언어 이해를 연구하고 있고 그 중에서도 의도파악
통사중의성 문장 유사도 측정 등을 주제로 연구하고 있습니다

12
00:01:18,580 --> 00:01:21,710
여러분들 발표회 본격적으로 들어가기 전에

13
00:01:21,710 --> 00:01:26,030
한국의 청각언어장애인 분들의 수가 얼마나 되는지 아시나요

14
00:01:27,020 --> 00:01:32,200
지금 한국에서는 총 39만 명에 청각언어장애인 있고

15
00:01:32,220 --> 00:01:38,530
이중 언어사용에 어려움을 겪는 분들은
무려 2만명 가량이라고 합니다

16
00:01:38,550 --> 00:01:41,130
2만명 결코 적지 않은 수죠

17
00:01:41,380 --> 00:01:49,900
이는 에이블 뉴스 기사인 <청각장애인의 발음교정 위한 기술적 접근들>
내용을 발췌한 내용입니다

18
00:01:50,290 --> 00:01:51,570
이 기사에 따르면

19
00:01:51,570 --> 00:02:01,590
대부분의 청각장애인분들은 말을 하는 것이 가능한 하나
듣지 못해 발음이 부정확해서 의사소통에 어려움을 겪고 계시고

20
00:02:01,590 --> 00:02:04,670
발음 교정의 여러가지 어려움을 격는다고 합니다

21
00:02:04,740 --> 00:02:12,060
또 선행연구에 따르면 재활치료를 위한 이동에
평균 20분에서 40분 정도에 시간이 소요된다고

22
00:02:12,090 --> 00:02:17,720
답하는 응답자가 43.4% 로 굉장히 높은 비율을 차지하였고

23
00:02:18,000 --> 00:02:24,420
발음 교정해 드는 비용이 부담된다는
응답자가 40% 대로 높은 비율을 차지하고 있어

24
00:02:24,450 --> 00:02:28,010
대부분의 장애인이 치료 부담을 느끼고 있다고 합니다

25
00:02:28,310 --> 00:02:36,490
저희는 이런 시간에 대한 문제 비용에 대한 문제들을
최대한 해결해보고자 어플을 기획 하게 되었습니다

26
00:02:37,100 --> 00:02:44,460
저희는 어플 사용자분들의 발화에 대해 인식을 하는 과정에서
자동 음성인식 ASR 시스템을 활용하고자 합니다

27
00:02:44,870 --> 00:02:52,420
음성인식 시스템은 보통 해당 언어 화자의
비교적 정확한 발음 및 잘 정제된 전사자료에 기준으로 학습됩니다

28
00:02:52,950 --> 00:02:58,210
이에 따라 일반적으로 정확하게 발화된 음성은
의도에 맞게 잘 인식 됩니다

29
00:02:58,430 --> 00:03:06,560
하지만 역으로 이는 의도에 맞게 인식되는 음성이 있을 경우
해당 음성이 발음이 정확하지 않을 수 있음을 시사합니다

30
00:03:07,030 --> 00:03:15,580
이 과정에서 저희는 단순히 다른 정도를 측정하는 것을 넘어
어떤 부분들이 어느 정도로 잘인식되지 않는 지를 살펴보고자 합니다

31
00:03:16,500 --> 00:03:26,710
저희가 구상하는 시스템에 필요한 것은 학습된 음성인식기, 예시 문장들,
예시 문장들에 따른 5~10인의 원어민에 발화입니다

32
00:03:27,300 --> 00:03:33,980
전체시스템은 사용자가 문장을 읽은 음성 파일을
입력으로 받아 이에 대한 인식 결과를 출력으로 냅니다

33
00:03:34,510 --> 00:03:39,660
이 때 대조군은 미리 준비해둔 5개~10개의 발화들에 대한 인식 결과로,

34
00:03:40,410 --> 00:03:45,410
어느 정도는 사용자가 읽은 문장에 대한
인식 결과보다 정확할 것으로 예측할 수 있습니다

35
00:03:45,940 --> 00:03:51,240
예컨대 ‘의도에 맞게 발화되지 않은 음성’ 이라는
스크립트를 가정해 보겠습니다

36
00:03:51,800 --> 00:03:56,310
정확한 발음에 익숙하지 않은 사용자가 이를 읽는다면,

37
00:03:56,590 --> 00:04:01,390
이제 ‘의도에 맞게 바라보지 않는 음성’으로 인식 결과가 나올 수 있습니다.

38
00:04:01,390 --> 00:04:07,090
음성 인식이 항상 정확하지는 않겠지만,
인식기에 예시 음성들을 통과시킨다면,

39
00:04:07,110 --> 00:04:14,780
예를 들어 세 개의 레퍼런스 음성들에 대해
candidate 1부터 3까지의 결과를 얻을 수 있을 것입니다.

40
00:04:15,260 --> 00:04:20,810
각각은 완전히 정확하지는 않아도,
처음에 의도한 스크립트에 보다 가깝습니다

41
00:04:21,340 --> 00:04:23,950
여기서 우리는 이들 후보 결과들을 바탕으로,

42
00:04:24,150 --> 00:04:34,750
일종의 majority voting을 통해, 개선이 필요한
어절 및 음절들을 찾아 사용자에게 알려주는 것을 목표로 합니다.

43
00:04:35,940 --> 00:04:44,250
이어서 모델 학습 및 개발을 발표하게된 문형기입니다
먼저 발표를 말씀드리기 앞서 먼저 문제 정의부터 말씀을 드리겠습니다

44
00:04:44,450 --> 00:04:48,430
음성 인식 문제 같은 경우는 음향 feature를 입력으로 받아서

45
00:04:48,430 --> 00:04:54,080
그거에 맞는 사람의 문장 혹은 텍스트를 만들어 맞추는 것이

46
00:04:54,080 --> 00:04:58,580
맞추는 확률을 극대화 시키는 것이 해결하고자 하는 문제입니다

47
00:04:58,810 --> 00:05:05,200
그래서 X가 음성인식 feature 들이고
L이 어떤 사람이 쓰는 어떤 단어 혹은 token 들 입니다

48
00:05:05,230 --> 00:05:12,450
그래서 음성인식 feature 주어졌을때 사람들의 단어가
말하는 것에 대해서 가장 높은 확률들을 극대화 시키는 것이

49
00:05:12,450 --> 00:05:15,910
여기서 정의하는 문제라고 보시면 될 것 같습니다

50
00:05:16,410 --> 00:05:25,200
음성인식의 특징에 대해 먼저 말씀을 드리자면은
첫 번째로 입력값 X와 정답 label 의 Y 사이에 alignment 가 맞지 않습니다

51
00:05:25,200 --> 00:05:32,950
두번째로 일반적으로 입력 label인 Acoustic feature 개 수가
정답 label 보다 아주 많다는 특징이 있습니다

52
00:05:33,870 --> 00:05:37,940
이어서 저희가 사용했던 딥러닝 모델 들에 대해 말씀을 드리겠습니다

53
00:05:37,960 --> 00:05:44,500
우선은 CTC* based ASR있고
그 다음에 Attention based ASR 있습니다

54
00:05:44,500 --> 00:05:48,440
그 다음 Joint CTC-Attention based ASR 이 있는데,

55
00:05:48,440 --> 00:05:48,450
저희들의 모델에서는
Joint CTC-Attention based ASR을 사용을 하였습니다.
그 다음 Joint CTC-Attention based ASR 이 있는데,

56
00:05:48,450 --> 00:05:55,250
저희들의 모델에서는
Joint CTC-Attention based ASR을 사용을 하였습니다.

57
00:05:57,200 --> 00:05:59,780
먼저 Joint CTC에 대해 말씀을 드리기 전에

58
00:05:59,840 --> 00:06:07,350
순서대로 CTC* based ASR과
Attention based ASR 한번 정리해보겠습니다

59
00:06:07,380 --> 00:06:11,500
먼저 CTC* based ASR에서  CTC 의 가정은 다음과 같습니다

60
00:06:11,900 --> 00:06:16,320
음성 인식에서 입력값의 개수와 동작값의 개수가 다릅니다

61
00:06:16,610 --> 00:06:25,820
그렇기 때문에 훈련 과정에서는 음성에 대한 정답들을 알고는있지만
정확한 정렬 alignment를 모른다는 것을 가정하고 있습니다

62
00:06:25,870 --> 00:06:29,810
그래서 이런 alignment가 맞지 않는다는 문제 해결하기 위해서

63
00:06:29,810 --> 00:06:35,880
첫 번째로 CTC 에서는 decoding 시에 몇 가지 규칙을
정해서 효율적으로 alignment를 만듭니다

64
00:06:36,120 --> 00:06:42,060
두 번째로 Blank label을 이용하여  중복되는
철자 토큰 문제 들을 해결하려고 하고 있습니다

65
00:06:42,720 --> 00:06:51,320
우선 이 그림 같은 경우는 음성인식기를 거쳐서 나온
어떤 확률분포들을 나타내는 그림입니다

66
00:06:51,340 --> 00:06:59,290
현재 음성 인식에서 음성인식 feature 들은 총 열 개가 들어 오고 있고

67
00:06:59,330 --> 00:07:03,800
10개 대해서 매칭이 되는 단어들은  BIYFIY 라고 하는 단어입니다

68
00:07:04,120 --> 00:07:08,410
한가지 재미있는 점이  있다면은
여기에 이것들은 전체가 vocab 인데

69
00:07:08,440 --> 00:07:15,830
여기에서 저희들은 정답을 알고 있기 때문에
정답에 해당되는 부분들을 따로 이렇게 추출하게 됩니다

70
00:07:16,560 --> 00:07:21,420
또 보시면 하나 특이한점은 IY 두번 반복된다는 특징이 있습니다

71
00:07:21,900 --> 00:07:28,000
이것과  이렇게 정답을 추출 해야 되는 이유로는
다음 장에서 말씀을 드리도록 하겠습니다

72
00:07:29,400 --> 00:07:34,230
저희들은 지금 이것들에 맞는 음성 alignment 맞추는 작업을 하고 있습니다

73
00:07:34,340 --> 00:07:37,360
몇 가지 제약 조건 들어 두는데

74
00:07:37,380 --> 00:07:45,800
디코딩 확률을 계산할 때 처음에는 왼쪽 위에서 부터 시작해서
오른쪽 아래로 내려가도록 제약을 걸어줍니다

75
00:07:46,190 --> 00:07:49,520
그래서 각각의 맞는 확률들을 최적 확률들을 계산하는데

76
00:07:49,530 --> 00:07:54,910
최적 확률들을  계산할 때는
forward-backward probability 를 이용해서 계산 하게 됩니다

77
00:07:55,220 --> 00:07:58,550
만약에 이렇게 decoding 을 거쳐 나온 결과물이

78
00:07:58,580 --> 00:08:05,410
B B B B IY F IY IY IY IY
였다면은

79
00:08:05,590 --> 00:08:11,090
이것들 거쳐서 B IY F IY라고 인식을 하게 됩니다

80
00:08:11,570 --> 00:08:17,250
그다음 두 번째입니다 blank label 을 이용해서
중복되는 철자 문제를 해결하고 있습니다

81
00:08:17,270 --> 00:08:21,550
자 가령 저희가 "deep" 이라고 하는 인식하고 싶은 상황입니다

82
00:08:21,550 --> 00:08:31,630
이것들을 CTC decoder에 넣었을때는 "deep"가 나왔을 때는
이 녀석은 "dep"로 인식을 된다는 문제가 있습니다

83
00:08:31,800 --> 00:08:42,170
하지만 저희가 원하던 것은"deep"이 였으므로 이러한 문제를
해결하기 위해 각각의 label 들 사이에 blank label을 사용하고 있습니다

84
00:08:43,290 --> 00:08:52,130
여기 중간에 보시는 blank label 에 들어가게 되서
d e e p 로 "deep"으로 인식할 수 있게 됩니다

85
00:08:53,560 --> 00:08:56,120
그 다음으로는 Attention based ASR 아닙니다

86
00:08:56,290 --> 00:09:04,790
기존의 방법들이었던 CTC, RNN-Transducer은 별도의
Decoding processing 을 거쳐서 alignment를 맞추고 있었습니다

87
00:09:05,540 --> 00:09:14,800
하지만 Attention based ASR 경우는 RNN 기반의
Seq2Seq와 Attention알고리즘 이용해서 alignment 를 맞추고 있습니다

88
00:09:15,490 --> 00:09:19,960
이것이 저희가 사용했던
Joint CTC-Attention based ASR 모델입니다

89
00:09:20,450 --> 00:09:27,070
여기 encoder 부분들은  RNN  계열들의 시퀀스모델
사용을 하고 여기서 두 개의 모델들이 존재합니다

90
00:09:27,100 --> 00:09:30,610
여기는 Attention decoder 이고 여기는  CTC decoder 입니다

91
00:09:30,640 --> 00:09:38,530
두 개 결과물들 나중에 이렇게 보시는 것처럼
합쳐 주는 방식으로 ASR를 진행을 하겠습니다

92
00:09:39,910 --> 00:09:49,820
그래서 이 모델의 특징 같은 경우는  CTC based 방식과  Attention based 방식을
결합하는데 CTC의 장점으로 Attention 단점을 보완을 합니다

93
00:09:49,850 --> 00:09:58,480
Attention 모델의 단점 같은 경우는 초기 Attention을 만들때
Attention이 잘 만들어지지 않을 경우에는 성능이 굉장히 떨어진 단점이 있습니다

94
00:09:58,480 --> 00:10:06,510
CTC에서 만들어 주는 alignment 를 이용해서
Attention 을 좀더 강건하게 학습하도록 보완을 하는 방식입니다

95
00:10:06,660 --> 00:10:14,910
Loss 를 계산 할 때는 alpha * CTC_Loss
+ beta * Seq2Seq_Loss로 방식으로 계산합니다

96
00:10:14,990 --> 00:10:22,600
알파와 베타의 합은 1로 고정시켜놓고 어떤 하나의
Weighted Sum 방식으로 시작한다고 봐 주시면 될 거 같습니다

97
00:10:23,960 --> 00:10:28,190
이번에는 저희가 구현제로 사용하였던 라이브러리에 대한 설명들입니다

98
00:10:29,250 --> 00:10:37,440
soundfile, librosa, typedocopt,
Pytorch 1.4, Pytorch-lightning 등 을 사용였습니다

99
00:10:37,440 --> 00:10:44,370
soundfile 라이브러리의 경우는 음성 파일을 읽을 때
사용하였고 실제 사용 예시는 아래 보시면 여기와 같습니다

100
00:10:44,620 --> 00:10:50,480
librosa의 경우는 2015년에 공개된
음성 및 음악 분석을 위한 파이썬 라이브러리 이고

101
00:10:50,480 --> 00:10:56,070
구현시에는 delta, delta-delta, log-melspectogram과
같은 feature 추출에 사용하였습니다

102
00:10:56,550 --> 00:10:59,880
마찬가지로 사용 예시들은 여기 보시는 거와 같습니다

103
00:11:01,730 --> 00:11:03,880
그 다음에는 Pytorch-lightning 입니다

104
00:11:03,920 --> 00:11:08,550
2019년 소개된 Pytorch를 위한 하이레벨 파이썬 라이브러리입니다

105
00:11:08,690 --> 00:11:13,440
라이트닝 모듈과 트레이너를 사용한다는 것이 기존과 굉장히 다른 차이점인데

106
00:11:13,520 --> 00:11:16,870
이것들을 이용해서 Pytorch 구현을 정형화 할 수 있고

107
00:11:16,920 --> 00:11:20,640
keras의 fit처럼 간단하게 모델을 학습하고 저장할 수 있습니다

108
00:11:21,040 --> 00:11:26,580
LightningModule의 경우는 forward, dataloader,
optimizer, train loop 등이 추상화되어 있고

109
00:11:26,580 --> 00:11:33,360
여기서 제공하는 인터페이스에 맞춰서 구현을  한다면은
모두 트레이너 들에서 간단하게 사용할 수 있습니다

110
00:11:33,550 --> 00:11:37,470
트레이너에서는 Average Mixed Precision,
Batch accumulation,

111
00:11:37,500 --> 00:11:41,830
logger_callback, checkpoint_callback,
gpu_number 등을 이용해

112
00:11:41,970 --> 00:11:46,690
Trainer의 복잡했던 구현들을
좀 더 간결하게 사용할 수 있다는 장점이 있습니다

113
00:11:48,830 --> 00:11:52,450
이것 같은 경우는 Pytorch-lightning 실제사용 예시입니다

114
00:11:52,980 --> 00:11:58,470
여기 보시면 lightning 모듈을 상속받아서   MNIST모델 클래스로 만들어냅니다

115
00:11:58,570 --> 00:12:05,350
각자 init 을 해놓고 보시는 것 처럼 forward,
traning_step, configure_optimizer 처럼 제공하는

116
00:12:06,960 --> 00:12:14,880
인터페이스에 맞게 칸들을 채워 넣으면  여기 트레이너를
이용해서 손쉽게  모델을 학습할 수 있다는 장점을 갖고 있습니다

117
00:12:15,780 --> 00:12:20,820
이어서 저희가 사용했던 모델 파라미터와 결과에 대해 말씀을 드리겠습니다

118
00:12:21,010 --> 00:12:29,200
데이터셋의 경우는 오늘 Clova Call를 사용하였고 청각 장애인들을 위한
앱 개발을 사용한다고 신청을 해서 허가를 받아 이용하게 되었습니다

119
00:12:29,470 --> 00:12:34,370
다음 각각이  윈도우 밀리세컨드  Hop밀리세컨드는 각 각 25ms 와 10ms 을 사용하였고

120
00:12:34,450 --> 00:12:39,470
feature 의 경우는
delta, delta-delta, log melspectogram 을 사용하였습니다

121
00:12:39,870 --> 00:12:47,920
Subsampler 는 VGG를 사용하였고
모델은 여섯 개의 encoder 와 3개의 decoder 로 이루어진 transformer로 만들었습니다

122
00:12:48,270 --> 00:12:53,250
Joint CTC ratio 의 경우는
아까 앞에 말씀드렸던 알파에 해당하는 값입니다

123
00:12:53,300 --> 00:13:00,280
0.4를 이용하였고 Drop out : 0.2
그리고 Lr Scheduler : noam을 사용하십니다

124
00:13:00,730 --> 00:13:09,840
Batch-size : 12로  잡혀 있는 이유는 저희가 사용했던
그래픽카드가 TITAN XP 싱글그래픽카드이기 때문입니다

125
00:13:09,840 --> 00:13:17,170
결과물로서 Validation cer : 0.14가 나왔고
Validation Acc : 0.92가 나왔습니다

126
00:13:17,200 --> 00:13:22,390
저희가 이 결과물들을 실제 음성인식에 이용하였을 때 몇가지 단점이 있었습니다

127
00:13:22,390 --> 00:13:28,120
그리고 단점들의 경우는 지금 Clova Call 데이터셋이
전화 주문으로 이루어져 있기 때문에

128
00:13:28,150 --> 00:13:32,470
어떤 주문이나 예약에 대한 부분들은 음성 인식이 매우 잘 하지만

129
00:13:32,670 --> 00:13:36,560
그렇지 못한 부분에 대해서는 성능이 떨어지는 것을 확인하였습니다

130
00:13:36,890 --> 00:13:42,720
그런 부분들에 대해서는 저희가 차후 데이터셋을
보완 하는 방식으로 한번 해 보려고 하고 있습니다

131
00:13:42,740 --> 00:13:50,560
해당되는 부분들의 오류 상황이나 어떤 체리피킹
혹은 잘 되지않는 또 부분들은 데모 스테이지에서 말씀 드리도록 하겠습니다

132
00:14:06,000 --> 00:14:08,780
예약 좀 할게요

133
00:14:15,830 --> 00:14:18,580
오늘 식당 예약되나요

134
00:14:23,220 --> 00:14:25,750
오늘 식당 자리 있나요

135
00:14:31,400 --> 00:14:33,060
가요

136
00:14:39,390 --> 00:14:41,790
경찰서는 어디에 있어요

137
00:14:42,320 --> 00:14:48,390
경찰서는 저기에요 하지만 버스를 타면 5분 걸려요

138
00:14:55,920 --> 00:14:57,980
수고 하셨습니다

139
00:15:04,750 --> 00:15:07,490
오늘 저녁 예약 가능한가요

140
00:15:12,200 --> 00:15:14,750
오늘 저녁 예약 가능한가요

141
00:15:21,730 --> 00:15:24,020
오늘 저녁 예약 가능한가요

142
00:15:30,360 --> 00:15:33,180
경찰서는 어디에 있나요

143
00:15:38,690 --> 00:15:40,540
경찰서는 어디에 있나요

144
00:15:46,830 --> 00:15:49,010
경찰서는 어디에 있나요

145
00:15:54,940 --> 00:15:58,620
내 마지막으로 구축한 시스템들을 통합을 할 차례입니다

146
00:15:58,910 --> 00:16:04,980
여기서 준비물은 음성 인식기 단어 혹은
문장인식의 결과 그리고 어플리케이션입니다

147
00:16:06,830 --> 00:16:11,100
앞서 말씀드린데로 일단 사용자가 음성인식기에 발화합니다

148
00:16:11,140 --> 00:16:16,220
여기서는 4어절을 발화 하는 것으로 되어 있고
오른쪽에 인식 결과가 저렇게 나오겠습니다

149
00:16:16,910 --> 00:16:22,370
초록색으로 표현된 부분이 ground truth 스크립트와
다른 부분들로 보시면 되겠습니다

150
00:16:24,490 --> 00:16:29,440
왼쪽 아래 있는 ground truth 스크립트는
모두 대문자로 되어 있고

151
00:16:30,000 --> 00:16:37,610
아까 사용자 발화하는 음성에서 다른 부분들이
인식된 것의 다른 부분이 초록색으로 오른쪽으로 표시가 되어 있고

152
00:16:37,850 --> 00:16:41,140
그 아래 있는 레퍼런스 부분들이

153
00:16:41,160 --> 00:16:48,000
이제 저희가 준비했던 레퍼런스 음성들을 음성인식기에
통과시켜서 나온 결과물들 이라고 보시면 될 것 같습니다

154
00:16:48,690 --> 00:16:55,240
해당 인식 과정은 온라인으로 될 필요는 없고
미리 준비해 두어도 전체 프로세스에 문제가 되지 않습니다

155
00:16:55,600 --> 00:17:01,110
이렇게, 사용자의 발음 인식 결과와
레퍼런스 음성들의 인식 결과들을 비교하여

156
00:17:01,220 --> 00:17:06,960
각 어절에서 ground truth일 것으로 추정되는
부분들과 차이가 있는 부분들을 표시해 알려줍니다

157
00:17:07,660 --> 00:17:11,810
여기서, 직접 ground truth와의 비교를 하지 않는 것은

158
00:17:12,110 --> 00:17:17,630
실제 해당 발화가 일반적으로 음성인식 과정에서 첼린징하기 때문에

159
00:17:17,850 --> 00:17:21,110
레퍼런스 음성들로도 오류를 유발할 수 있기 때문입니다

160
00:17:22,380 --> 00:17:29,830
저희가 목적으로 하는 것이 사용자의 발화 음성 인식결과가
ground truth에 가깝게 되는 것이 아니라

161
00:17:29,970 --> 00:17:37,480
레퍼런스 음성들의 인식 결과의 차이를 줄여 주는 것이
목적이 라는 것을 여기서 말씀드리고 싶었습니다

162
00:17:40,540 --> 00:17:47,050
이제 음성 인식 모듈 및 개선점 판단 모듈을 통과한
정보들을 어플리케이션에서 넘겨 받았습니다

163
00:17:47,290 --> 00:17:49,770
이런 정보가 관리는 어떻게 진행 될까요

164
00:17:50,580 --> 00:17:59,440
저는 어플리케이션 개발자로서 파이썬의 외적으로 얘기를 해 볼 건데요

165
00:18:00,150 --> 00:18:13,880
음성인식 모델을 서버로 올린 다음에 어플리케이션이란
소통을 하는 방법을 저희는 REST api로 활용을 할 예정입니다

166
00:18:13,910 --> 00:18:21,740
REST api를 간단하게 설명을 해 드리자면
웹의 장점을 최대한 활용할 수 있는 네트워크 기반의 아키텍처입니다.

167
00:18:21,870 --> 00:18:30,960
그리고 서버에 요청을 보낼 때 어떤 uri를 넣고 어떤 method를
사용할지에 대해 개발자들 사이에 지켜지는 '형식'의 약속입니다.

168
00:18:30,990 --> 00:18:40,010
저희 모델을 이러한 REST api의 형식으로  서버에 배포하고
어플리케이션에서 녹음된 음성 데이터를 보내고

169
00:18:40,040 --> 00:18:46,730
서버에서 서버에서 결과를 json 형식으로 받는 형식으로 진행될것입니다.

170
00:18:47,220 --> 00:18:57,460
그리고 현재 저희 어플리케이션은 POC 단계에 있으며
음성인식 모델은 rest api 설계가 완료된 상태이고요

171
00:18:57,620 --> 00:19:03,850
그리고 어플리케이션은 자바를 활용한 백엔드 부분이 완성된 상태입니다

172
00:19:03,880 --> 00:19:09,850
이후 저희 팀은 온디바이스 작업
그리고 어플리케이션의 프론트엔드 작업을 하여

173
00:19:09,870 --> 00:19:14,770
정상적인 운영이 가능하게 하는 것을 목표로 진행할 예정입니다

174
00:19:15,860 --> 00:19:21,120
이것의 끝을 저희 프로젝트에 발표를 마치도록 하겠습니다

175
00:19:21,150 --> 00:19:22,650
들어 주셔서 감사합니다

