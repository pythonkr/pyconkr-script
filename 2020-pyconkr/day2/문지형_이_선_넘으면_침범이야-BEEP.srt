1
00:00:10,090 --> 00:00:14,335
안녕하세요 저는 "이 넘으면 침범이야 -BEEP!" 이라는 제목으로

2
00:00:14,360 --> 00:00:16,968
발표를 하게 된 문지형입니다

3
00:00:17,320 --> 00:00:22,062
발표자료의 제목에서 눈치를 채셨겠지만 오늘 제가 소개할 내용은

4
00:00:22,093 --> 00:00:26,639
온라인상에서의 혐오 표현을 어떻게 탐지할 것인가에 대한 것입니다

5
00:00:27,370 --> 00:00:32,120
제가 하고 싶은 것 목표로 하는 것을
 한 줄로 설명을 해보라고 하면

6
00:00:32,159 --> 00:00:34,516
저는 이렇게 말할 수 있을 것 같아요

7
00:00:34,540 --> 00:00:38,805
AI와 사회의 다양한 문제 사이의 갭을 줄이고 싶은 사람입니다

8
00:00:40,060 --> 00:00:44,216
그 일환으로 작년에는 
뉴스 댓글 플랫폼을 흐르는 어뷰저 분석을

9
00:00:44,241 --> 00:00:46,489
 사이드 프로젝트로 수행을 했습니다

10
00:00:47,020 --> 00:00:52,277
TMI지만 이쪽에 관심이 있다 보니 네이버 파파고를 퇴사하고

11
00:00:52,302 --> 00:00:57,801
좋은 분들과 함께 너울이라는 회사에서 
더욱 직접적으로 관련된 일을 할 수 있게 될 것 같습니다

12
00:00:58,710 --> 00:01:01,640
어뷰저를 정의하고 그들을 찾아내는 작업은

13
00:01:01,665 --> 00:01:04,291
작년 어느 정도 마무리를 했다고 느꼈습니다

14
00:01:04,510 --> 00:01:08,533
크롤링한 데이터로 해볼 수 있는 것은 꽤나 해봤다는 생각이 들더라고요

15
00:01:09,070 --> 00:01:12,210
그래서 이번에는 요즘 들어 더욱 수면 위로 올라온

16
00:01:12,235 --> 00:01:16,320
온라인 공간에서 등장하는 혐오 발언에 대한 문제에 관심이 생겼습니다

17
00:01:17,140 --> 00:01:20,725
현재 네이버나 Daum, Nate와 같은 온라인 웹포털에서는

18
00:01:20,750 --> 00:01:24,076
댓글 공간을 막는 방법으로 대응하고 있습니다

19
00:01:24,100 --> 00:01:28,818
하지만 저는 이 방법이 완벽하게 이 문제를 해결하지 못했다고 생각을 합니다

20
00:01:29,230 --> 00:01:32,753
혹자는 내 표현의 자유를 침해받았다고 생각을 할 수도 있고

21
00:01:33,524 --> 00:01:38,571
혹자는 거기서 못 쓰게 한다면 다른 곳에
쓰면 되지라고 가볍게 생각할 수도 있을 것 같아요

22
00:01:39,040 --> 00:01:45,046
후자의 경우 혐오 댓글에 노출되는 연예인의 입장에서는 
혐오 댓글 총량은 크게 변화 변하지 않았을 것 같습니다

23
00:01:46,600 --> 00:01:52,576
그렇다면 어떤 텍스트의 혐오 발언이 담겨
있는지를 검출하면 어떨까 하는 생각이 들었습니다

24
00:01:52,600 --> 00:01:58,654
일단 검출이 되면 해당 댓글은 다른 내용으로
전환하거나 가리는 등의 조치를 취할 수도 있고요

25
00:01:59,140 --> 00:02:04,780
또 이왕 만들거면 현재 검출하고 있는 모델보다
잘하는 모델이었으면 좋겠다. 라는 생각이 들었습니다

26
00:02:04,874 --> 00:02:10,139
또 나 말고도 이 문제에 관심이 있는 사람들이 
함께 문제를 풀어나가면 좋겠다라는 생각도 들었고요

27
00:02:10,892 --> 00:02:15,532
부푼 마음으로 모델을 만들고 싶었지만 일단 데이터가 없어서

28
00:02:15,557 --> 00:02:18,767
좋은 데이터부터 모으자가 이 모든 일의 시작이 되었습니다

29
00:02:19,960 --> 00:02:24,491
그래서 구체적으로 오늘 할 이야기는
연예 뉴스 댓글에 등장하는

30
00:02:24,516 --> 00:02:27,272
 한국어 혐오 발언 탐지 모델을 만들기 위해

31
00:02:27,296 --> 00:02:31,686
데이터 수집부터 라벨링까지 하게 된 긴 여정입니다

32
00:02:32,350 --> 00:02:36,998
혐오표현이 담긴 컨텐츠에 대한 문제는 우리나라에서만 발생했던 것은 아닙니다

33
00:02:37,360 --> 00:02:42,226
우리나라에서 이 문제가 크게 이슈가 되었던 시기가 2019년이라면

34
00:02:42,250 --> 00:02:44,539
외국에서는 보다 더 일찍 이슈가 되었어요

35
00:02:45,400 --> 00:02:51,243
일례로 페이스북은 2017년부터 본격적으로
혐오 콘텐츠에 대응하는 모습을 보여줍니다

36
00:02:51,610 --> 00:02:56,626
그 이후로 계속 혐오 콘텐츠를 탐지하고 막는 비율이 늘어남을 보여주면서

37
00:02:56,650 --> 00:02:59,907
꾸준히 노력하고 개선 중이다 라는 메시지를 전달하고 있죠

38
00:03:01,128 --> 00:03:05,805
그리고 좀 자료를 찾아보니까 최근 트위터에서 언급했던 것처럼

39
00:03:06,080 --> 00:03:09,856
이러한 혐오 콘텐츠를 막기 위해 사용한 기술은 AI이고

40
00:03:09,880 --> 00:03:14,005
보다 구체적으로는 XLM과 같은 언어 모델인 것으로 추정이 되었습니다

41
00:03:14,500 --> 00:03:19,906
참고로 XLM이라고 불린 언어 모델은
페이스북 AI research 에서 만든 모델입니다

42
00:03:20,650 --> 00:03:24,345
네이버도 최근 들어 혐오 댓글과 열심히 싸우고 있습니다

43
00:03:24,977 --> 00:03:29,524
페이스북과는 다르게 기술뿐만 아니라 정책적으로도 대응을 하고 있어요

44
00:03:30,400 --> 00:03:34,782
이번에 개편된 댓글 정책을 살펴보면

45
00:03:34,807 --> 00:03:38,446
댓글 작성자 닉네임과 활동 이력 제공

46
00:03:38,740 --> 00:03:43,516
그리고 특정 댓글 작성자의 글 원천 차단기능 도입 있었고요

47
00:03:43,540 --> 00:03:47,836
이 덕분에 악성 댓글의 60% 이상 줄었다고 보도되었습니다

48
00:03:48,610 --> 00:03:53,688
기술적인 측면은 페이스북과 마찬가지로 AI가 도입되었습니다

49
00:03:54,580 --> 00:04:03,315
최근 발행된 네이버 다이어리 내용에 따르면
 CNM, Bi-LSTM, LSTM 모델 구도에

50
00:04:03,340 --> 00:04:08,261
Persona embedding pre-training 방법이 추가된 모델을 사용한 것을 알 수 있습니다

51
00:04:09,558 --> 00:04:12,496
전반적으로 이 모델에 대해서 잘 알지 못하더라도

52
00:04:12,520 --> 00:04:17,686
어떤 딥러닝 모델을 통해서 학습을 통해 지금 검출을 하고 있구나

53
00:04:17,710 --> 00:04:19,545
정도로 알면 좋을 것 같습니다

54
00:04:20,590 --> 00:04:27,308
성능을 보면 자체적으로 구축한 테스트 셋에서 F1 Score 기준 0.95 정도됩니다

55
00:04:28,266 --> 00:04:32,187
숫자는 높지만 테스트 셋이 공개되어 있지 않고

56
00:04:32,212 --> 00:04:35,116
또 다른 외부 모델과의 비교가 없기 때문에

57
00:04:35,140 --> 00:04:38,476
이게 정말 잘하는 건지 또 얼마나 잘하고 있는지

58
00:04:38,953 --> 00:04:44,056
예를 들어 페이스북이 발행한 모델보다  얼마나 
잘한다는 건지 쉽게 말하기는 좀 어려운 것 같습니다

59
00:04:45,280 --> 00:04:50,295
뭐 어쨋든 내부 스코어를 봤을 때 가장
점수가 높은 모델을 사용 중이구나

60
00:04:50,320 --> 00:04:52,163
정도로 이해가 되었습니다

61
00:04:52,930 --> 00:04:56,640
여기서 얻은 insight 는 뭐 페이스북도 네이버도

62
00:04:56,665 --> 00:05:00,678
AI 모델을 사용해서 혐오유무를 판단하고 있다는 사실입니다

63
00:05:00,967 --> 00:05:03,811
저도 이런 모델을 만들어보고 싶은데

64
00:05:04,578 --> 00:05:08,506
이런 모델을 만들기 위해서는 데이터가 필요합니다

65
00:05:08,530 --> 00:05:10,764
모델링의 시작은 무조건 데이터죠

66
00:05:11,080 --> 00:05:13,314
그럼 어떤 데이터를 모아야 할까요?

67
00:05:13,780 --> 00:05:17,959
무조건 많은 데이터가 아무렇게나 있다고 해서 좋은 결과가 나오지는 않습니다

68
00:05:18,400 --> 00:05:21,915
그래서 풀고 싶은 문자에 가까운 데이터를 모으고 싶었어요

69
00:05:22,330 --> 00:05:26,416
우선 저는 연예인에게 행해지는 악성 댓글 문제를 풀고 싶었고

70
00:05:26,440 --> 00:05:29,010
또 한국어에 대해서 모을 필요성을 느꼈습니다

71
00:05:29,560 --> 00:05:33,886
그리고 댓글은 다양한 플랫폼에 존재하는 데이터를 수집할 수도 있지만

72
00:05:33,910 --> 00:05:38,088
가장 악성 댓글이 문제가 되는 공간이 어디인지를 고민을 하게 되었고

73
00:05:38,113 --> 00:05:41,933
그 답으로 검색엔진내 포털의 뉴스 댓글을 생각했습니다

74
00:05:42,760 --> 00:05:46,463
뉴스라는 매체는 누구나 접근해서 소비하는 콘텐츠이기 때문에

75
00:05:46,488 --> 00:05:52,683
많은 사람들이 모이며 이 때문에 특정 커뮤니티들과 다르게 다양한 생각을 가진 사람들이 존재합니다

76
00:05:53,380 --> 00:05:57,770
이런 공공장소 같은 곳에서 발생하는 혐오는
댓글을 보게 된 사람들에게

77
00:05:57,811 --> 00:06:01,639
그리고 댓글 내용의 주체가 되는 사람들에게도 악영향을 미칩니다

78
00:06:02,320 --> 00:06:07,816
안타깝게도 수집 당시에 다음은 연예뉴스 댓글창은 닫은 상황이었고

79
00:06:07,840 --> 00:06:12,590
아직 네이버는 다른 방식으로 악성댓글 문제를 풀기 위해 노력하던 시기였습니다

80
00:06:13,090 --> 00:06:16,465
그래서 네이버 연예 뉴스의 댓글 데이터를 수집하기도 합니다

81
00:06:17,140 --> 00:06:19,882
제가 긁은 데이터는 네이버 연예면에

82
00:06:19,907 --> 00:06:25,530
일간 가장 많이 본 뉴스 기사 30개와 그리고 각 기사에 달린 댓글이었습니다

83
00:06:26,140 --> 00:06:33,015
2018년 1월부터 연예 뉴스 댓글란이 폐쇄지되기 전인 2020년 2월까지 데이터를 수집하였고요

84
00:06:33,280 --> 00:06:38,240
결과적으로 약 2만여 개의 기사와 1000만 개 댓글 데이터가 모였습니다

85
00:06:39,062 --> 00:06:41,648
아쉽게도 1000만 개 댓글에 대해

86
00:06:41,640 --> 00:06:44,771
혐오가 포함되어 있는지 일일이 태깅하는 것

87
00:06:44,796 --> 00:06:47,811
 짧은 시간 안에 해낼 수 없는 너무 큰 프로젝트입니다

88
00:06:48,881 --> 00:06:53,787
게다가 많은 사람들을 동원할 수 있도록
크라우드 소싱 업체를 통하게 되더라도

89
00:06:53,849 --> 00:06:56,099
비용이 엄청나게 소모가 되기 때문에

90
00:06:56,440 --> 00:07:00,908
개인이 하는 사이드 프로젝트로는 좀
이만큼의 데이터에 대해서

91
00:07:00,933 --> 00:07:03,916
좋은 라벨을 얻기가 어렵다 라는 판단을 했습니다

92
00:07:04,244 --> 00:07:07,572
그래서 이 중에서 만개 댓글을 선택을 했습니다

93
00:07:08,170 --> 00:07:12,013
선택한 방식은 특정 시위에 편중되지 않도록

94
00:07:12,038 --> 00:07:14,476
 날짜별로 두개의 기사를 샘플링하고

95
00:07:14,500 --> 00:07:20,625
또 기사에 적힌 댓글 중에서 싫어요. 비율 순서대로
상위 20개 댓글을 선택한 후 중복을 제거했습니다

96
00:07:21,250 --> 00:07:26,421
너무 짧은 경우에는 어떤 맥락에서 사용된
것인지 파악하기가 좀 어렵기 때문에 제거했고

97
00:07:26,530 --> 00:07:30,780
너무 길어도 판단이 어려워지는 경우가 존재하기 때문에 이들도 제거를했습니다

98
00:07:31,420 --> 00:07:37,456
이렇게 골라진 만개의 데이터 준비가 되었으니 이제 좋은 라벨을 태깅하기만 하면 되는 상황이 됩니다

99
00:07:39,280 --> 00:07:43,756
그런데 좋은 라벨을 얻기 위해서 꼭 필요했던 질문이 있었습니다

100
00:07:43,780 --> 00:07:45,833
과연 혐오표현이란 무엇일까?

101
00:07:45,858 --> 00:07:49,906
욕설이 있으며 혐오표현이다 아닌 것 같았습니다

102
00:07:49,930 --> 00:07:51,844
감정이 긍정적으로

103
00:07:51,869 --> 00:07:54,937
격해도 욕설을 하는 나 혹은 친구들의

104
00:07:54,962 --> 00:07:57,805
모습이 떠오르면서 아 이건 아닌 것 같다 라는 생각이 들었죠

105
00:07:58,047 --> 00:08:00,429
혐오 표현은 부정적인 감정이다

106
00:08:00,454 --> 00:08:04,606
웃으면서 욕하고 비꼬는 댓글들을 보면서  이것도 아니다싶더라고요

107
00:08:04,630 --> 00:08:10,458
그렇다면 비난받을 만한 대상에게 표현하는 혐오도 혐오로 봐야 할까

108
00:08:10,512 --> 00:08:14,896
이 문장은 두 가지 개념의 기준이 모호하기 때문에

109
00:08:14,920 --> 00:08:17,029
좀 어려웠던 질문이었습니다

110
00:08:17,620 --> 00:08:21,760
비난받을 만한 대상이라는 것은 누가 정하는 걸까

111
00:08:21,785 --> 00:08:24,026
그리고 혐오의 기준은 어디까지일까?

112
00:08:24,430 --> 00:08:29,446
비난받을 만한 대상이 실제로 많은 사람에게 큰 피해를 끼치고 큰 잘못을 했다면

113
00:08:29,477 --> 00:08:33,286
 당연하게도 비판의 목소리가 쎄지는데요

114
00:08:33,310 --> 00:08:37,630
이때 비판과 비난의 경계를 어떻게 잡아야 할지가 가장 어려웠던 것 같습니다

115
00:08:38,350 --> 00:08:44,206
이전까지의 질문은 댓글 데이터를 좀 찾아보고
읽어가면서 답을 할 수 있었던 것이었다며

116
00:08:44,230 --> 00:08:50,651
이 질문은 혐오 표현에 대해 다른 곳은 어떻게
기준을 세우고 있는지 살펴보게 되는 계기가 되었습니다

117
00:08:50,950 --> 00:08:56,004
다행인지 불행인지 저에게만 혐오에 대한
기준을 잡는 것이 어려웠던 것은 아니었습니다

118
00:08:56,410 --> 00:09:00,097
모델을 만들기 시작했던 2017년의 페이스북도

119
00:09:00,122 --> 00:09:04,566
어디까지 혐오로 규정하는 것이 옳은지에 대해 크게 고민했던 흔적이 보았습니다

120
00:09:04,690 --> 00:09:08,979
그 고민의 결과물은 현재 페이스북에 커뮤니티 스탠다드에 나와 있습니다

121
00:09:09,910 --> 00:09:14,206
페이스북에서 보는 혐오 콘텐츠는 크게 3등급으로 나뉘고요

122
00:09:14,230 --> 00:09:20,502
인종, 민족, 국적, 종교, 성적취향, 성별 또는 성정체성,

123
00:09:20,527 --> 00:09:23,563
심각한 신체적 장애 또는 질병과 같이

124
00:09:23,588 --> 00:09:27,572
보호되는 특성을 가진 어떻게보면 소수자에
대한 직접적인 공격을

125
00:09:27,597 --> 00:09:29,423
혐오 발언으로 정의를 하고 있습니다

126
00:09:29,950 --> 00:09:33,364
네이버의 경우 비슷한 듯 다른 기준을 가지고 있었습니다

127
00:09:34,128 --> 00:09:38,190
차별적인 표현을 제외하고는 대상이 누구인지에 관계없이

128
00:09:38,215 --> 00:09:40,792
표현 자체가 조금 더 신경을 쓰는 모습이었습니다

129
00:09:41,290 --> 00:09:45,368
댓글에 욕설이 있거나 혹은 저속한 표현이 존재하거나

130
00:09:45,393 --> 00:09:49,188
선정적이거나 폭력적이거나 비하하는 발언이 있는 경우

131
00:09:49,188 --> 00:09:51,157
 혐오 표현으로 태깅을 하고 있었습니다

132
00:09:51,730 --> 00:09:55,470
국가인권위원회의 경우는 페이스북과 비슷하게

133
00:09:55,495 --> 00:10:00,555
소수자에 대한 모욕, 비하, 멸시, 위협, 차별, 폭력 등을

134
00:10:00,580 --> 00:10:04,080
정당화하거나 조장하거나 강화하는 표현은 뜻합니다

135
00:10:04,870 --> 00:10:08,146
한국형사정책연구원까지 한번 찾아봤는데요

136
00:10:08,170 --> 00:10:12,623
여기서는 명시적으로 표현의 자유가 소수자나 사회 취약계층을

137
00:10:12,648 --> 00:10:15,873
공격하기 위해  작동하는 것이 아니라고 기술하고 있습니다

138
00:10:16,240 --> 00:10:20,029
다시 말해서 고의적으로 소수자나 사회취약계층의

139
00:10:20,054 --> 00:10:24,115
인간으로서의 기본권을 침해하는 것을 혐오표현으로 판단한 것을 알 수 있죠

140
00:10:24,370 --> 00:10:27,565
자 지금까지 살펴본 내용에 따르면

141
00:10:27,590 --> 00:10:31,713
표준이 되는 혐오표현의 정의는 없다 라고 정리를 해볼 수 있을 것 같습니다

142
00:10:31,823 --> 00:10:35,448
그렇다면 제가 직접 내가 풀고 싶은

143
00:10:35,473 --> 00:10:38,783
 문제에 맞는 기준을 세워야 한다 라는 생각이 들었습니다

144
00:10:39,700 --> 00:10:44,192
저는 악성 댓글을 볼 수 밖에 없는 이들과 제 가족 혹은 친구같이

145
00:10:44,217 --> 00:10:48,240
네이버 뉴스 댓글을 보는 일반 사람들이 입는 피해 또한 줄이고 싶었습니다

146
00:10:49,150 --> 00:10:55,066
소수자에게 향한 혐오만 혐오라면 연예인 혹은 제
주변 사람들은 좀 소수자로 보기 어려운 사람들이 대부분이 없겠죠

147
00:10:56,740 --> 00:11:02,575
그래서 저는 표현의 자유와 혐오 댓글을 경계가
애매한 경우 읽는 사람이 조금이라도 상처가 된다면

148
00:11:02,630 --> 00:11:04,669
그건 혐오표현으로 간주하였습니다

149
00:11:04,990 --> 00:11:08,626
이렇게 가장 개인적인 것이 가장 보편적인 것이다 라는 마음으로

150
00:11:08,650 --> 00:11:12,704
이 프로젝트를 함께 진행했던 분과 함께 댓글 하나하나 살펴보고

151
00:11:12,729 --> 00:11:14,806
 서로 토의해가면서 기준을 정립해 같습니다

152
00:11:15,310 --> 00:11:20,895
약 한 2000여 개 정도는 직접 눈으로
보면서 가이드라인을 만들었던 것 같아요

153
00:11:21,370 --> 00:11:24,791
다양한 예문 있어야 저뿐만 아니라 제3의 인물이

154
00:11:25,120 --> 00:11:29,096
 태깅을 할 때 혐오에 대한 기준 혹은 감수성이 유사해지기 때문에

155
00:11:29,081 --> 00:11:31,620
 다양한 사례를 가이드라인 첨부하였습니다

156
00:11:31,990 --> 00:11:36,560
이렇게 만들어진 가이드라인을 바탕으로 앞서 샘플링 했던

157
00:11:36,585 --> 00:11:39,427
일만여개 댓글에 대해 라벨을 수집을 하게 됩니다

158
00:11:39,850 --> 00:11:45,600
그리고 모델 학습을 위해서 학습용 데이터,
검증용 데이터, 테스트용 데이터로 나눴고요

159
00:11:46,540 --> 00:11:50,407
여기서는 라벨을 수집했다라는 한문장으로 요약을 했지만

160
00:11:50,432 --> 00:11:55,219
사실 그 문장을 말하기 위해서 요약 한 3 ,4개월 정도의 시간이 투자되었던 것 같습니다

161
00:11:56,095 --> 00:11:58,430
이렇게 열심히 만들었으니 모델링하기 전에 널리 아무리 좋은 가이드라인이었어도

162
00:11:58,456 --> 00:12:03,540
이걸 제대로 읽지 않았던 크라우드 소싱 언어 데이터가 대부분이었고요

163
00:12:03,790 --> 00:12:07,147
그중에 잘 태깅해주시는 분들을 우리가 일일이 찾아내고

164
00:12:07,172 --> 00:12:10,375
그리고 태깅해주신 내용을 또 일일이 검토하고 피드백을 하면서

165
00:12:10,400 --> 00:12:12,540
그분들을 좀 훈련시키는 작업이 있었습니다

166
00:12:12,940 --> 00:12:17,666
이런 작업들 덕분에 어느 정도 퀄리티 있는 데이터가 모일 수 있었습니다

167
00:12:18,719 --> 00:12:21,844
이렇게 열심히 만들었습니다 생각보다 열심히 만들었더라고요

168
00:12:21,869 --> 00:12:25,461
이렇게 열심히 만들었으니 모델링하기 전에 널리 널리 알렸습니다

169
00:12:26,050 --> 00:12:32,566
생각보다 내가 풀고 싶은 문자에 적합한 잘 만든
데이터가 부족하다는 사실을 저도 제가 절실히 느꼈고

170
00:12:32,590 --> 00:12:35,058
때론 있는지도 모르는 경우가 있었습니다

171
00:12:36,009 --> 00:12:41,674
그래서 이왕 잘 맞는것 또 이 문제에 관심 있었던
사람들의 참여를 독려할까 열심히 홍보하고 다녔어요

172
00:12:42,010 --> 00:12:43,385
먼저 논문을 썼습니다

173
00:12:43,690 --> 00:12:47,604
논문이 억셉되면 일단 연구자들이 데이터를 쓰기가 편해지거든요

174
00:12:48,010 --> 00:12:52,939
열심히 모델링해서 논문 썼는데 누군가가
 이 데이터는 믿을 만한 거야 라고 물어보면

175
00:12:52,964 --> 00:12:55,964
떳떳하게 인용할 수 있도록 논문을 열심히 썼습니다

176
00:12:56,034 --> 00:13:00,799
다행히 억셉이 됐고 그래서 앞으로는 마음껏 데이터를 쓰셔도 됩니다

177
00:13:01,510 --> 00:13:05,361
이렇게 만든 데이터는 누구나 다운받을 수 있도록 github에 공개했고

178
00:13:06,160 --> 00:13:10,316
쉽게 다운로드 받아서 쓸 수 있는 Koco라는 패키지도 만들었습니다

179
00:13:11,251 --> 00:13:16,006
보시면 그냥 import koco  한 다음에 
koco에서 어떤 데이터인지 명시를 해주고

180
00:13:17,170 --> 00:13:23,974
모두가 트레이닝인지 혹은 테스트인지를 명시를 해주면 
그게 맞는 데이터 셋을 다운받아서 쓸 수 있습니다

181
00:13:24,160 --> 00:13:26,378
그리고 가끔 모델링을 하다 보면

182
00:13:26,403 --> 00:13:31,832
어 내모델 성능이 0.4인데 이게 잘 한 번지 못한 건지 감이 잘 안 오잖아요

183
00:13:31,900 --> 00:13:37,048
그래서 이때 결과를 다른 사람들과 쉽게 
비교할 수 있도록 Kaggle에 Competition도 만들었습니다

184
00:13:37,300 --> 00:13:43,956
그때 기본적인 baseline 모델 정도만 
간단하게 만들어서 업무들을 해두었는데요

185
00:13:44,170 --> 00:13:47,026
벌써 많이들 좋은 결과를 내주고 계셨습니다

186
00:13:47,050 --> 00:13:48,932
굉장히 신기한 경험이었습니다

187
00:13:49,240 --> 00:13:55,982
그래서 저는 그냥 혐오 발언 혐오 표현 감지라는 
작은 문제를 풀고 싶었을 뿐인데

188
00:13:56,044 --> 00:13:59,386
이걸 제대로 풀려고 하다 보니까 이게 좀 많이 커졌어요

189
00:13:59,410 --> 00:14:02,363
그리고 그 과정에서 배운 점도 참 많았던 것 같습니다

190
00:14:03,010 --> 00:14:08,524
네 그리고 다양한 배움의 과정 중에는
혐오 표현에 대한 배움도 있었던 것 같아요

191
00:14:08,549 --> 00:14:12,697
아 사람들이 이렇게까지 혐오표현의 표현하는데 창의적일 수 있구나

192
00:14:12,722 --> 00:14:15,436
정말 다양하구나 아 나 자신도 많이 배웠고

193
00:14:15,460 --> 00:14:19,600
괴물의 심연을 들여다보면서 인류애도 많이 깎였던 슬픈 사연이 있었습니다

194
00:14:20,140 --> 00:14:26,725
어 이제 이렇게 여차저차 데이터를 열심히 만들었으니
이제 저도 드디어 모델을 만들 수 있겠더라고요

195
00:14:27,460 --> 00:14:31,381
그렇지만 그 전에 다른 모델은 성능부터 파악하는 것이 먼저였습니다

196
00:14:32,114 --> 00:14:36,826
제일 궁금했던 건 네이버에서 제공하는 클린봇 성능이 어떤지 확인해볼 필요가 있었습니다

197
00:14:39,966 --> 00:14:45,910
지금 네이버 뉴스 뭐 정치 혹은 사회면 폐지되지 않은 다른 댓글란에 보시면

198
00:14:45,935 --> 00:14:52,802
이렇게 클린봇이 작동해서 자동으로 혐오가 있다고 
판단되는 경우에 대해서 가려주는 것을 볼 수가 있습니다

199
00:14:53,230 --> 00:14:57,878
안타깝게도 클린봇은 저희가 만들었던 데이터의
테스트셋에 대해서

200
00:14:57,903 --> 00:15:00,946
혐오 표현이라고 탐지를 잘하지 못하더라고요

201
00:15:00,970 --> 00:15:05,235
그래서 0.147이라는 성능을 보였고

202
00:15:05,298 --> 00:15:10,477
추후에 Kaggle에 올라온 모델의 성능과 비교했을 때는 가장 낮은 점수였습니다

203
00:15:11,020 --> 00:15:15,918
그렇다면 단순하게 접근을 해보려고 했을 때

204
00:15:15,943 --> 00:15:19,645
그냥 욕설기반의 모델을 사용하면 어떨까 라는 생각이 들었습니다

205
00:15:20,650 --> 00:15:24,720
욕설이 있는 경우에는 이것은 혐오 표현이다 라고 예측을 하는 거죠

206
00:15:25,150 --> 00:15:32,460
이를 위해서 일단 욕설 단어가 공개되어 있는 github에 있는 데이터를 다운로드하고

207
00:15:32,485 --> 00:15:36,646
욕설 단어를 취합한 이후에 문장의 욕설이 등장한 경우

208
00:15:36,670 --> 00:15:38,638
혐오 표현이라고 예측을 했습니다

209
00:15:39,160 --> 00:15:45,894
이때 예측 성능은 0.274로 클린봇 보다 조금 낮지만 엄청 잘하는 수준은 아닙니다

210
00:15:46,000 --> 00:15:51,398
사실 이 욕설 기반의 모델은 높은 성능을 기대하긴 어려울 수 밖에 없습니다

211
00:15:51,730 --> 00:15:56,675
앞에서 언급했듯이 욕설이 있다고 해서
혐오표현이라고 보기는 어려울 수 있으니까요?

212
00:15:56,800 --> 00:15:59,526
그럼 어떻게 해야 더 잘하는 모델을 만들 수 있을까?

213
00:16:00,057 --> 00:16:01,456
고민을 시작했습니다

214
00:16:01,660 --> 00:16:07,935
더 잘하기 위해서는 뭐 욕설로 보이는 단어가 
있더라도 모델이 문장의 의미를 이해해야 하고요

215
00:16:08,208 --> 00:16:11,763
욕설로 보이는 단어가 없더라도 문장의 의미를 이해합니다

216
00:16:11,793 --> 00:16:15,465
결국 모델이 문장의 의미를 잘 이해해야 하죠

217
00:16:15,940 --> 00:16:19,244
또 우리에겐 열심히 만든 좋은 품질의 데이터가 있습니다

218
00:16:19,330 --> 00:16:23,986
이 데이터를 통해서 문장 전체와 라벨의 관계를 학습하는 모델이 필요합니다

219
00:16:24,610 --> 00:16:28,836
그런데 이 데이터에 가장 큰 한계점은 양이 적다라는 사실입니다

220
00:16:28,930 --> 00:16:33,789
너무 적어서 이 데이터로만 학습을 하게 되면 문제가 생길 수 있어요

221
00:16:34,570 --> 00:16:39,999
그래서 모델은 전반적인 언어적 지식을 함유하고
있으면 좋을 것 같다 라는 생각이 들었습니다

222
00:16:40,390 --> 00:16:45,804
그래서 앞서 소개했던 이 두 가지 문제를
해결할 수 있는 좋은 솔루션에 떠올랐고 답은

223
00:16:46,202 --> 00:16:49,889
 Bidirectional Encoder Representations
from Transformers라는

224
00:16:49,914 --> 00:16:52,426
긴 이름을 가진 모델입니다

225
00:16:52,450 --> 00:16:56,416
줄여서 BERT 라고 불리는 모델을 사용하는 거죠

226
00:16:56,440 --> 00:17:02,182
BERT 를 처음 들어보는 분들을 위해서 
이모델에 대해 짤막하게 소개하려고 합니다

227
00:17:02,710 --> 00:17:06,358
BERT 는 구글에서 만든 사전학습 기반의 언어 모델입니다

228
00:17:06,880 --> 00:17:11,489
라벨이 없는 대량의 문서로 미리 학습에 두고
특정 과제를 풀기 위해서

229
00:17:11,514 --> 00:17:15,255
그 과제에 적합한 라벨이 달인 데이터로 추가학습을 진행합니다

230
00:17:15,849 --> 00:17:19,177
이때 미리 학습된 지식이 활용되면서 좋은 성과를 내죠

231
00:17:19,599 --> 00:17:25,520
사전학습에 대해서 좀 더 자세히 설명하면 문자 그대로 미리 학습한 모델입니다

232
00:17:25,780 --> 00:17:28,014
그래서 Pre-traning이라고 부르죠

233
00:17:28,929 --> 00:17:33,297
보통 딥러닝 모델은 풀고자 하는 과제에 적합한 데이터로 학습하지만

234
00:17:33,640 --> 00:17:37,804
자연어 처리 대부분의 과제는 딥러닝이 풀기에는 언제나 데이터가 부족합니다

235
00:17:38,855 --> 00:17:42,933
제가 풀고자 했던 혐오탐지과정 만해도 데이터로 얻기 위해서

236
00:17:42,958 --> 00:17:45,796
한국어를 유창하게 하는 사람들이 필요하고

237
00:17:45,820 --> 00:17:49,195
그리고 그중에서 혐오에 대한 기준이 학습된 사람들이 필요했습니다

238
00:17:49,727 --> 00:17:55,993
이런 사람들을 얻기도 쉽지 않은데 
이분들과 함께 대량의 데이터를 얻기 위해서는

239
00:17:56,020 --> 00:18:00,605
이들이 일관되게 컨디션에 따라서 어떨
때는 좋은데 태깅을 하고

240
00:18:00,630 --> 00:18:04,906
 어쩔때는 나쁜 태깅 하는 그런 상황을 방지하는 작업 환경과

241
00:18:04,930 --> 00:18:09,106
그리고 이분들에 데이터를 검수할 수 있는 검수자도 필요하고

242
00:18:09,130 --> 00:18:12,669
또 이러한 그런 것들을 할 수 있는 많은 돈과 시간이 필요합니다

243
00:18:13,360 --> 00:18:18,914
그리고 아무리 대충 수집하더라도 백만 문장에
대해서 라벨을 얻는 것은 쉬운 일은 아닙니다

244
00:18:20,590 --> 00:18:24,254
그런데 생각을 해보면 우리가 이런 테스크를 풀 때

245
00:18:24,279 --> 00:18:28,396
 이 테스크에 관련된 지식만 습득을 하는 것은 아니잖아요

246
00:18:28,420 --> 00:18:31,373
우리는 전반적인 언어에 대한 지식을 가지고 있고

247
00:18:31,398 --> 00:18:36,795
이에 추가적으로 혐오가 무엇인가에 대한 과제에 맞는 지식을 습득합니다

248
00:18:37,548 --> 00:18:41,125
그래서 이러한 우리의 학습방식처럼

249
00:18:41,134 --> 00:18:44,305
과제와 무관한 대량의 텍스트 데이터를 바탕으로

250
00:18:44,330 --> 00:18:47,235
언어의 전반적인 지식을 미리 학습시키는 과정이

251
00:18:47,260 --> 00:18:51,330
다양한 과제를 풀기 위한 사전 지식을 얻을 수 있는데 큰 도움이 됩니다

252
00:18:51,730 --> 00:18:55,456
그럼 이 사전 학습 방식은 과연 어떻게 이루어질까요?

253
00:18:56,290 --> 00:18:59,258
라벨이 없는 일반적인 문장들만 모였기 때문에

254
00:18:59,282 --> 00:19:01,891
모델에게 뭔가 예측할 거리를 던져주면서

255
00:19:01,961 --> 00:19:04,797
그 과정에서 언어에 대한 지식을 습득할 수 있어야 합니다

256
00:19:05,204 --> 00:19:08,969
그래서 원래 문장의 일부를 가리고

257
00:19:09,430 --> 00:19:12,500
가려진 부분이 무엇인지 맞추는 방식을 통해서

258
00:19:12,602 --> 00:19:14,578
사전학습이 진행이 됩니다

259
00:19:15,100 --> 00:19:19,810
이 과정에서 모델은 가려진 부분을 맞추기 위해 주변 맥락에 열심히 봅니다

260
00:19:19,960 --> 00:19:21,881
그래서 단어의 쓰임새를 알게 되죠

261
00:19:22,382 --> 00:19:25,436
예를 들어서 우리는 이 문장에서

262
00:19:25,461 --> 00:19:29,140
마스크로과 가려진 부분에 들어갈 적절한 단어를 유추할 수 있습니다

263
00:19:29,770 --> 00:19:33,637
아마 좋다. 좋으다 좋아죽겠다. 같은 단어들이겠죠

264
00:19:34,390 --> 00:19:39,811
결과적으로 우리가 비슷한 의미라고 느끼는
단어들이 모델에도 비슷한 단어들이라고 인식됩니다

265
00:19:40,391 --> 00:19:44,555
사전학습이 끝난 모델은 이제 풀고 싶은 문제에 활용될 준비가 됩니다

266
00:19:45,130 --> 00:19:49,458
그래서 문장을 사전 학습된 모델에 입력하고 모델의 결과를

267
00:19:49,483 --> 00:19:53,169
풀고 싶은 문제에 입력으로 활용해서 과제에 맞게 학습시켜줍니다

268
00:19:54,590 --> 00:20:00,489
발표 시작할 때 즈음에 제가 언급했던
페이스북의 언어 모델은 XLM이 혹시 기억하시나요

269
00:20:00,850 --> 00:20:05,053
이 모델도 BERT와 유사한 사전학습 기반의 언어 모델입니다

270
00:20:06,470 --> 00:20:11,001
보통 연구자들은 BERT를 많이 쓰는데 굳이
페이스북에서 BERT를 쓰지 않은 이유는

271
00:20:11,026 --> 00:20:13,775
아마도 그 둘의 보이지 않는 관계 때문이었을 것 같습니다

272
00:20:14,807 --> 00:20:19,971
아무튼 본격적으로 BERT를 혐오 표현 검출 과제에 활용을 해봅니다

273
00:20:21,099 --> 00:20:25,816
앞서 만든 혐오 표현 데이터와 그리고 사전 학습된 BERT를 불러오고

274
00:20:25,840 --> 00:20:28,652
그리고 과제에 맞게 학습시키는 과정이 필요하겠죠

275
00:20:29,470 --> 00:20:33,001
데이터는 Koco 패키지를 통해 쉽게 불러올 수 있습니다

276
00:20:33,430 --> 00:20:40,276
얼추 데이터를 살펴보면 우리가 예측할
클래스의 개수가 hate, offensive, none이라는

277
00:20:40,300 --> 00:20:42,547
세계의 클래스인 것은 알 수 있죠

278
00:20:43,090 --> 00:20:45,230
그리고 BERT 를 불러옵니다

279
00:20:46,120 --> 00:20:50,940
사실 huggingface 에서 만든 이런
transformers라는 패키지 덕분에

280
00:20:50,965 --> 00:20:54,946
우리가 지금은 쉽게 사전 학습된 BERT를 불러올 수 있는데요

281
00:20:54,970 --> 00:20:59,891
사실 이게 없었더라면 우리가 직접 BERT를 만들어서 활용을 했어야 합니다

282
00:21:00,310 --> 00:21:04,708
그리고 BERT를 만들기 위해서는 앞서
언급했듯이 대량의 문서와

283
00:21:04,733 --> 00:21:06,927
그리고 대량의 GPU가 필요합니다

284
00:21:07,210 --> 00:21:12,916
그런데 사실 개인의 입장에서는 정말 많은 데이터를 수집하기도 어려울 뿐더러

285
00:21:12,940 --> 00:21:16,440
이러한 좋은 학습환경을 구축하기가 쉽지가 않습니다

286
00:21:17,110 --> 00:21:22,366
그래서 이러한 개인 연구자들을 위해서
huggingface에서 만든 transformers라는 패키지는

287
00:21:22,390 --> 00:21:26,446
논문에서 언급되어 있던 BERT를 손쉽게 불러올 수 있게 만들어주었고요

288
00:21:26,470 --> 00:21:30,282
또 누군가가 만든 나만의 BERT를 학습에서 huggingface에 올리면

289
00:21:30,307 --> 00:21:33,649
 또 그걸 쉽게 다운로드 받을 수 있는 환경을 만들어주었습니다

290
00:21:35,032 --> 00:21:41,016
저희는 이 검출과제를 위해서 가장 적합한 BERT를 골랐어야 했는데요

291
00:21:41,320 --> 00:21:46,765
최근에 이준범 님께서 대량의 한국어 댓글
데이터를 활용한 BERT를 만들어 주셨습니다

292
00:21:47,260 --> 00:21:52,336
이름은 Korean comments BERT의 줄임말로 KcBERT라고 되어 있어요

293
00:21:52,360 --> 00:21:55,360
그래서 huggingface에 모델 검색하는 곳에 들어가서

294
00:21:55,385 --> 00:22:00,650
KcBERT을 치면 준범님께서 올려주신 사전 학습된 BERT들을 볼 수가 있습니다

295
00:22:01,060 --> 00:22:06,958
그리고 그 BERT는 transformers라는 패키지를 통해 쉽게 불러올 수가 있어요

296
00:22:07,388 --> 00:22:09,911
그래서 저희가 KcBERT를 활용한 이유는

297
00:22:09,950 --> 00:22:15,747
 오늘 다룰 댓글 텍스트에 대해서 가장 좋은 사전지식을
함유하고 있을 것이라고 판단했기 때문입니다

298
00:22:16,742 --> 00:22:22,024
이제 이렇게 불러온 BERT를 혐오표현 데이터에
대해서 학습을 시키는 과정이 필요합니다

299
00:22:22,721 --> 00:22:24,752
그런데 이제 모델 같은 경우에는

300
00:22:24,729 --> 00:22:29,502
 저희가 보는 일반적인 하나의 라인 문장을 입력을 받으면

301
00:22:29,527 --> 00:22:31,246
이해를 할 수가 없기 때문에

302
00:22:31,270 --> 00:22:35,137
모델이 이해할 수 있는 형태로 이 문장을 분리를 해야 합니다

303
00:22:35,440 --> 00:22:39,143
그래서 모델에 맞는 tokenizer를 통해서 문장을 쪼갭니다

304
00:22:39,820 --> 00:22:46,209
그리고 이렇게 쪼갠 데이터는 문장씩 모델을 
입력이 되어서 학습이 되는 과정이 아니라

305
00:22:46,234 --> 00:22:49,437
Batch라는 여러 문장의 집합체로 입력이 되어야 합니다

306
00:22:49,780 --> 00:22:55,201
그래서 저는 서른 두 개의 문장을 하나로 합쳐서
모델에 입력할 수 있는 Dataloader 를 만들었고요

307
00:22:55,553 --> 00:22:57,779
그리고 보통 이런 모델의 학습 과정은

308
00:22:57,804 --> 00:23:00,856
traning 데이터를 통해서 학습이 진행이 되면

309
00:23:00,880 --> 00:23:06,536
validatation 데이터를 통해서 이 모델이
얼마나 잘 학습이 되었는지를 검증하기 때문에

310
00:23:06,598 --> 00:23:09,871
training Dataloader와 validatation Dataloader를 만듭니다

311
00:23:10,420 --> 00:23:13,209
그리고 모델이 학습을 하기 위해서 필요한

312
00:23:13,186 --> 00:23:16,402
optimizer 라든지 scheduler 그리고 loss-function

313
00:23:16,427 --> 00:23:19,607
 다시 말해 criterion이라고 불리는 것에 설정을 합니다

314
00:23:20,380 --> 00:23:24,481
이 단계가 완료되면 모델은 학습에 필요한 모든 준비가 끝나게 됩니다

315
00:23:25,090 --> 00:23:29,326
이 준비된 모든 재료를 바탕으로 학습을 진행할 수 있습니다

316
00:23:29,350 --> 00:23:33,498
그래서 보시면 중간 중간에 트레이닝이 얼마가 진행이 되었고

317
00:23:33,545 --> 00:23:35,959
그리고 그렇게 진행이 완료된 모델에 대해서

318
00:23:35,984 --> 00:23:38,209
validation loss가 얼마나 되는지

319
00:23:38,201 --> 00:23:40,787
혹은 정확도가 얼마나 되는지 확인할 수 있습니다

320
00:23:41,060 --> 00:23:44,700
그래서 이 값들을 계속 보면서 학습이 잘
진행되고 있는지 확인을 하고

321
00:23:44,725 --> 00:23:47,536
어느 정도 학습이 완료가 되면 이걸 저장을 합니다

322
00:23:47,997 --> 00:23:50,637
그래서 모델저장을 할 때는 모델뿐만 아니라

323
00:23:50,662 --> 00:23:54,582
그 모델을 학습할 때 사용했던 파라미터들까지 저장을 합니다

324
00:23:55,210 --> 00:23:59,491
이렇게 저장을 해야 누구나 다시 재구현했을 때

325
00:23:59,516 --> 00:24:02,342
저와 같은 모델을 만들 수가 있습니다

326
00:24:03,040 --> 00:24:06,438
그리고 이를 불러와서 모델 학습할 때 전혀
보지 못했던

327
00:24:06,463 --> 00:24:10,594
 다른 문장에 대해서는 얼마나 잘 예측하는지 확인을 할 수가 있습니다

328
00:24:11,905 --> 00:24:15,405
아까 앞에서 만들었던 Koco test set을 이용해서

329
00:24:15,430 --> 00:24:17,866
line-by-line 으로 모델에 입력을 하고

330
00:24:17,890 --> 00:24:21,390
그리고 그 모델이 어떤 라벨이 예측하는지 따로 저장을 합니다

331
00:24:21,820 --> 00:24:26,195
그렇게 활용된 모델의 결과를 봤더니 성능이 굉장이 많이 좋아졌습니다

332
00:24:26,500 --> 00:24:30,429
Leaderboard 에서는 아주 근소한 차이지만 1등을 찍었어요

333
00:24:31,060 --> 00:24:33,856
여태까지의 내용을 한번 정리해 보겠습니다

334
00:24:33,880 --> 00:24:38,950
데이터 수집 그 다음에 라벨링 그 다음에 모델링까지에 대한 내용이었는데요

335
00:24:39,430 --> 00:24:43,984
이 과정을 통해서 우리는 혐오란 무엇일까? 라는

336
00:24:44,009 --> 00:24:48,496
좀 쉬워 보이지만 사뭇어려웠던 주제에 
대해서 고민을 할 수 있었던 것 같습니다

337
00:24:49,990 --> 00:24:54,693
그리고 가장 기본적인 욕설 Term 기반으로

338
00:24:54,718 --> 00:24:57,046
 혐오표현 탐지를 시도를 했고요

339
00:24:57,070 --> 00:25:00,460
그리고 성능이 얼마나 좋지 않은지

340
00:25:00,500 --> 00:25:04,030
그리고 왜 좋지 않은지에 대한 한계점을 조금은 보았던 것 같습니다

341
00:25:04,840 --> 00:25:08,785
그리고 huggingface의 Transformers 와 PyTorch를 활용을 해서

342
00:25:08,801 --> 00:25:13,418
모델 학습을 진행할 수 있는 코드를 잠깐이나마 맛볼 수 있었고요

343
00:25:14,050 --> 00:25:19,096
만약에 앞으로 자연어 처리에 좀 관심이 있다면 huggingface의 Transformers의

344
00:25:19,120 --> 00:25:22,729
다양한 모델들이 올라와 있고 이런 학습 프레임워크를 사용을 해서

345
00:25:22,784 --> 00:25:25,151
어쉽게 모델을 학습을 할 수가 있습니다

346
00:25:25,840 --> 00:25:29,416
여기서는 혐오표현 탐지에 대한 과제를 수행했지만

347
00:25:29,441 --> 00:25:32,753
확장해서 다른 문제에 대해서도 쉽게 적용을 해볼 수가 있습니다

348
00:25:33,610 --> 00:25:39,297
그리고 제가 소개한 모델 코드는 다음과 같이 github에 올라와 있기 때문에

349
00:25:39,328 --> 00:25:44,236
한번 살펴보시고 쉽게 쉽게 모델을 학습을 해보시면 좋을 것 같습니다

350
00:25:44,260 --> 00:25:50,744
혹은 학습이 만료된 모델을 결과를 확장해서 다른
테스크에 연장해서 사용해 볼 수도 있을 것 같습니다

351
00:25:53,830 --> 00:25:58,095
네 여기까지가 저의 발표 내용입니다.
 들어주셔서 감사합니다




