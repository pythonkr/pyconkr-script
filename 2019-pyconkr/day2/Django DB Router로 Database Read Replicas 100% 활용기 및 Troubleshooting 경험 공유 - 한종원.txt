https://youtu.be/eAP8zimx05M

< Django DB Router로 Database read replica 100% 활용기 및 Troubleshooting 경험 공유>
-안녕하세요? 이른 아침부터 오신 여러분, 환영합니다. 저는 Django DB Router로 Database read replica 100% 활용기 및 Troubleshooting 경험 공유...
아, 화면이 안 나왔네요. 잠시만요.
네, 다시 말씀드리겠습니다.
저는 Django DB Router로 Database read replica 100% 활용하는 방법과 Troubleshooting에 대한 경험담을 발표할 한종원입니다.
저희 회사에서 실제로 쓰고 있는데 그 과정에서의 경험담들을 오늘 소개해드리겠습니다.
먼저 발표자인 저를 잠깐 소개드리겠습니다. 저는 2012년도에 스타트업을 시작하면서 그때부터 회사에 메인 개발원으로 파이썬을 사용하고 있습니다.
현재는 인공지능 QA테스트 자동화 서비스 HBsmith의 대표를 맡- 있고요. 저희 회사 서비스의 API 서버와 관리자 웹콘솔 다 장고로 작성되어 있습니다.
이렇듯 저희 회사는 장고를 적극적으로 활용하는데요. 심지어 저희는 장고걸스대전이라는 데에 스폰을 하고 있고요. 그래서 장고에 관해서 관심이 있거나 장고로 업무를 하고 싶은 분은 저희 회사 오시면 좋을 것 같습니다.
오늘 발표 내용입니다. 먼저 데이터베이스의 인프라로 아마존 웹서비스, AWS의 Aurora RDS가 무엇이고 왜 이걸 썼는지 소개해드리고 핵심 기능 중의 하인 read replica 구성에 대해서 소개해드린 다음에 이후에 장고의 DB 라우터 기능을 소개해드리고 이것을 작성하면서 어떤 문제가 있어서 수정하고 또 수정하는 과정들을 설명드리겠습니다. 버전1에서 5까지 바꿔왔고요.
먼저 데이터베이스 인프라입니다. 저는 2006년도에 게임회사에서 실제로 DB 관리자로 IT커리어를 시작했습니다. 그때 당시에는 IDC에 직접 들어가서 물리서버에 오퍼레이팅 시스템도 설치하고 최적화도 하고 그런 것들을 전부 제가 직접 했었습니다.
지금 생각해보면 굉장히 오래된 방법이고 어떻게 보면 굉장히 원시적인 방법인데요.
이후에 2012년 제가 처음으로 AWS라는 퍼블릭 클라우드를 접하게 되었습니다. 그때부터 VM를 사용하게 되었는데 이때 이후로는 제가 데이터베이스 서버를 운영할 때 크게 두 가지 방식읕 선택하는데 두 번째는 EC2에 DBMS를 설치하는 방식이고 두 번째는 RDS라는 서비스를 사용하는 방식입니다.
클라우드 인프라에 대해서 굉장히 여러 가지 추상화 레벨을 구분을 지을 수 있는데요. 가장 흔하게 구분짓을 방법이 SaaS라는 방법이고 있고 PaaS가 있고 IaaS가 있습니다. 제가 게임회사에서 근무할 때 했던 했던 방식인 온프레미스는 클라우드를 안 쓰는 거죠.
첫 번째 방법, DBS를 설치하는 방법은 IaaS에 속합니다. 그래서 오퍼레이팅 시스템까지는 이미 설치되어있는 거죠.
그리고 RDS는 어떻게 보면 플랫폼 자체가 가성화가 되어 있기 때문에 PaaS 레벨에 속한다고 보시면 됩니다.
보시면 알겠지만 오른쪽에 회색과 파란색이 있는데 회색이 많으면많을수록 클라우드가 직접 관리해줍니다.
즉 PaaS가 서비스 업체에서 관리해주는 부분이 많습니다.
그렇기 때문에 제가 과거에 원시적인 방법으로 데이터베이스를 관리했을 때 직접 플랫폼 쪽에 관리하는 데 많은 시간을 보냈는데요. RDS 같은 경우는 많은 부분을 다 알아서 해줍니다.
어떻게 보면 DB의 입장에서는 굉장히 편해진 거죠. 하지만 그래서 요즘 그런 거를 쓴다고 하는 일이 없는 게 아니라 기존에는 플랫폼 쪽에 많은 시간을 보냈다면 이제는 애플리케이션 쪽에서 많은 시간을 보낼 수 있게 되었습니다.
저 역시 예전에는 플랫폼에 많은 시간을 들였기 때문에 오늘 발표와 같은 이런 것을 하는 데도 시간이 오래 걸렸는데 지금은 그런 것을 어느 정도 AWS가 커버해주기 때문에 애플리케이션 레이어에서 이것저것 조작해볼 수 있는 거죠.
사실 RDS가 굉장히 혁신적이었습니다. 그런데 아마존은 이것에 만족하지 않고 오픈소스를 자기들이 직접 수정해서 자기네 클라우드에 최적화된 형태로 커스터마이징한 형태인 Aurora RDS를 론칭하게 됩니다.
이 수정사항은 아마존에서 공개하지 않기 때문에 알 수는 없지만 무엇보다 클라우드 인프라를 사용할 때 왜 쓰는가, 가장 큰 장점은 탄성적으로 인프라를 스케일 업다운 할 수 있다는 거죠.
굉장히 쉽게 할 수 있다는 건데 Aurora DB는 기존의 RDS에 비해서 훨씬 더 스케일 업다운이 쉽습니다.
오늘의 주제 read replica는 총 15개를 붙였다 뗐다 하는 데 전혀 다운타임이 없고요.
그리고 멀티 데이터베이스, 분산DB환경을 구축해보시면 알겠지만 가장 골치아픈 게 리플리케이션들 간의 데이터 싱크 동기화인데요. 이게 수 밀리세컨드 내에서 해결이 됩니다.
저도 이거를 실제 여러 개 분산처리를 구성을 해봤는데 그 당시 했던 거는 수백 밀리세컨드 정도 차이가 나서 이게 약간 던지는 시점과 타이밍에 따라 DB 데이터가 제대로 보이지 않고는 했습니다.
그런데 오로라는 인프라 내에서 알아서 수 밀리세컨드 내로 동기화가 되기 때문에 거의 동일한 데이터를 보고 있다고 할 수도 있을 정도로 굉장히 빠릅니다.
그리고 이런 Aurora DB에는 접근하는 주소, endpoint가 네 가지 타입이 있습니다.
첫 번째는 빨간색으로 되어 있는 클러스터 endpoint입니다. 저거는 읽고 쓰기가 전부 다 가능한 주소고요.
두 번째는 리더 endpoint인데 리플리케이션, 레플리카, 리더라고도 부르는데 보시는 것같이 두 개의 리더가 붙어있는데 리더 endpoint는 읽기 전용으로 리더들간의 모드밸런싱을 해주는 주소입니다.
세 번째는 커스텀 endpoint인데 이건 뒤에 가서 조금 더 설명드리겠습니다.
마지막으로는 인스턴스 endpoint입니다. 클러스터 단위로 접근하는 게 아니라 세 개의 인스턴스가 있죠.
이거 각각의 인스턴스에 내가 직접적으로 접근하고 싶을 때 쓰는 주소인데요. 사실 저는 실제 실무에서 저 단위로 접근할 일은 거의 없고 대부분 클러스터나 리더 endpoint, 아니면 커스텀 endpoint를 씁니다.
그리고 클러스터에 대해서 설명드리자면 클러스터는 인스턴스들의 집합입니다. 한 개 이상의 인스턴스가 포함되어있는 형태의 그룹이고요. 그래서 여러 가지 구성이 가능한데요. 라이터 인스턴스와 리더 인스턴스를 몇 개를 넣느냐에 따라서 여러 가지 조합이 가능합니다.
첫 번째 조합은 가장 심플하죠. 오직 하나의 라이터 인스턴스만 있는 그룹입니다. 이런 경우에는 라이터 endpoint와 리더 endpoint가 동일한 위치를 갖게 되고 두 번째는 하나의 리더와 하나의 라이터를 가지고 있습니다. 이때부터 갈라지기 시작하죠.
마지막세 번째 구성은 현재 저희 회사에서 사용하고 있는 구성인데요. 한 개의 라이터와 복수 개의 리더를 붙였을 경우입니다. 앞서 설명했던 것처럼 클러스터 endpoint는 라이터로 가고 리더는 몇 개 있든 간에 그들간에 밸런싱하는 구성을 가지고 있습니다.
참고로 제가 이 발표를 준비할 때는 없었던 건데 제가 지금 보시면 모든 클러스터들이 라이터 하나밖에 없잖아요. 저번 달까지는 이게 제한조건으로 라이터가 오직 한 개만 추가할 수 있었는데 2주 전에 8월 8일날 아마존에서 라이터도 두 개 이상 붙일 수 있게 기능을 확장했습니다. 그래서 저희 회사도 그런 멀티라이터를 도입하려고 해보고 있고요.
오늘 발표에서는 라이터 하나만 있는 형태로만 말씀드리고 있습니다.
그리고 아마 눈치채신 분이 있겠지만 클러스터 구성에 따라서 endpoint나 IP주소가 바뀌는데요. 실제로 클러스터 구성을 바꾸고 보면 IP주소가 제대로 나오지 않는 경우가 있는데 그건 OS 레벨에서 DNS 캐시를 삭제하면 되는 거죠.
이거는 저는 맥을 쓰고 있는데 강제로 맥 OS에서 잡고 있는 거를 다 다시 해서 현재 상태에서 정말 진짜 사용하는 IP주소 값을 가져올 수 있도록 했습니다.
그러면 실제로 클러스터 endpoint IP주소가 바뀌는 걸 보여드리겠습니다.
그래서 제일 첫 번째 구성이 가장 심플한 구성이죠. 클러스터 내에 한 개의 endpoint만 있는 경우, 둘 다 다 라이터 인스턴스를 가지게 되죠.
이때는 아무리 뭘 해봐도 클러스터 endpoint와 리더 endpoint의 IP주소가 늘 동일합니다. 당연하죠. 라이터가 하나밖에 없기 때문에요.
여기서 이제 리더를 하나 추가해봅니다. 리더를 하나 추가해보면 이때부터 갈린다고 했죠.
클러스터 endpoint 라이터, 그리고 리더 endpoint는 리더 쪽으로 가게 되죠.
바로 때려보면 클러스터 endpoint의 IP주소와 리더 endpoint IP주소가 달라진 걸 볼 수 있습니다.
다음으로 그러면 장고 DB라우터에 대해서 설명드리겠습니다.
장고쪽 설정인데요. 제가 쓰고 있는 건데 이 위에는 테스트니까, 그거 말고 그 밑에 실제 오퍼레이션 레벨에서 쓰고 있는 설정을 보시면 장고 설정에 보시면 대문자로 데이터베이시스라는 설정값에 복수로 데이터베이스 연결을 추가할 수 있습니다.
그런데 주의사항이 있습니다. 뭐 이게 보시면 지금 제가 디폴트와 리플리카라고 하는데 저게 사실 얼마든지 자기가 이름 바꿀 수 있거든요.
특히 디폴트는 DB1, 아니면 뭐 MAIN 이런 식으로 이름을 바꿀 수 있는데 이 이름을 사실은 전에 바꾸려고 다른 일을 하다가 바꾸려고 했는데 이게 절대 바꾸면 안 되더라고요. 왜냐하면 가끔 장고의 써드 파티 라이브러리 중에서 디폴트라는 이름이 하드코딩되어있는 경우가 있더라고요.
그래서 디폴트는 그냥 원래부터 처음에 장고를 설치하실 때 나오는 디폴트를 먼저 쓰기 때문에 말그대로 그건 건들지 않고 디폴트로 쓰는 게 마음의 평안을 얻을 수 있을 것 같습니다.
맨 마지막줄에 데이터베이스 언더바 라이터가 있는데요. 여기 리스트 형으로 직접 라우터를 추가할 수 있습니다.
그러면 제가 작성한 라우터코드를 보시면 총 4개가 있습니다.
사실 세 번째, 네 번째 함수는 거의 건들 필요 없이 그냥 디폴트 값을 쓰시면 되고요. 두 번째는 간단하게 디비 포 라이터.
그리고 앞으로 계속 수정할 게 디비 포 리드인데요. 이게 제가 첫 번째로 수정했던 게 리플리카, 그러니까 리더 endpoint로 가도록 설정했습니다. 오른쪽에 장고 설정과 왼쪽에 라우터 코드를 보시면 이해하실 수 있을 것 같습니다.
그런데 DB for reader가 하는 데 여러 가지 문제점이 있어서 이걸 첫 번째 버전이라고 부르는데요.
첫 번째 버전, 버전1의 문제점은 디비포리드를 할 때 그게 굉장히 기계적으로 그냥 오른쪽에 있는 저 리더에 보내죠. 그런데 사실은 라이터 인스턴스도 읽기 처리가 가능한 DB이기 때문에 조금 아쉽죠. 그러니까 리드가 왔을 때 절반 정도는 리더에 가고 절반 정도는 라이터로 갔으면 좋겠는데 그게 아니라 현재 버전1에는 무조건 100% 리더에 꽂히는 상황입니다.
그래서 버전2로 수정했습니다. 보시면 되게 간단하죠. 그냥 단순하게 간단한 아이디어죠. 그래서 아마 장고의 튜토리얼들을 보면 이런 걸 좋은 프렉티스로 나오는데요. 그냥 리스트 만든 다음에 거기에 디폴트 넣고 리플리카 붙인 다음에 랜덤초이스를 하면 알아서 50%씩 반반씩 가게 됩니다.
하지만 역시 이것도 문제가 있습니다. 저희가 처음에는 서비스가 단순했어요. 그래서 트랜젝션 이런 걸 안 썼는데 서비스가 복잡해지고 결제 같은 거 할 때, 결제 되게 민감하잖아요. 트랜젝션 시작해서 DB에 락을 걸기 시작했는데 그러니까 이 버전2에 문제가 생겼습니다.
참고로 저희는 데이터베이스 락을 걸 때 MySQL을 사용하는데 이때 락을 최대한 짧게 잡고 범위를 작게 잡을수록 좋잖아요. 그래서 굉장히 날카롭게 얇게 잡기 위해서 셀렉트 포 업데이트를 쓰고 있습니다.
버전2의 문제점이 두 가지 케이스가 있는데 첫 번째 먼저 락을 리더에 걸 수 있는데 만약에 리더에 가서 락을 걸었다 칩시다.
그리고 뭔가 쓰기 작업을 합니다. 그러면 당연히 라이터로 가겠죠. 라이터에 가서 어떤 변경사항이 있습니다. 그러면 오로라에서는 이 변경사항들을 리더한테 싱크를 맞추려고 할 텐데 리더는 락을 걸어놨기 때문에 쓸 수 없는 상황이죠. 그렇게 될 경우에 라이터 변경사항이 리더 쪽으로 싱크맞출 때 문제가 발생하게 되고요.
반대로 락이 리더가 아니라 라이터에 걸린 상태입니다. 그리고 셀렉트포업데이트가 락이 걸려있고.
그런데 사실 트랙젝션을 마지막에 끝났을 때 커밋 명령을 내리지 않으면 실제로 DB에 반영이 안 되죠. 아직까지는 트랜젝션 중입니다.
그런데 앞서 우리가 리드 커리를 50%씩 왔다 갔다 하게 했는데 그 트랜젝션 안에 여러 개 실행하다 보면 변경하고 리드커리를 할 때 그게 운이 나쁘게도 리더쪽으로 가면 변경 전의 데이터를 읽는 문제가 발생했습니다.
어떻게 보면 결제는 심각한 문제죠. 계좌에 어떤 걸 봤는데 최종 확인했는데 리더에서 이 부분은 결제하기 전의 잔고가 나온다면 굉장히 큰 문제죠.
그래서 이걸 어떻게 해결할지 고민했습니다. 사실 멀티 데이터베이스를 실제로 실무에서 쓰시는 분들이 그렇게 많지는 않은 것 같더라고요. 몇 군데 있기는 한데 얘기를 해보니까 이런 방법들이 있더라고요.
그래서 왼쪽에 버전2이고 오른쪽이 버전3인데 디비포리드를 보시면 저기에 어떤 리퀘스트가 들어왔을 때 이런 걸 체크할 수 있습니다. 현재 이 쓰레드가 트랜젝션을 넣은 게 있는지 없는지 확인할 수 있습니다.
그래서 읽기처리를 보내기 전에 만약에 열려있는 게 하나라도 존재한다면 혹시 모르니까 그 전에 변경사항이 있을 수 있으니까 이때는 그냥 뒤돌아보지 말고 무식하게 디폴트, 라이터 쪽으로 가라고 라우터를 고쳤습니다.
사실 이렇게 했을 때 좀 해피했어요. 뭐 큰 문제 없이 잘 된다고 생각했습니다.
그런데 저희가 서비스가 처음에는 트래픽도 별로 없고 사람도 없었는데 점점 고객사도 늘고 사용자도 늘다 보니까 데이터베이스가 성능이 달리기 시작했습니다.
저희가 지금 50%씩 트래픽을 보내고 있는데 사실 리플리카에는 두 개의 리더가 붙어있기 때문에 또 인프라 레벨에서 다시 또 반반씩 나누니까 최종적으로 보면 전체에서 50%는 라이터가 처리하고 리더는 25%씩 처리하게 된다는 게 문제가 생기더라고요.
그런데 이게 보니까 나중에는 더 심각해지겠더라고요. 만약 더 서비스가 커져서 올리고 싶다고 할 때 리더를 더 추가하면 그렇다고 해도 라이터는 50%씩 처리가 되는 거죠.
그래서 이거 수정이 필요한 것 같은데 어떻게 하면 밸런싱을 라이터까지 포함해서 n등분 할 수 있을지 고민하게 되었습니다.
그런데 생각보다 어떻게 보면, 이거 코드 보면 좀 부끄러운데 원래 단순한 게 제일 좋은 것 같더라고요. 되게 여러 가지 방법을 고민했었어요. 그런데 그냥 저기 리스트에 리더 개수만큼 리플리카를 한 번 더 추가했습니다.
그러면 저렇게 분산이 되고 66% 리더를 받으면 또 리더0와 리더1한테 반반씩 나누죠.
그런데 이것도 문제가 있죠. 리더를 추가를 하면 장고 소스코드를 수정해야 한다는 문제가 있죠. 어쨌든 되게 단순하지만 이런 방법으로 해결할 수 있다, 그래서 저는 사실 개발철학 중에 하나가 심플 이즈 베스트라고, 간단하게 처리하자고 했는데 조금 지저분하고 찝찝한 감이 있었는데 어쨌든 쓰고 있었는데.
이게 제가 또 다른 데 가서 비슷한 걸 발표를 했거든요. 그런데 그때 청중이 피드백을 주시더라고요. 이거 직접하실 필요가 없다고, 이미 뭐가 있다고.
그래서 제가 이걸, 역시 모르면 손발이 고생한다고. 세상에는 저 같은 고민을 한 사람이 한 명만 있는 건 아니더라고요. 이 문제를 되게 예전부터 고민한 사람이 많이 있었고 제가 이거를 그냥 혼자서 하고 있었더라고요.
설명드리자면 작년 11월에 아마존에서 그동안 클러스터 endpoint, 그리고 리더 endpoint만 있었는데 커스텀 endpoint라는 기능을 추가를 했어요.
그러면 기존에 클러스터 endpoint는 그냥 기계적으로 라이터에 가고 리더들 사이에서만 밸런싱을 해줬는데 커스텀 endpoint는 내가 추가하고 거기에 물려있고 인스턴스들은 자기 마음대로 할 수가 있는 거예요.
그래서 이 노란색으로 표시된 커스텀 endpoint 같은 경우에는 모든 라이터와 모든 리더들을 바라보고 있죠. 그리고 녹색은 이렇게만 보고 있는데 이렇게 다양한 구성으로 내 마음대로 할 수 있는 구성이 가능해졌습니다.
그래서 이렇게 하면 굉장히 소스코드가 단순해지더라고요. 보면.
여기 보시면, 어떠세요? 그래서 아까 좀 지저분하다고 생각했던 그 리스트 다 없어지고 오른쪽에 깔끔하게 커스텀이라는 이름으로. 물론 제가 장고 설정은 빼먹었는데 거기에도 커스텀이라고 커스텀 endpoint를 올려놨고요.
그런 식으로 하면 굉장히 심플하고 아름답게 끝나죠.
그래서 한번 적용한 결과입니다. 저희가 이 장표는 제가 우선 라이터 하나랑 리더 하나 있을 때 해봤던 건데요. 최초로 처음 적용할 때 그래프인데 지금은 리더가 두 개 있으니까요.
우선은 이때 파란색이 라이터이고 주황색이 리더입니다. 왼쪽은 셀렉트거리, 그러니까 읽기커리에 대한 변화량이고요. 오른쪽은 CRUD 전체에 대한 건데 왼쪽에 보면 저희가 적용하고 나서부터 파란색 라이터로 몰렸던 셀렉트커리 처리가 그동안 탱자탱자 놀고 있었던 리더 쪽으로 몰려서 파란색, 주황색 선이 거의 1:1로 붙는 모습을 볼 수 있습니다.
그리고 앞서 말씀드렸듯이 트랜젝션 같은 경우는 그 안에서 발생되는 건 혹시 모르니까 무조건 다 라이터 쪽에 처리하게 했기 때문에 완벽하게 딱 그 두 라인이 붙지는 않아요.
그런데 거의 1:1로 처리하게 되죠.
그리고 오른쪽에 CRUD 같은 경우는 이게 서비스마다 조금 다르기는 한데 웬만한 서비스는 전부 다 쓰기 요청보다 읽기 요청이 압도적으로 많잖아요. 저희도 그런 형태거든요.
그래서 전체적으로 커리 처리를 봤을 때도 라이터가 빠르게 그동안 모든 요청을 처리하고 있었던 것 중에서 상당부분이 분산처리되는 걸 보면 "아, 이거 괜찮다." 해서 사실 저희가 이렇게 하면서 원래 라이터 성능을 올려야 하는 이슈가 있었는데 그거 안 하고 그냥 리더 추가하는 걸로. 그래서 굉장히 만족스러웠습니다.
그래프만 보면 되게 아름답지 않나요? 이렇게 밸렁신이 아름답게 되면 기분 좋더라고요. 저만 그런지는 모르지만.
조금 진행이 빨랐나 봐요. 결론을 말씀드릴게요. 우선은 저희가 이렇게 해봤는데 어쨌든 이걸 하게 된 동기는 데이터베이스를 조금 더 유연하게 수평확장으로 도전해보자는 거였고요. 하면서 사실 저희가 이걸 6개월 동안 Troubleshooting을 많이 했었어요.
그래서 그 과정을 지금 제가 되게 압축해서 보여드리는데 사실은 버전 1, 2, 3, 4, 5가 물흐르듯이 쭉 갔던 건 아닙니다. 하면서 무슨 일이 있어서 왔다 갔다 하고 했었는데 우선 첫 번째, 우선 장점단점 중에 단점을 먼저 말씀드릴게요.
단점은 우선 써드 파티 라이브러리들이 실제로 이 멀티 데이터베이스에 대응이 안 되는 게 좀 있어요. 그런데 되게 크리티컬한 라이브러리들 있죠.
저희 같은 경우는 모스트프로바이더와 장고캐시라는 게 문제가 발견됐는데 이게 틱틱틱 문제가 발견되는 게 항상 발견되는 것도 아니고 되게 가끔씩 발견되는 거예요. 한 달에 한두 번씩. 갑자기 없는 데이터를 읽었다, 이런 게 장고에서 터지더라고요.
그래서 "이게 도대체 뭐지?" 해서 알아봤는데 이쪽에 소스코드 중에 어떤 부분인지 디폴트를 그냥 딱 찍어서 보내는 경우가 있더라고요.
그래서 그냥 저희가 어떻게 보면 케바케로 저렇게 오른쪽에 있는데 그래서 장고 앱에서 리드커리가 왔을 때 이게 어떤 장고 앱에서 왔는지를 확인해서 만약에 오스트프로바이더와 장고캐시에서 온 경우는 뒤도 돌아보지 않고 그냥 디폴트로 보내기로 했습니다. 그러니까 문제가 해결되더라고요.
오스트프로바이더는 오스트 인증을 하는 거고 장고캐시 저거 왜 필요하냐고 하시는 분이 있는데 장고서버도 여러 개 있을 거 아니에요.
장고 서버들 간에 정보를 공유를 해야 되는데 원래는 다른 캐시 DB 같은 거 만들어서 할 수도 있지만 그냥 따로 별도의 인프라 구축 없이도 같이 공유할 수 있게 해주는 기능이 있거든요.
그래서 저것도 멀티 데이터베이스에 최적화가 안 되어 있더라고요. 그래서 지금까지 저희 현재 3, 4개월 운영하면서 발견된 라이브러리고요. 그런데 치명적인 것 같아요. 최근에 수정되었는지는 잘 모르겠습니다.
그리고 앞서 말씀드렸듯이 이게 락에 문제가 생기면 가끔씩 터지면 도대체 왜 문제가 생겼는지를 알기가 힘들어요. 그래서 한 달에 한두 번 터지는데 찝찝한 거죠. 결제하다가 터지면 어떻게 하지? 굉장히 걱정이 되는 거죠.
그래서 차라리 우리가 지금 올해 제가 처음에 DB 포 리더에서 리더뿐만 아니라 라이터쪽으로도 셀렉터쿼리를 보내자는 전략을 했었는데 이게 너무 위험한 것 같다. 차라리 확실하게 문제가 있으면 조금 100% 문제가 나올 수 있게 하기 위해서 그냥 리드 쿼리를 눈감고 다 리플리카 쪽으로 최대한 보내도록 수정을 했습니다. 그래서 지금 현재 이 상태에서 나중에 문제가 생기면 또 찾을 텐데 아직까지는 문제가 크게 없는 것 같아서 다시 커스텀 endpoint로 바꿔도 되지 않을까 생각하는데 저렇게 바꿔놓고 발견된 게 있거든요.
그러면 이제 단점들을 이야기하면 "이거 도대체 왜 듣고 있지? 문제 많은 거 아니야?" 하는데 이미 말씀드렸지만 이게 장점이 있으니까 했겠죠.
저는 데이터베이스 관리를 하면서 가장 큰 문제중에 하나가 처음에 서비스 시작할 때부터 DB를 적당한 수준의 사양으로 시작하는데 서비스가 되게 흥했어요. 그러면 DB서버를 늘려야 되는데 거기에 스케일링하는 문제가 굉장히 큰 이슈죠.
왜냐하면 DB를 뭔가 점검을 한다, 사용을 올린다, 이러면 뭔가 서비스를 내려야 되고 DB가 내려간다는 얘기는 전체 서비스가 내려가는 거죠.
그런데 요즘 추세가 글쎄요. 서비스 점검한다고 공지 뜨고 안 움직이는 온라인 서비스 없잖아요. 대부분 다 디폴트가 무중단 점검이잖아요.
그래서 저는 셀렉트 쓰루풋 같은 경우에 이거를 무중단으로 올려보고 싶다.
물론 RDS도 read replica 써서 무중단으로 하는 방법이 있기는 하지만 그 방법이 그렇게 좋아보이지는 않았어요.
왜냐하면 어쨌든 계속 스케일업 하더라도 한계는 있잖아요.
그래서 저는 스케일업보다는 그냥 수직상승보다는 수평확장이 더 좋지 않을까?
그리고 가끔씩 보면 이런 거 있죠. 프로모션 해서 여러분이 서비스가 대표님이 아니면 기획에서 이번에 뭔가 기가 막힌 기업에서 이번에만 일주일동안만 트래픽을 20% 정도 뻥튀기할 것 같아. 그때 대비해서 스케일업하기는 조금 그렇잖아요. 그런데 수평확장하면 수평축소하기도 훨씬 쉽잖아요.
그래서 스케일아웃을 하기에 대비를 해놓다 보니까 나중에 트래픽이 갑자기 늘어나거나 특정 기간에만 변하는 것들이 굉장히 편하게 됐죠.
그리고 저는 대부분의 아마 여기 운영하시는 분들 DB는 하나만 놓는 게 아니라 이중화 시켜놓잖아요.
그래서 대부분 스탠바이 형태로 안 쓰시고 그냥 관두고 계세요. 그런데 이렇게 되면 되게 아쉽잖아요. 이것도 하나의 비용인데.
그래서 그동안 놀고 있었던 리플리카를 제대로 활용하게 되니까 아까 말씀드렸던 것처럼 또 트래픽이 늘어나니까 또 리플리카 늘리고, 이런 식으로 기존에 안 쓰던 자원을 잘 쓸 수 있다는 점에서 굉장히 만족도가 높은 작업이었습니다. 물론 되게 힘든 일도 있었지만요.
어떻게 보면 조금 빠르게 진행했는데 여기까지가 제 발표였고요. 혹시 질문 있으면 질문 받도록 하겠습니다.
-발표 잘 들었습니다. 궁금한 게 트래픽을 분산시키잖아요. 그런데 만약에 라이터 쪽에 있는 DB가 뭐가 문제가 생겨서 내려가게 된다고 하면 그 라이트 트래픽이 알아서 리드 쪽으로 연결을 해주나요? 장고에서?
-네, 그게 아까 말씀드렸듯이 원래는 제가 RDS나 오로라를 쓰기 전에는 직접 제가 그런 페일오버 전략 같은 것도 세우고 알아서 복구할 수 있도록 플랫폼을 준비를 했어야 하는데 지금 요즘 클라우드 업체 뭐 제가 지금 특정 거를 이야기해도 그렇기는 하지만 다른 것도 다 비슷한 걸 지원하는 걸로 알고 있는데 라이터 쪽에 문제가 생기면 바로 리드 레플리카가 하나가 상승해서 받아줍니다.
그래서 read replica를 하나씩 붙여놓으라고 하는 게 그런 거거든요.
다른 질문사항 있으십니까?
그러면 여기서 발표를 마치도록 할까요? 조금 빨리 끝났는데요. 들어주셔서 감사합니다.
(박수)