https://youtu.be/ctyV06FaUh4

-이번 시간은 김준성 님께서 발표를 진행해 주시겠습니다. 따뜻한 발표로 맞이해 주시기 바랍니다.
-혹시 마이크 나오나요?
잠시만 기다리시면 마이크 수정을 하고 시작하겠습니다.
마이크가 잘 들리나요? 나오나요? 유선이 안 되는 거였군요.
그런데 일단 저희가 먼저 제 시간에 발표를 시작해야 하는데 진행이 안 된 부분을 사과드리겠습니다. 무선 마이크가 작동이 되는데 유선 마이크가 제대로 작동이 안 돼서 이것을 확인해 보고 진행하도록 하겠습니다. 기다리게 해 드려서 죄송합니다.
시작이 지연되는 부분에 대해서는 저희가 다시 한 번 사과 말씀 부분을 드리고 지연이 된 부분 만큼 뒤에 시간을 내도록 하겠습니다. 죄송합니다.
-우선 마이크 때문에 조금 시간이 늦어졌는데요. 여러분 오늘 이른 아침에도 불구하고 들으러 와주셔서 정말 감사드리고 많은 분이 와주실 줄 몰랐는데 사람이 많으니까 긴장되기는 하네요.
오늘은 안녕하세요? 오늘 발표를 하게 된 스캐터랩의 머신러닝 엔지니어 김준성이라고 합니다.
오늘은 100억 건의 카카오톡 데이터로 똑똑한 일상대화 인공지능이라는 주제로 재미있는 주제로 여러분에게 발표를 드리려고 합니다. 저는 지금까지 NLP 대화 분야에서 연구개발을 2년 동안 진행했고 네이버 클로버 인턴십을 거쳐서 지금은 핑퐁이라는 팀에서 사람 같은 인공지능을 만들기 위해서 여러 가지 연구와 개발을 하고 있습니다.
TMI로 설명을 드리면 관련된 여러 오픈 소스 프로젝트를 진행하고 있어서 깃허브에 가보시면 파이토치와 관련된 다양한 주제의 프로젝트를 보실 수 있을 것 같습니다.
먼저 오늘 발표 제목에도 있듯이 일상대화 인공지능에 대해서 다루게 될 건데요. 오늘 발표의 본론에 들어가기에 앞서서 앞으로 계속 언급하게 될 일상대화 인공지능이 무엇인지 왜 어렵고 오늘 어떤 문제를 풀게 될지 여러분께 설명을 드리도록 하겠습니다.
먼저 우리가 요즘 주위에서 자주 볼 수 있는 구글 어시스턴트나 클로버처럼 AI 인공지능을 볼까요? 날씨 알려줘 배송일자 변경해 줘처럼 사람들이 대신할 일을 하고 있어요. 이런 형태의 인공지능을 기능대화 인공지능이라고 얘기를 하는데요.
그와 반대로 일상대화은 클로버나 구글 어시스턴트의 뭐 해줘처럼 명령이나 질문을 하는 인공지능이 아니라 친구처럼 대화를 나눌 수 있는 사람 같은 인공지능을 이야기합니다.
마치 인공지능 고양이인 저 도라에몽이라는 친구가 진구에게 혼내고 말을 걸고 위로를 건네는 것처럼 말이죠.
그러면 일상대화의 기술적 난이도에 앞서서 기능대화 인공지능을 만드는 데 어떤 노력이 필요하고 얼마나 어려울까요? 기능대화는 해당 분야에 관련된 명령이나 아니면 질문에만 동작하기 때문에 사용자가 어떤 질문을 할지 예상되는 범위가 한정적입니다. 그렇기 때문에 개발자들은 해당 분야에 대한 구체적인 시나리오를 만들고 학습에 필요한 데이터 역시 고객 상담 데이터나 인터넷에서 검색해서 결과를 알려드립니다. 이렇게 대답하는 경우도 있잖아요.
그래서 어느 정도 사용자의 질문에 대해서 예측이 가능하고 사용자가 질문할 범위가 굉장히 적습니다.
그에 반해 일상대화 인공지능은 정말 훨씬 더 어려워요.
그 이유는 사람들이 어떤 말을 할지 예상할 수 없기 때문인데요. 사람은 상황이나 감정 상태 시간 그리고 감정이나 경험에 따라서 대화의 문맥이 굉장히 많이 달라지고 또 이런 무한한 문맥에 대한 답변 역시 무한한 경우의 수를 갖게 됩니다.
때문에 일상 대화 인공지능에서 시나리오를 만들거나 아니면 간단한 모델을 만들어서 학습하는 방식으로는 절대 이 문제를 풀 수 없어요.
이런 어려움 때문에 지금까지 일상대화 기술은 잘 연구되지 못했고 또 활발하게 연구개발이나 아니면 실제 서비스로 사용되는 사례들이 많이 나오지 않는 문제점이 있었습니다.
그러면 이 문제를 풀 수 있는 유일한 방법은 뭘까요? 바로 사람처럼 깊은 이해와 추론 능력을 가질 수 있는 딥러닝 모델을 사람처럼 만드는 방법입니다. 일상대화에서는 모델을 다양한 경험을 통해 사람처럼 이해하고 또 사람을 표방해서 잘 대답할 수 있도록 만들어줘야 하는데요. 마치 어린 아이들이 유치원에서 친구들과 대화를 하면서 다양한 경험을 하고 그 과정에서 어떻게 대화를 하는 게 좋은 것인지 배우는 과정을 딥러닝역시 다양한 경험과 좋은 학습 방법을 통해서 대화라는 게 무엇인지 어떻게 잘 대화하는지 추상적인 개념을 딥하게 배워야 하는 문제로 푸는 방법밖에 없습니다.
그런데 이런 일상대화 인공지능이 연구되지 못했던 난이도도 있지만 또 하나의 이유는 데이터가 없어요.
외부에서 연구용으로 공개되거나 아니면 실제 기업용으로 사용되는 연구데이터가 사용자가 조금 시간과 돈을 들여서 누군가에게 시켜서 레이블링을 하거나 실제 대화 데이터를 인위적으로 만들어서 학습을 시켜야 하는데 그러다 보면 엄청나게 다양한 경우의 수를 다 파악할 수 없잖아요.
그렇기 때문에 그런 데이터가 부족한 것 역시 일상 대화 인공지능이 잘 연구되지 못했던 메인 요소이기도 합니다.
하지만 제가 있는 핑퐁팀에게는 약 한국어 100억 건의 카카오톡와 일본어 2억 건의 라인 데이터를 보유하고 있어요. 우리는 이 정도의 데이터 양은 사람이 일평생 다 읽지 못할 정도의 많은 데이터인데요.
이렇게 사람들이 실제로 대량의 데이터가 있다면 사람의 언어를 학습할 때 필요로 하는 수년간의 경험을 대처할 수 있는 것은 당연하고 저희가 이 데이터가 단순히 몇 천 명 몇 백 명의 데이터가 아니라 몇십만명의 데이터를 모았기 때문에 다양한 사람의 사고방식 문체를 학습할 수 있어서 인공지능이 사람처럼 이해하는 데 도움이 될 수 있는 데이터를 가지고 있습니다.
그러면 데이터도 있겠다 본격적으로 어떻게 인공지능이 대화를 이해할 수 있는지 한번 알아보도록 하겠습니다. 이번 섹션의 키포인트는 어떻게 대화의 의미와 문맥을 잘 이해할 수 있는 모델을 만드는 것인데요.
앞으로 계속 나올 단어라서 제가 설명을 드리면 네추럴 랭기지 언더스탠딩이라고 할 수도 있고 NLU라고 얘기도 합니다. 10살짜리 애기가 수능 국어 문제를 푸는 것을 보신 적 있으신가요? 천재가 그럴 수는 있지만 10살 정도 어린아이가 수능을 풀기는 굉장히 어렵습니다.
수능 문제를 푸는 테크닉이 없어도 이기도 하지만 근본적으로 언어에 대한 이해도가 낮아서 인데요.
아직은 다양한 글이나 다양한 경험 문맥에 대한 다양한 배리어스들을 경험해 보지 못했기 때문에 자연스럽게 어려운 언어로 쓰여져 있는 수능 국어 문제는 풀기 어려워집니다.
그렇기 때문에 언어의 이런 것처럼 사람도 언어의 이해도가 없다면 어떤 문제를 풀지 못하는 것처럼 인공지능 역시 언어에 대한 이해가 없다면 문제를 풀 수 있어도 잘 풀지 못하고 깊이 있는 이해가 없습니다.
머신러닝도 사람과 유사하기 때문에 여러 연구를 통해서 언어에 대한 이해를 먼저 학습하고 그 이후에 실전 문제를 풀었을 때와 단순히 문제를 풀도록 학습하도록 했을 때와 비교했을 때 먼저 이해를 한 머신러닝이 더 좋은 성능을 갖는 것을 볼 수 있었습니다. 그러면 이제 카카오톡 데이터 대화 데이터를 가지고 저희가 어떻게 학습할 수 있을까요?
먼저 저희도 역시 머신러닝의 언어의 이해도를 최고로 끌어올릴 수 있도록 것을 학습하게 됩니다.
언어에 대한 경험을 모델이 이해할 수 있도록 하고 그 이후에 이렇게 학습한 언어 이해 모델 NLU 모델을 이용해서 실제 문제를 풀게 됩니다. 우리 대화 시스템에서는 어떻게 대답할 수 있을지 주제가 되겠죠.
이렇게 오늘은 이 두 가지가 어떻게 이루어지고 각각의 모델 구조가 어떻고 어떻게 학습됐는지 알려드리려고 하는데요. 먼저 어떻게 언어를 이해하는 학습하는 이해하는 모델을 만들 수 있을지 여러분에게 알려드리려고 합니다.
지금까지 그러면 연구된 NLU 모델들을 먼저 살펴 보려고 하는데요. 여러분도 아실 수도 있고 모르실 수도 있을 것 같은데 가장 유명하고 여러분이 알 수 있는 모델을 꼽자면 하나의 단어에 대해서 워드 투 백이라는 라이브러리를 아실 것입니다. 주변 위치를 학습해서 각각의 단어를 학습하는 방법으로 언어에 대해서 이해하게 되는데요.
젠심이라는 라이브러리를 통해서 많은 사람들에게 사용이 되고 있었고 또 머신러닝 지식이 없어도 개발자분들도 쉽게 이해하고 사용할 수 있었기 때문에 다양한 분야에서 많은 응용이 이루어졌습니다.
그리고 이 워드투백을 이용해서 어떤 한 문장에 대한 의미를 추론할 때는 각각 단어밖에 추론을 못하기 때문에 단어에 대한 벡터를 뽑고 단어에 대한 벡터에 평균을 내서 문장에 대한 의미를 이해하는 방식으로 진행했습니다. 워드투백 이후에 나온 논문에서 연구에서 가장 대표적으로 꼽을 수 있는 것은 엘모인데요. 엘모는 문장 단위 웹프로젠테이션이라고 해요.
워드투백은 간 단어별로 의미를 파악하면 엘모는 그 문장에 대해서 어떤 의미인지 또 어떤 단순히 각 단어의 벡터를 평균하는 게 아니라 구조를 이용해서 앞뒤 간의 단어 관계를 반영하는 모델이 있습니다. 이렇고 하면 단순히 단어의 의미뿐만 아니라 앞뒤 간의 관계 컨텍스트 문맥을 반영할 수 있어서 워드투백보다 더 좋은 모델을 만들 수 있게 됩니다. 하지만 이렇게 소개해 드린 두 모델 역시 우리가 갖고 있는 대화 데이터를 사용하기에는 굉장히 부족한부분이 많았습니다.
첫 번째로는 이렇게 부족한 부분의 첫 번째로는 각 문장에 대한 깊은 이해를 하지 못해요. 그러니까 분명 예를 들면 동일한 문장을 가지고 있는 동일한 의미를 가지고 있는 문장이 2개 있다고 가정했을 때 이 문장이 의미가 같다고 추론을 하려면 두 벡터가 비슷해야 해요. 체언이나 용언 명사나 아니면 동사가 그런데 조금씩 바뀌면 완전히 다른 의미의 문장으로 변화되는 것처럼 되게 용언이나 체언에 민감해지는 문제가 있습니다.
두 번째로는 이해해야 될 문장의 길이가 길어질수록 이해도가 급격하게 떨어져요.
세 번째로는 저희는 대화라는 전체적인 플로우 그러니까 단순히 문장을 이해하는 게 아니라 각 문장이 어떻게 연결되고 어떤 대화의 흐름을 가지고 있는지 의미를 파악해야 하는지 지금까지 설명해 드린 엘모나 워드투백은 다이얼로그의 플로우를 이해할 수 있는 모델은 아닙니다. 그렇기 때문에 이런 문맥을 이해할 수 있는 기능은 전혀 들어가 있지 않아서 대화 시스템에 적용하기에는 어려움이 있습니다. 그래서 저희는 세 가지가 기존의 문제라고 정의하고 문제를 해결할 수 있는 모델을 찾다가, 2017년도에 2018년도 10월에 나온 새로운 논문을 통해 영감을 받게 됩니다.
2017년도에 구글은 단어를 병렬적으로 처리할 수 있는 트랜스포머라는 구조를 만들었는데요. 이 모델을 통해서 기계번역 태스트에서 굉장히 높은 성능을 보여줬어요. 1년 뒤인 2018년 10월에 이 구조를 기반으로 하는 이런 기술을 공개했습니다. 그런데 모델 이름이 너무 걸죠? 사람들은 보통 이 모델을 버트라고 이야기합니다. 버트 모델은 단어의 전후 맥락을 보면서 전체 문맥을 파악하고 단어의 의미를 학습할 수 있도록 만들어진 모델 구조입니다.
그래서 이러한 셀프 어텐션이라는 기법을 이용해서 각 단어의 어떤 단어에 영향을 주는지 이해를 하게 되는데요. 공개된 모델은 웹에 있는 위키피디아나 공개된 소설 아니면 자막 데이터들을 이용해서 전체 다양한 텍스트를 모았고 다양한 경험에 대해서 학습할 수 있는 모델을 만들게 되었습니다. 그런데 제가 이렇게 갑자기 뜬금없이 버트를 소개해 드린 이유는 버트에는 엄청나게 뛰어난 성능이 있기 때문인데요.
버트는 뛰어난 모델 구조와 학습 방법을 이용해서 11개의 NLP 테스트에서 1번의 즉 최고 성능을 기록하는 결과를 얻었습니다. 심지어 그냥 조금 오른 수준이 아니라 이전 모델에 비해 굉장히 크게 점수가 올라서 NLP계의 게임 챌린저라고 부르기도 하죠. 더 소름 돋는 건 성적이 오른 것뿐만 아니라 사람보다 더 높은 점수를 기록하게 되어 더 화제가 되기도 했습니다.
그러면 이렇게 좋은 NLU 모델이 있는데 우리도 한번 써볼 수 있지 않을까 당연히 우리는 NLP에서 굉장히 많은 연구를 하고 있는데 버트를 대화체에 학습시켜보지 않는다면 굉장히 큰 실험 하나를 넘겨주는 느낌이었어요. 이론적으로 생각해 봤을 때 버트는 대화체 NLU 모델로 사용하게 되면 얻는 이점이 많았어요. 대화 시스템에 플로우를 이해하기도 굉장히 적합한 모델 구조여서 이 모델을 실험을 안 해 볼 이유를 찾을 수 없었어요.
그래서 저 역시 대화체를 위한 대화체에 적합한 대화 인식 모델을 만들 수 있게 되었습니다.
그러면 앞으로는 어떻게 이 다이얼로그 버트를 학습시켰는지 소개해 드리려고 하는데요. 간단합니다. 두 가지 학습 방법을 동시에 이용하는데요. 첫 번째는 전체 단어에서 15%를 랜덤하게 삭제시키고 그 단처를 맞히는 거예요. 이렇게 말이죠. 전체 대화가 있는 것 중에 단어가 삭제됐잖아요. 그러면 사람한테 저기 빈 칸에 무슨 단어가 들어갈까라고 물어보는 거예요. 계속.
그리고 그 과정을 계속 무한반복해요. 그러다 보면 이 단어를 맞힐 때마다 리워드를 주고 틀릴 때마다 너 틀렸다고 감정을 주면 머신러닝이 알아서 전체적인 컨텍스트을 봤을 때 이 단어에는 어떤 것이 들어가는 것이 맞겠다. 그러면 단어를 학습하는 과정에서 언어에 대한 이해를 하게 되는 거죠. 수능 문제에서 빈 칸에 어떤 단어가 들어갈까를 맞히는 것을 반복적으로 학습한다고 생각하면 될 것 같아요.
두 번째로는 어떻게 문맥을 학습는 하는 방법인데요.
앞에서는 단어들에서 맞혔다면 이번에는 문장에 대해서 맞히는 거예요. 현재 문맥에서 이렇게 이번에 트와이스 신곡 뮤비 봤어? 최고야 진짜라는 대화를 하고 있는 과정에서 다음 문장으로 어떤 게 나올지 예측하는 건데요. 예를 들면
먹은 거 어머님에게 말씀드렸어라는 문장이 나오면 갑분싸라고 하죠? 말이 안 되는 문장이 나온 거예요. 그래서 이런 것은 0으로 그러니까 틀렸다고 예측을 하도록 만들고 예를 들면 그래서 이번에 앨범 사려고 이렇게 자연스러운 문장이 나오게 되면 컨텍스트에 맞는 1로 학습을 하게 됩니다. 이러면 자연스럽게 연속된 대화에서 다음 문장이 뭐가 나올지 컨텍스트이 뭔지 자연스럽게 학습시킬 수 있어요.
그래서 이렇게 아까 말했던 단어의 빈칸의 단어를 맞히는 방법과 문맥을 학습시키는 방법을 동시에 학습시켜서 대화가 어떤 시스템으로 이루어지는지 어떤 구조로 대화를 이해해야 하는지를 평가하고 학습할 수 있는 모델을 만들 수 있게 되었습니다.
하지만 이렇게 두 가지 구조만으로는 기존 저희가 만들려고 하는 대화 시스템에 최적화되어 있지 않아요.
그 밑에 하나를 더 추가해 줘야 하는데요. 저희가 추가한 내용은 어떤 대화가 있으면 그 대화를 각각 저희가 연결해서 문장을 붙여서 인풋을 넣게 되는데 각각 이렇게 인풋을 넣은 단어들을 구분할 수 없었어요. 몇 번째 턴에서 나왔는지 알 수 없었기 때문에 기존 대화 시스템 오리지널 버트를 적용하기 어려워서 턴임베티트라는 메소드를 추가시켜서 최적화시켰습니다. 이후에는 구글에서 공식적으로 적용한
버트 NLU 모델을 학습켰는데요. 텐서플로를 이용했어요. 가장 큰 이유는 물론 공식 레포이기도 하지만 구글에 있는 TPU를 사용해서 빠르게 학습시킬 수 있다는 건데요. 논문에서는 이것을 이용해서 굉장히 빠르게 학습하는 것을 보여주어서 스타트업은 시간이 금이잖아요. 그래서 저희도 굉장히 빠른 학습을 위해서 TPU에서 18일 정도 카카오톡 10억 건을 필터링해서 학습을 진행했습니다. 이렇게 학습했을 때 두 가지 결과를 볼 수 있었는데요. 첫 번째에서는 53% 빈칸을 맞히는 것. 53%가 어떻게 보면 정확도가 떨어지는 거 아니냐고 생각할 수 있지만 사람한테 맞혀보라고 하세요.
혹시 그것을 다 100% 완벽하게 맞힐 수 있는 사람이 있을까요? 똑같이 기계도 반 정도 하면 잘 맞히는 거예요. 사람도 어렵거든요. 그래서 보시면 굉장히 빠르게 수렴하고 이후에는 굉장히 성능이 오르는 것을 볼 수 있는데 언더피팅 상태예요. 아직 학습이 안 된 상태인데 비약적인 성능 상태가 없어서 중지를 했고 두 번째는 두 문맥에 맞는 테스크에 대해서는 88% 정도의 성능을 갖는 모델을 만들 수 있었습니다.
그러면 이렇게 이해시킨 모델을 가지고 진짜 잘 응용할 수 있을까? 얼마나 응용할 수 있을까 평가를 해 봤는데요. 기존에 저희가 버트 없이 언어에 대한 이해 학습 없이 또는 엘모를 이용해서 조금 학습을 한 이후에 언어 테스크를 풀었을 때와 실제 저희가 만든 버트를 비교할 때 굉장히 큰 차이가 났어요. 내부적으로 사용할 수 있는 언어 이해 테스트가 있는데요.
두 문장의 의미가 유사한지 어떤 답변을 줬을 때 이 답변이 유효하고 적절할지 또 이 질문에 대한 의도는 무엇인지 구별하는 세 가지 모델에 대해서 버트랑 비교해 봤을 때 첫 번째 모델 같은 경우는 한 8% 정도의 높은 성능 향상이 있었고 그 이후에도 계속 81% 정도 됐던 게 86% 정도 되고 이후에 한 85% 정도되는 게 89% 오르는 성능 향상을 볼 수 있었습니다. 이렇게 저희가 되게 이해도 높은 언어 모델을 만들 수 있었고 이것을 가지고 대답을 해야 하잖아요.
이것을 가지고 대답을 해야 실제 인공지능을 만드는 거지. 그러면 어떻게 이 언어 모델을 가지고 이해를 할 수 있을지를 소개를 해 드리려고 하는데요. 뛰어난 이해력을 무기로 갖췄으니 말을 하도록 가르쳐 봅시다. 저희가 아까 설명드렸던 것 중에 일상 대화는 어떤 질문이 들어올 수 있을지 예상할 수 없다고 했잖아요. 일상대화는 모든 들어올 수 있는 쿼리 스페이스 그러니까 예상되는 질문의 스페이스가 엄청나게 무수하게 많아요.
그래서 우리는 무수히 많은 것을 버트 이용해서 언어를 이해시키고 언어 이해를 통해서 특정한 벡터, 이 문맥은 뭐고 이 문장은 뭐고를 추론할 수 있는 모델을 만들었어요.
그런데 문제는 뭐냐 하면 답변도 무한하다는 거예요. 답변도 무한해서 그러면 이렇게 이해한 것을 가지고 무한한 답변을 만들어야 하나? 그러면 만약에 답변이 무한한 상태로 가정하고 모델을 만들게 되면 제너레이션 모델 생성 모델을 만들게 해야 하는데 챗봇이 직접 생성하는 말을 사용자에게 전달하게 될 텐데 우리 스스로 챗봇이 무슨 말을 할지 예측할 수 없잖아요.
이 모델을 구글이나 다른 클로버 같은 큰 기업에 협업을 하게 되면 클로버가 이상한 말을 할 수 있고 우리가 생각하지 못했던 말을 할 수 있을 거예요. 그러면 바로 신뢰성이 부족하거나 불확실성 어떤 답변을 할 수 있을지 결정할 수 없는 그런 문제점이 있습니다.
그래서 저희는 이 답변을 단순히 제너레이션을 푸는 게 아니라 통제 가능한 답변을 만들기 위해서 무한 개의 답변을 유한 개로 축소하는 전략을 취하게 되었어요.
유한 개의 답변으로 어떻게 적절한 대답을 하지? 어떻게 똑똑한 인공지능을 만들지 생각할 수 있는데요. 맞습니다. 그게 핵심 본질이에요. 어떻게 유한 답변을 가지고 실제로 많은 쿼리들을 커버할 수 있는지 저희는 계속 그것을 높여야 해요. 파이썬에서 테스트 코드를 이용해서 많은 테스트 커버리지를 높이기 위해서 노력하는 것처럼 저희도 유한 개의 답변이 얼마나 많은 무한한 스페이스에서 질문을 커버할 수 있는지 향상시키는 게 목적입니다.
그러면 이렇게 답변을 유한 개로 만드는 과정에서 답변을 어떤 것을 선택하는지가 굉장히 중요할 것 같은데요. 가장 간단한 방법은 실제 데이터에 사용자가 어떤 말을 하는지 알아보는 거예요.
어떤 말을 많이 하고 어떤 말들을 통해서 사용자의 커버리지 높일 수 있는지 알아보는 건데요. 그루트가 맨날 아임 그루트라고 하잖아요. 사용자들도 어느 정도 하는 말이 정해져있지 않을까? 제 카톡을 봐도 다양한 문장 또는 쿼리에 대응하는 문장을 쓸 때가 있지만 저도 카카오톡 데이터에서 분포를 비교해 보자고 해서 전체 데이터 분포를 비교해 봤어요.
빈도수 기준으로 디스트리뷰션을 봤고 상위 랭킹에 있는 말들이 리액션에 가까운 만들이었어요.
맛있게 먹어 사랑해 이런 식으로 간단하고 사용자들이 많이 쓰는 답변들을 볼 수 있었는데요. 전체 8000만 개 메시지, 제가 임의로 샘플링 했는데 이 중에서 20%를 차지했어요. 20%를 차지한다는 말이 20%를 절대적으로 커버한다는 얘기가 아니라 만약에 우리가 이 모델을 가지고 리액션를 만들게 되면 리액션을 이해하는 모델을 만들 수 있는 거잖아요.
그러면 단순히 무조건 리액션 답변만 예측하는 것이 아니라 우리가 리액션 할 수 있는 모든 답변에 대해서 커버를 할 수 있다는 거예요. 그러면 커버리지를 20% 이상 3배 이상의 커버리지를 나타낼 수 있어요.
그래서 우리는 많은 대화를 리액션를 이용해서 처리를 하게 됩니다. 대화에서 리액션은 다양한 상황에서 굉장히 유용하게 사용이 되는데요. 적절한 반응과 공감은 상대방의 대화를 유도하고 애착 관계를 형성하게 됩니다.
대화에서 상대방의 말에 공감하고 또 인스턴트하게 대응하기에는 리액션이 유용하기 때문에 다양한 상황에서 사용할 수 있고요.
그래서 이런 말도 있죠. 리액션만 잘해도 남자친구나 여자친구에게 이쁨 받는 대화를 할 수 있다는 말을 할 수 있을 정도죠.
그러면 진짜 사람처럼 리액션를 잘하는 모델을 만들게 되면 전체 빈도수에서 높은 커버리지를 만들 수 있지 않을까 라는 가설을 세우게 됐어요. 리액션를 잘해서 사람의 대화를 유도해 나가면 자연스럽게 사람같은 모습을 보여줄 수 있고 더 대화를 길게 해 나갈 수 있도록 생각하게 됐어요.
그래서 우리가 봤던 이 문제를 유한개 답변을 정하는 문제를 리액션 모델을 통해서 리액션이라는 유한개의 답변을 선택하는 모델로 바꾸게 됩니다.
그러면 실제로 모델을 한번 만들어볼까요? 아까 우리가 뽑았던 상위 1만개 답변 중에서 중복되는 것을 제외하고 비슷한 것은 묶고 해서 2000개의 리액션 클래스로 줄이게 됩니다.
줄인 이후에 앞에 있는 문맥을 인풋으로 주고 보시는 것처럼 다양한 문맥들 있죠? 문맥을 주고 그 다음에 나올 리액션에 대해서 예측하는 방식으로 데이터셋을 구성을 해서 리액션 학습 데이터셋을 만들게 됩니다. 그런데 저희가 가지고 있는 데이터의 장점은 다양한 사람들이 이야기를 하잖아요. 다양한 상황이나 문체가 나올 수 있어요.
그래서 그런 컨텍스트를 잘 이해하고 문맥을 이해할 수 있는 모델들을 만들게 됩니다.
실제 리액션 모델을 학습할 때에는 굉장히 많은 구조가 바뀔 것 같은만 단순한 모델 하나를 붙여서 만들 수 있게 되는데요. 버트 모델을 이용해서 그 뒤에 리액션을 클래시피케이션을 할 수 있는 레이어를 붙이게 됩니다. 모델의 많은 변경 없이도 단순하게 리액션 모델을 변경할 수 있게 되었고요.
이렇게 변경한 모델을 기존에는 텐서플로를 이용해서 버트를 학습을 시키게 되었잖아요. 저희가 리액션 학습하게 되면 다양한 실험이나 여러 가지 변수들로 실험을 해야 해요. 텐서플로가 적합하지 않다고 생각해서 이런 라이브러리를 이용해서 텐서플로에 있는 웨이트를 파이토치로 가지고 와서 학습을 하게 됐고 파인튜닝 과정을 진행하게 되었습니다.
멀티 GPU로 연결해서 학습을 하게 되었습니다.
그러면 설명은 다 끝났고요. 그러면 실제로 리액션 모델이 얼마나 잘 하나를 궁금해 할 것 같아요. 얼마나 잘 할지 알아보도록 할까요? 일단 첫 번째로는 리액션 모델은 다양한 상황에 대처할 수 있어요.
기존 봇이라면 IF 문으로 이런 상황에서는 이렇게 대답해라고 만들어줘야 했다면 이것은 알아서 판별해요.
보시는 것과 같은 다양한 상황에서 대처할 수 있고 우리가 인간적으로 생각하는 사회적 개념에 대해서도 이해를 하고 있어요. 예를 들면 오늘 월요일이라고 하면 알아요라고 시큰둥하게 얘기를 하지만 금요일이라고 하면 신나요 즐거워요 이런 식의 사회적 이해를 하고 있는 모델들을 만들 수 있게 됩니다.
또한 단순히 어떤 상황이나 이해하는 것뿐만 아니라 긴 문장도 잘 이해하고 구체적인 답변도 많이 해요. 어제 늦게 잤더니 아침도 못 먹고 일어났어라고 하면 이렇게 구체적으로 물어보는데 리액션에도 클래스가 다양하기 때문에 구체적인 답변이나 긴 문장에 대해서도 구체적인 답변을 낼 수 있는 특성을 내고 있습니다.
그리고 강조했던 부분 중에 하나는 문맥을 이해할 수 있다는 건데요.
저희가 만든 리액션 모델은 쿼리를 하나 보여드렸지만 여러 개의 쿼리를 이제 보여드리려고 해요. 이런 상황에서 우울해 무슨 일이 있어요? 이런 식의 문맥을 파악할 수 있는 답변이 가능한데요.
그러면 그럴까를 고정하고 문맥을 바꾸면 어떻게 될까요? 예를 들면 신난다 무슨 일이 있어요? 오늘 날씨가 너무 좋아 한강이라도 가요 이런 것처럼 어디에 갈지 어떤 곳에 갈지에 대한 문맥팔악하고 상대방에게 갈까요라고 질문을 해서 상대방의 대화를 유도하는 전략을 취하고 있어요.
파이콘 역시 제가 어제 밤 새서 발표자료를 만들어서 시도를 했는데 이따가 집에 가서 자면 되는 그런 위로를 해 주는 모습도 보여줄 수 있었습니다.
이런 데모 같은 경우에는 저희 홈페이지에 들어가 보시면 실제로 여러분께서 실시간으로 해 보실 수 있도록 다양한 문맥들을 넣어보시고 답변이 어떻게 나오는지 볼 수 있기 때문에 되게 재미있을 것 같아요.
또한 단순히 리액션 모델만 가지고는 사실 인공지능을 만들기가 어려워요. 왜냐하면 인공지능은 다양한 말을 해야 하잖아요. 그런데 리액션은 어떻게 보면 단순한 말 간단한 말들밖에 할 수 없기 때문에 구체적인 답변이나 아니면 특정 프로필에 대한 질문들 이런 것들을 대답할 줄 알아야 해요.
그렇기 때문에 리액션 모델뿐만 아니라 다른 모델들도 추가해서 저희가 보여준 것처럼 페이스북 메시지에서 서비스를 하고 있고 지금 제가 다 설명드리지 못하겠지만 이후의 모델들은 네이버 테크톡에서 발표한 자료가 있어서 아래의 링크를 보시면 도움이 되겠습니다.
그래서 저희가 앞으로 도전할 과제들은요, GPT로 만든 인공지능 소설처럼 답변을 선택하는 게 아니라 진짜 답변을 생성할 수 있는 모델을 만들어 보고 싶
고요. 아니면 개인화 답변, 사용자가 여자친구랑 헤어진 상태인지 답변이 달라질 수 있잖아요. 연애중이냐고 물어봤을 때 연애중이라고 대답할 수 있고 아닌 상태에서는 연애중 아닌데라고 대답할 수 있잖아요. 그런 개인화 답변. 그리고 페르소나 사용자마다 문맥이 다르기 때문에 예를 들면 알라딘의 캐릭터 말투를 이용해서 답변을 만들어라라는 문체변화 모델. 지금보다 더 긴 문맥을 이해할 수 있고 문맥이 바뀌면 그것을 캐치해서 문맥을 커트할 수 있는 더 긴 롱거 컨텍스트 모델. 이것을 총괄하는 더 똑똑하고 사람처럼 소름돋는 인공지능을 만들기 위한 모든 연구들을 앞으로 진행할 예정입니다.
그래서 우리랑 같이 이런 문제를 풀고 싶은 분들은 핑퐁팀에 오시면 굉장히 좋을 것 같고 뒤에 부스를 하고 있으니까 관심이 있는 분들은 오셔서 많은 데모랑 여러 이야기들을 하시면 좋은 것을 얻을 수 있을 것 같습니다. 이렇게 발표를 마치도록 하겠습니다. 감사합니다.
(박수)
-지금까지 발표해 주신 김준성 님께 다시 한 번 큰 박수를 부탁드립니다.
(박수)
질문이 있으신 분은 가운데 마이크 앞쪽으로 나와서 질문 부탁드립니다.
-발표 잘 들었습니다. 저는 개발자가 아니어서 프로그래밍에 대해서는 잘 모르는데요. 궁금한 점이, 카카오톡 데이터를 이용했다고 하셨는데 카카오톡 데이터가 음성언어하고는 전혀 다르고 이모티콘이라든지 축약어라든지 이런 정보들이 상당히 많은데 그것을 노이즈로 보고 그냥 제거를 하고 학습을 했는지 아니면 이모티콘 같은 거나 ㅠㅠ 니 ㅋㅋ니 이런 정보들이 감정적인 문맥을 파악하는 데 유용한 정보여서 어떻게 활용했는지 설명해 주시면 좋은 건데요.
-저희 내부에서도 노멀라이즈 과정이 전처리라고 해서 반복된 이상한 단어를 줄이는 로직이 있어요. 물론 사용자가 풍부한 답변을 할 수도 있겠지만 되게 ㅋㅋㅋㅋ 만 30개 치거나 똑같은 말만 몇 줄 이상 이렇게 하는 경우도 굉장히 많거든요.
그래서 이모티콘 같은 경우는 의미를 반영한다고 생각하기 때문에 그런 것은 많이 살리는 편이고요. 이에 반복되는 것이라든지 오타라든지 아니면 사용자가 봤을 때, 우리가 봤을 때도 그렇고 이상한 말이라고 하면 필터링하는 로직이 앞에 들어가 있습니다. 하지만 최대한 구어체이기 때문에 어떤 말이 나올지 모르잖아요. 오타도 굉장히 많고 어떻게 대답하는지 문체도 다르기 때문에 한정할 수는 없어요. 이것을 무조건해야겠다. 그래서 저희는 최대한 안 좋은 문장이 나와도 잘 이해할 수 있도록 모델이나 아니면 전처리 과정에서 노력을 많이 하고 있습니다.
-죄송한데 그러면 같은 단어를 여러 개의 비표준적인 철자로 표기할 수 있잖아요. 그런 경우에는 다 살려서 이해할 수 있도록 하신 건가요?
-아니에요. 줄이는 경우가 더 많은 것 같아요.
-하나보다 표준적인 철자로 통일?
-예를 들면 크기 6글자 이상 반복되면 2글자나 3글자로 리듀싱하는 방식처럼 그런 축약의 방식으로 의미를 축약하는 방식을 취하고 있습니다.
-질문이 있는데요. 이런 코퍼스는 어떤 코퍼스 위주로 사용을 하시나요? 코퍼스마다 대화에 대한 그것을 수집한 코퍼스에 따라서 다른 대답이 나올 것 같은데 핑퐁 관련된 코퍼스는 주로 어떤 걸 사용을 하셨나요?
-여러분도 많이 궁금해 하실 것 같은데요. 저희가 사용하고 있는 데이터, 그러니까 이런 대화 데이터를 어디에서 얻을 수 있었냐 하면 저희가 맨 처음에 시작했던 서비스가 연인간의 카톡 데이터를 보내주면 연인 간에 서로 얼마만의 감정도를 가지고 있는지 썸탈 확률은 얼마나 되는지 이런 것들을 분석해 주는 서비스를 먼저 시작을 했어요.
그때 저희가 수집했던 데이터들, 사용자들이 카카오톡 데이터를 내보내기해서 저희한테 Email를 보내주신 거든요. 그때 머신러닝 연구 목적으로 사용해도 되는지 동의받고 수집한 데이터를 저희가 사용하고 있습니다.
-여쭤보고 싶은 게 있는데요. 발표 잘 들었습니다. 그런데 우선 제가 생각한 모델은 학습을 하고 대답을 한다고 했을 때 저는 당연히 이 전에 있는 문장에 따라서 대답을 한다고 생각했는데 발표 자료에서 그럴까라고 했었는데 이 전에 있었던 내용들에 대해서 답변이 바뀌는 것을 봤거든요. 그래서 그거 같은 경우에는 어떻게 이어지는 건지 아직 개념이 잘...
-그것은 버트의 모델 구조랑 어떻게 컨텍스트를 이해하는지에 대한 조금 더 이해가 필요로 하는데요. 예를 들면 저희가 봇이랑 얘기를 할 때 단순히 전에 얘기했던 것을 가져오는 게 아니라 앞에 위에 얘기했던 히스토리 5턴 7턴 정도를 인풋으로 모델을 같이 넣는 거예요. 그러면 모델은 전체적인 이게 아까 저희가 버트 모델을 보여드렸잖아요. 버트 모델의 컨텐스트를 넣으면 어떤 이야기를 하고 있구나. 어떤 답변을 하고 있고 이런 질문을 했는데 문맥을 고려했을 때 버트 모델이 이해하게 됩
니다. 이해한 것을 바탕으로 이해하는 모델을 학습시켜서 모델을 진행할 수 있게 만들어졌습니다.
-감사합니다.
-네.
-강의 잘 들었습니다. 아까 1만개 정도 되는 단어들을 2000개로 비슷한 단어끼리 묶어서 줄였다고 했는데 그 과정을 혹시 자세하게 알 수 있을까요?
-여러 가지 과정을 사용했는데요. 챗봇 기획자 분들이 보시는 케이스도 있고요. 저희가 내부적으로 팀에 전체 데이터들에서 검증을 해 주시고 확인하는 분들이 계세요. 그분들께서 이런 부분은 우리가 뭉치면 잘 클래시피케이션을 할 수 있겠다. 이것은 중복되겠다. 이것은 리액션할 수 없겠다고 처내기도 하고 저희가 프로그래밍적으로 유사한 문장인지 아닌지 파악으로 해서 전처리로 드리는 케이스도 있습니다. 사람과 머신러닝의 중간적 요소가 들어가서 클러스터 했다고 생각하시면 될 것 같습니다.
-그러면 사람들이 1만 개 정도 단어들 중에서 2000개 줄이는데 코스트가 많이 드는 거 아닌가요?
-어떻게 보면 그럴 수도 있을 것 같은데 저희가 하는 태스크가 클래스가 10만 개짜리도 있고 문제가 엄청 많아요. 그런데 그것에 비해서 1만 개 정도로 줄이고 축약하는 것은 2~3명이서 나누어서 하면 큰 문제는 아닙니다. 왜냐하면 모델이 전처리를 했기 때문에 후처리로 사람이 검증을 하는 거지 사람이 다 하는 것은 아니기 때문에.
-감사합니다.
-챗봇이다 보니까 반응 속도가 중요할 것 같은데 평균적인 속도와 문장이 길어지면 느려지는지 궁금합니다.
-제가 자랑하고 싶은 게 그거인데요. 지난분기에 최적화하는 작업을 많이 했어요.
CPU에서도 100ms 안에서도 서비스할 수 있도록 최적화를 했고요. 지금도 실시간으로 서비스를 하고 있습니다.
-안녕하세요? 발표 잘 들었고요. 궁금한 게 있는데 텍스트 자체를 머신러닝이 이해할 수 있게 피처로 바꾸려면 기본적으로 형태소 분석이 필요한데 거기에서 어떻게 형태소로 잘랐는지 범위를 제거했는지 팁을 알려주시면 가할게요.
-저희는 말씀드렸던 것처럼 최대한 원본을 보존하려고 해요. 왜냐하면 이상한 문장이기는 해도 어떤 의미를 담고 있을 거잖아요. 저희가 형태소를 삭제하거나 그런 케이스는 없고요. 저희는 사용하는 것은 토크나이저라고 해서 옛날에 나왔던 트위터 토크나이저나 이런 여러 가지 토크나이저나 어떤 것을 선택하냐의 문제일 수 있을 것 같은데 저희가 블로그에 어떤 토크나이저를 선택했는지 명확하게 분석을 해 놨어요. 왜 이게 좋고 이런 성능이 되고 블로그에 올라왔으니 참고하시면 좋을 것 같습니다.
-시간이 1분 정도 남아서 지금 질문하시는 분까지 받고 다음분은 앞에서 질문 부탁드리겠습니다.
-버트 사용에 대해서 궁금한데요. 제가 알기로 버트가 프리트레이닝 모델을 사용해서 위키 아니면 공개하는 소설 같은 것을 이용하는 것을 알고 있는데 여기에서는 어떤 모델 프리트레인드 했는지 궁금합니다.
-저희가 공개되어 있잖아요. 저희가 그것을 쓸 수 있을지 평가를 해 봤는데요. 대화 데이터랑 도메인이 달라서 쓸 수가 없는 결론을 내게 되어서 저희가 이것은 사용하지 않고 직접 프리트레인드는 시켜서 학습을 한 케이스입니다.
-그러면 따로 없이 카카오톡이나 말씀하신 데이터로 처음부터 학습시켰다고 보면 되는 거죠?
-맞습니다.
-이것으로 이번 세션 모두 마치겠습니다.
발표해 주신 김준성 님께 다시 한 번 큰 박수를 부탁드립니다. 그리고 질문해 주신 앞에서 네 분까지 앞으로 나와서 책 받아가시면 좋겠습니다.