https://youtu.be/BCBYmhnNzGQ

-안녕하세요? 이번 시간에는 이홍주 님께서 집에서 만든 머신러닝 기반 자동 번역기라는 제목으로 발표해주시겠습니다. 원활한 발표를 위해 질의응답은 발표 후 시간이 남으면 진행하도록 하겠습니다. 그럼 큰 박수 부탁드립니다.
-안녕하세요? 소개받은 이홍주입니다. 세션 제목이 집에서 만든 머신러닝 기반 번역기인데요. 사실 슬라이드 제목은 이거예요. SMT 베이스 로만 투 코리아 트랜슬레이터. 잠깐 읽어드리겠습니다. 이거예요. 막상 봤는데 도대체 이걸 어디다 쓰느냐 이런 생각하실 분들이 분명 있을 겁니다. 그래서 설명을 조금 드리고 시작할게요.
저는 윈도우 3.1 이럴 때도 컴퓨터를 썼습니다. 많은 분이 그러셨을 거지만 사실 이런 얘기하면 어떤 분들은 자기는 수업시간에 천공카드에 OMR 구멍 뚫어서 제출했다는 분도 있는데 도스 시절도 있지만 한글 입력하는 게 그렇게 불편했어요. 가령 한글 입력기가 윈도우에 기본으로 딸려오지 않는 경우도 있었기 때문에. 윈도우하고 랭귀지 팩을 따로 구매하다 보니까 해외를 나가거나 할 때 랭귀지팩 CD를 안 가지고 가면 그 나라 CD를 써야 하니까 한글을 쓸 수 없게 됩니다. 채팅을 할 때 곤란한 상황이 발생하는 거죠.
이 영화 아시는 분들 접속이라는 영화 97년도 영화죠. 무슨 영화냐면 위가 한석규, 이분 아이디가 해피엔드. 전도연 여인2라는 아이디로 나오죠. 채팅으로 밀당하는 내용입니다. 안 보신 분들 한번 보세요. 둘이 채팅하는 장면이 있죠. 저도 이런 경험을 여러 번 해봤는데. 이게 둘이 처음 채팅에서 만났던 그때 영상이에요. 해피엔드 한석규가 이수현님이 맞습니까? 전도연이 끄덕끄덕하죠. 10초 전으로 돌아가서 한석규가 말을 처음으로 걸었는데 한글 입력이 안 되면 어떻게 할 거예요?
혹은 전도연이 해외출장이라 한글을 쓸 수 없어요. 지금 같으면 이런 방법이 있겠죠. 이응을 대문자 O로 쳐야겠지만 당시에는 OO이라는 말이 없었어요. 이렇게 했을 리 없죠. 저는 이렇게 했습니다. 예스 아이 엠, 유창한 영어로. 상대방이 되게 싫어해요, 재수없다고 하고. 깐느 여왕이니까 유창한 영어로 대답할 수 있었겠지만 제일 현실적인 방법이 이렇게 알파벳으로 우리 말을 치는 거예요. 상대방이 별로 좋아하지는 않습니다.
그런데 어떤 사람들은 재미있게 받아들이기도 했어요. 진짜 재미 없는 경우가 영문으로 영어 발음나는 대로 쓰는 거. 이런 식으로 해외에서 이메일도 써본 적이 있어요. 윈도우 가면 한글 입력 안 되는 컴퓨터를 자주 만나게 되거든요. 저는 이런 경험들을 오래전에 했는데 어느날 구글 인풋툴스라는 걸 보게 됐어요. 안 써본 분들 많을 거예요. 이 url 접속하면 경험할 수 있는데 저걸 활성화시켜놓고 이메일 같은 데다 영문으로 가령 인도 말 같은 경우 나마스테 치면 현지 말로 타이핑됩니다.
지원하는 오른쪽 테이블 보면 언어가 굉장히 많은데 크로스 된 거 보시면 트랜슬레이션을 지원하는 목록인데 코리아는 없습니다. 그래서 내가 만들어보자고 시작한 게 조금 아까 보신 거예요. 저걸 만들기 위해서 몇 가지 궁리를 했는데 사실 머신 트랜슬레이션 기술을 사용해서 만든 거거든요. 일반적인 자동 번역기 기술을 그대로 사용했기 때문에 머신 트랜슬레이션에 대해서 일반적인 설명을 드리고 이 프로젝트를 일반적인 머신 트랜슬레이션 문제에 대입해서 트레이닝하고 만드는 과정에서 이런 순서대로 설명을 드리겠습니다. 이 사람이 자기 논문에다 그린 삼각형인데 소스텍스 텍스트가 있고 타겟 텍스트가 있죠. 번역하는 단계를 도식화했는데요. 상중하로 3단계로 나눠져 있는 걸 볼 수 있습니다.
가장 아래에 있는 다이렉트 트랜슬레이션부터 확인하면 단어 하나하나 번역해서 나열하는 방식이 되겠죠. 어떤 언어에 대한 분석이 없다 보니 문제가 생기는데 어순이 다른 경우 번역할 수 없게 돼요. 예시에 있는 것처럼 영어를 번역하면 우리나라 말과는 어순이 다르기 때문에 그대로 단어를 나열하게 되면 소식통은 어제 IBM이 로터스를 샀다고 말했다, 이런 식으로 번역하게 되거든요. 어순이 뒤바뀌어 있죠. 그다음에 같은 단어가 다른 의미로 사용되는 경우에도 이건 대응할 수 없습니다.
그 밑에 댓 하고 밑에 있는 댓하고는 의미가 다른 말이잖아요. 얘는 구분하지 못해요. 이런 문제가 있죠. 동일한 의미를 가진 타깃 언어를 제너레이션해주는 형태의 트랜슬레이션 방법이에요. 요즘 나오는 딥러닝 번역 기술이 여기에 되겠죠. 소스 언어의 스트럭처를 분석하고 분석하는 화살표가 올라가죠. 어느 단계에 이르면 그걸 타깃 언어 트랜스퍼, 타깃 언어를 제너레이션하면서 밑으로 내려오는 접근법인데 여기서 말하는 언어의 구조는 3개로 나뉠 수 있어요. 오른쪽에 보시는 것처럼 워드 베이스의 경우. 매칭되는 단어로 바꿔주는 그리고 순서를 재정렬하는 워드 베이스 트랜슬레이션이고 중간에 있는 것처럼 얼라이먼트하고 어구 같은 걸 추출해서 그걸 베이스로 똑같은 일을 하는 게 프레이스 베이스고 마지막으로 문장의 문법적인 구조를 서브트리 단위로 번역하고 순서 정렬도 하는 이게 신텍스 베이스 트랜슬레이션입니다. 워드 베이스 트랜슬레이션이 원시적인 건지는 머신 트랜슬레이션 역사를 보면 알 수 있어요. 2013년이면 이 무렵부터 딥러닝에 대한 이야기가 활발했던 시기거든요.
결국 2016년부터는 기존 SMT 베이스 번역기를 전부 대체해버립니다. 3년 만에 학계에서 얘기된 게 프로덕션 레벨까지 가는 건 굉장히 3년이라는 시간은 짧은 거거든요. 그만큼 활발한 연구가 이루어졌던 거죠. 파파고도 2016년에 나왔어요. 그런데 자동번역기 연구는 사실 80년대 이전부터 시작됐거든요. 88년도에 획기적인 연구결과를 내놓기 전까지 좋은 결과가 나오지 못한 채로 많은 시간이 흘렀습니다. 비용이 많은 연구이기 때문에 하지 말아야 한다고도 하고 연구소에 펀딩이 끊겨서 연구를 못하는 상황에 이르기도 했어요.
사실 SMT 기술이 활발하게 발전하는 데 굉장히 중요한 키가 됐던 이론이나 기술들은 1940년대에 만들어진 코딩 이론이나 암호화 기술이었거든요. 심지어는 더 거슬러 올라가서 18세기 되게 큰 역할을 해요. 우리는 거기서부터 얘기를 시작해볼 거예요. 훗날 48년에 노이즈 채널이라는 이론이 머신 트랜슬레이션에 큰 기여를 하게 되죠. 통신할 적에 통신상 에러를 검출하거나 수정하고 암호화, 2차 세계대전 직후였기 때문에 암호화하는 데 사용됐던 기술입니다. 이 노이즈 채널이 머신 트랜슬레이션에서 어떤 기여를 하냐면 노이즈 채널을 통해서 아웃풋이 부에노스 디아스라고 스페인어로 출력되는 겁니다.
이걸 계속 반복하면 노이즈 패턴을 파악할 수 있겠다고 생각했던 거죠. 이 그림을 좀 더 이해하시려면 그리고 앞으로의 내용을 더 이해하시려면 익숙해져야 할 게 있는데요. 파란 상자 속 이 대문자는 소스 랭귀지예요. 대문자는 언어를 뜻하고 잉글리시의 E겠죠. 그거에 속한 소문자 a는 한 문장을 뜻하는 겁니다. 마찬가지로 랭귀지 F는 소문자 f는 포린 랭귀지의 포린 센텐스가 되겠죠. SMT 왓슨 연구소에서 얘기했던 논문에서 사용했던 게. 마침 F가 포린 랭귀지의 F라는 중의적인 의미도 있었기 때문에 언어에 상관없이 타깃 텍스트는 E, 소스 텍스트는 F로 사용되는 경우가 많습니다. 앞으로 계속 사용될 기호이기 때문에 우리가 프렌치에서 영어, 포린 랭귀지에서 영어로 번역한다고 이해하시면 돼요.
아까처럼 소스 랭귀지가 입력되었을 때 타깃 랭귀지 F가 출력되면 우리가 배울 수 있는 확률 모형은 이에 대한 F의 조건이 되겠죠. 사실 우리가 하고 싶은 트랜슬레이션은 맨 밑 F를 입력하는 P 함수거든요. 이건 소스가 프렌치여야겠죠. 그래서 소스 F에 대한 E의 조건부 확률을 계산하게 되죠. 방향이 반대죠. 우측에 이 베이즈 룰이 이 식이 대입돼서 이렇게 바뀐 거예요. 이 밑에 분모에 f에 대한 확률이 없어졌죠. 아그맥스 e를 실행하면서 그냥 생략한 겁니다. 트랜슬레이션 시스템은 이 위 식처럼 표시해요. f를 입력하고 그건 e에 대한 확률과 조건부 확률에 대한 곱 아그맥스의 결과. 이 식에서 우리는 세 가지로 나눌 수 있어요. 첫 번째가 랭귀지 모델 P고요. 마지막으로 디코더입니다. 아그맥스.
랭귀지 모델은 번역 유창함을 담당해요. 그래서 랭귀지 모델을 학습할 때는 영어 코퍼스 하나만 있으면 되겠고요. 트랜슬레이션 모델은 번역기의 정확도를 맡게 되는데 이걸 트레이닝하려면 포린 랭귀지와. 마지막에 디코더는 위에서 트레이닝한 랭귀지 모델과 트랜슬레이션 모델을 가지고 스코어링한 다음에 제일 좋은 결과를 준 문장 e를 찾아내는 거예요. 그래서 디코더라고 불러요. 검색 영역이 되겠고요. 생각보다 시간이 빨리 가네요.
이걸 시스템 관점에서 보면 이런 식으로 볼 수 있는데 넘어가서. 이게 트랜슬레이션 모텔을 통했을 경우 몇 가지의 캔디데이트 문장을 만들어내게 돼요. 그것의 트랜슬레이션 프로버블리티는 오른쪽에 빨간색 표시된 게 되겠고 제일 좋은 결과를 보여주는 거는 맨 아래에 있는 거. 뭔가 부정확하죠. 여기 랭귀지 모델이 어떤 역할을 하냐면 사실 영어를 가지고 학습시킨 랭귀지 모델에 대해서 이 문장들을 입력합니다. 그럼 그것에 대한 확률이 리턴되고 그걸 곱해줬을 경우에 그 결과는 바뀌게 돼요. 그럼 아까는 거의 꼴찌였던 아임 소 헝그리가 이번에는 가장 높은 확률로 아그맥스에 의해서 출력될 수 있도록 되는 거죠. 세 가지 시스템이 같이 워킹해서 트랜슬레이션 시스템이 구성되는 거예요.
이렇게 SMT는 세 가지 컴포넌트 시스템으로 구성돼 있는데 이거 각각에 대해서 어떻게 학습시키고 어떻게 구현하는지 보도록 하겠습니다. 먼저 랭귀지인데요. 위처럼 세 가지 문장으로 구성된 미니 쿠퍼스가 있다고 생각하죠. 아이엠이라는 워드 시퀀스가 주어졌을 때 그다음 단어 나올 확률은 무엇이 될까. 확률로 표시하면 피 오브 샘 기븐 아이엠. 저 확률을 구하려면 아이엠 샘이 출현하는 빈도를 카운트 수로 나눠주면 돼요. 1/2이 답인데 다음 단어가 나올 조건부 단어를 트라이그램. 바이그램이라면 이런 식으로 아이 다음에 엠이 나올 확률은 아이 개수를 카운트하고 아이엠의 개수를 카운트 해서 나눠주면 2/3 확률을 구할 수 있고 이게 바이그램이라고 하고요.
그래서 랭귀지 모델은 조건부확률이 아니라 문장 전체에 대한 확률도 알려줄 수 있습니다. 주어진 문장이 실제로 얼마나 있을 법한 얼마나 자연스러운 문장인지 스코어로 나타내줄 수 있는 역할을 하는 거죠. 이제 정리해보면 일반화시켜서 이야기했을 때 랭귀지 모델이란 어떤 시퀀스가 있으면 그것보다 짧은 길이의 워드 시퀀스에 대해서 그 다음에 출현할 수 있는 단어 조건부 확률 구하기도 하고. 이런 랭귀지 모델을 어디 활용하냐. 사실 여러분 주변에서 굉장히 많이 활용되고 있어요. 검색엔진에서 워드 입력하면 그다음 글자나 다음 단어가 나오잖아요.
여러분이 이메일 한다든지 문자메시지 보낼 때 몇 글자 치면 내가 원하는 단어들이 나오잖아요. 그게 평소 여러분이 입력하고 있는 글자에 대해서 이런 바이그램 트라이그램을 학습하다가 보여주는 거예요. 굉장히 많이 유용하게 사용되는 랭귀지 모델이죠. 이걸 어떻게 계산할 수 있을까. 이 단어의 조합이 너무 많기 때문에 그 많은 조합에 해당하는 스트링을 찾아낸다는 건 불가능한 일이에요. 그렇기 때문에 우리는 에스티메이션해야 하고 그 방법은 체인룰을 사용하고 그래도 버겁기 때문에 미래의 스테이트는 현재 또는 과거 몇 개의 스테이트만 의존한다는 가정이거든요.
그렇기 때문에 긴 체인에 대해서 굉장히 짧게 줄일 수 있게 돼요. 그래서 그런 옵션 사용하게 되면 맨 밑에 수식대로 이걸 에스티메이션할 수 있게 되고 긴 조건에 대한 i번째 조건부 확률은. 그래서 바이그램을 구할 수 있게 됐으니까 아까 봤던 미니코퍼스에 대해서 전체 시퀀스에 대해 아이엠샘이라는 프로버블리티를 구하게 되면 I의 확률, 길게 설명드리지는 않을 텐데 확률을 고르게 분포시키기 위해서 스타트하고 문장의 시작과 끝을 표시하는 S라는 태그를 사용해서 앞의 것을 조건부확률로 시작하게 되고 끝에도 마지막 단어에 대한 엔드 조건부 확률로 끝나도록 이런 식으로 확률을 구하게 됩니다.
그렇게 구하면 아래에 있는 것처럼 스타트 아이엠샘 앤드에 대한 확률은 또 다른 값을 가지게 되죠. 똑같은 코퍼스 가지고 바이그램 랭귀지 모델을 만드는 코드입니다. 그다음에 트랜슬레이션 모델인데 트랜슬레이션 모델이라는 건 주어진 영어 문장에 대해서 가장 가까운 프렌치 문장 포린 문장을 얻어내는 거예요. 그러려면 어떻게 계산해야겠어요? 어떤 단어가 또 다른 단어에 얼마나 잘 매칭되는지를 우리가 계산해야겠죠.
그런데 아까 예를 들었던 모닝이라고 매칭됐는지 투모로우라고 매칭됐는지 이런 걸 계산하는 조건부 확률이 바로 트랜슬레이션 모델이 되겠습니다. 이것의 문제가 뭐냐면 언어마다 어순이 달라요. 그래서 예시를 여기서 든 건. 6개의 단어로 이루어져 있죠. 프랑스 말 읽지 않겠습니다. 일곱 단어로 이루어진 문장으로 매칭되는데 이것의 얼라이먼트는 순서가 각자 다릅니다. 얘 같은 경우는 3개 단어에 매칭되죠. 어떤 경우에는 한글과 우리 말과 영어 같은 경우에는 순서가 X자로 엇갈리기도 해요. 그것의 워드 간 얼라이먼트부터 정확하게 얻어내야 그다음에 트랜슬레이션 모델을 찾을 수 있으니까요. 트랜슬레이션 모델을 만들기 위해서는 워드에 대한 얼라이먼트 문제부터 풀어야 합니다. 왜 머신 트랜슬레이션이 비슷하냐면 얼라이먼트 같은 것을 누군가 레이블링을 해야 해요. 그쪽 사람들에게 돈을 주고 수십만 개의 문장에 대해서 레이블링하라고 하면 비싼 연구가 되는 거죠. em 알고리즘인데 시간상 뛰어넘을게요. 이런 거죠. 우리가 얼라이먼트를 알고 있어요. 그럼 트랜슬레이션 파라미터를 알 수 있죠. 트랜슬레이션 파라미터를 알고 있으면 다른 단어에 어떻게 매칭되는지 알 수 있기 때문에. 어떤 게 먼저 선후 관계에 있다고 할 수 없기 때문에 익스펙테이션 값을 구하고 반복함으로써 가장 프로버블한 얼라이먼트를 풀어내는 거죠.
그리고 그걸 가정으로 해서 트랜슬레이션 모델을 만들어내게 됩니다. 처음에는 어떤 단어가 매칭되는지 모르지만 이엠 알고리즘을 돌면서 굵어지는 선들이 있죠. 이것과 매칭되겠구나 얼라이먼트 알게 되고 알게 되면 밑처럼 해당되는 단어끼리 트랜슬레이션 프로버블리티를 구할 수 있기 때문에 그걸 집합해서 트랜슬레이션 모델을 만들 수 있게 되는 거예요.
이 코드는 그렇게 하는 알고리즘이 여러 가지가 있어요. IBM 1에서 5 있고. 가장 원시적인 모델1 코드를 조금 줄여서 적어놨으니까 나중에 시간 되시면 보시고요. 마지막으로 디코더인데요. 디코딩은 검색입니다. 랭귀지 모델과 트랜슬레이션의 곱이 가장 큰 문장을 찾아내는 거죠. 아더맥스로. 그런데 사실 디코딩은 엠피 컴플릿 문제예요. 트랜슬레이션 모델을 통해 얻어낸 뽑아낸 타깃 언어의 단어들을 그래프의 로드라고 보고 그 사이 에지를 바이그램 랭귀지 모델의 웨이트라고 생각하면 됩니다. 그랬을 경우 가장 자연스러운 조합으로 한 줄 긋게 하는 거예요. 한 줄로 쫙 다 긋는 거.
그래서 굉장히 어려워요. 이 문제를 풀기 위해서 이건 만약에 모델 러닝 트레이닝 단계면 시간이 오래 걸려도 괜찮아요. 이 디코딩은 인퍼런스하는 단계거든요. 아까처럼 내가 뭐라고 감사합니다 입력하면 아웃풋이 바로 나와야 하는데 오래 걸리면 사람들이 안 쓰겠죠. 속도에 민감하기 때문에 사용할 수 있는 솔루션은 다이내믹 프로그램이 있고. 이전에 풀었던 서브 프로블럼을 저장해놨다가 나중에 다시 풀지 않게 하는 프로그램이죠.
빅토리 알고리즘이라고 있는데 스테이트 개수가 고정돼 있어야 하고 그것과 유사한 게 빔 서치 스택팅 코딩이라는 방법이 있는데 비터비 서치를 하면서 전부 다 리스톱해서 트리를 서칭하는 게 아니라 베스트 n개 모아서 서칭해놨기 때문에 속도가 훨씬 빠릅니다. 빔서치 스택팅 코딩을 구현해서 사용했고 스택팅 코딩인데 스택 사이즈를 조절함으로써 내가 담는 캔디데이트 숫자를 톱 n개로 조정함으로써 서칭 속도를 빠르게 할 수 있어요.
한글 다 아시니까 한글 특성에 대한 건 생략하고요. 어떤 언어를 로만 표기로 바꿔서 쓰는 거예요. 가령 뒤에 보시는 한글로 된 케이팝 가사를 알파벳으로 써놓은 걸 로마나이제이션이라고 합니다. 반대로 제가 아까 구현해서 보여드렸던 건 우리말로 한글 라이팅 시스템으로 바꿨으니까 백 로마나이제이션이겠죠. 로만 투 코리안 트랜슬레이션하고 같은 겁니다. 약간 의미상 헷갈리는 게 있어서 트랜슬레이션 반드시 앞에 로만 투 코리안이라고 하면.
이 시스템 목적은 로만 캐릭터로 발음나는 대로 사용된 워드를 한글 시스템으로 다시 써주는 그런 거죠. 그런데 이게 살펴보면 아까 얼라이먼트 문제하고 똑같아요. 가령 한글이라 쓰면 한글이라고 영어로 여러 가지를 쓸 수 있어요. 여러 가지 경우의 수가 생겨요. 그러니까 룰베이스로 코딩하려면 너무 많은 룰베이스로 코딩할 수 있기 때문에 어렵습니다. 머신러닝으로 해야겠다고 생각한 이유고요. 세그멘테이션이 있어요. 칸막이라고 썼을 때 우리는 아니까 랭귀지 모델이 내장돼 있기 때문에 이걸 칸막이 이렇게 얼라이먼트를 하는데 외국 사람들이 읽을 때는 칸마기 이렇게 얼라이먼트 할 수 있겠죠. 랭귀지 모델하고 얼라이먼트 모델이 우리 안에 있기 때문에 세그멘테이션해줄 수 있는 거거든요.
한글 단어에 대한 랭귀지 모델링을 통해서 풀어보자, 이렇게 해서 만들어진 게 코리아나이저고요. 처음에는 캐릭터를 자음과 모음을 분리해서 로만 캐릭터하고 매칭시킨 다음에 재조합시키는 방법을 쓰려고 했는데 문제점이 있어서 로만 캐릭터하고 우리 음절 자음 모음 받침이 합쳐진 음절의 덩어리 세그먼트하고 얼라이먼트 하는 방법으로 다시 바꿔서 트레이닝했어요. 그래서 디코딩은 말씀드린 대로 아그맥스를. 사실 코리아나이저는 몇 년 전에 만들었습니다. 우리말을 발음나는 대로 영어로 쓴 걸 어디서 봐요? 구할 수 없었어요.
되게 제한된 양밖에 없었는데 얼마 전에 우연히 케이팝 팬들이 케이팝 한글 가사를 영어 캐릭터로 만든 걸 발견하려고 했습니다. 많이 사용하시는 라이브러리를 사용해서 1만 2000개에서 가사 150만여 줄을 취했고 그중 모르는이라고 바이워드 페어스를 190만 개를 얻을 수 있었습니다.
이거를 아까처럼 트랜슬레이션 모델 트레이닝하고 한글 단어에 대해서 두 개 곱하고 출력물을 보인 게 이거죠. 당분간 띄워놓을 거니까 갖고 놀아 보시고요. 발표 들어주셔서 감사합니다.
-발표해주신 이홍주 님 감사합니다. 혹시 질문 있으신 분이 없으시면 지금 발표 시간이 종료되어서. 딱 한 질문만 받겠습니다. 짧고 간단하게 부탁드립니다.
-비슷한 문제로 고민하고 있었어서 저는 영문으로 된 한국인들의 이름을 한국 이름으로 바꾸는 예를 들면 HONG ~ 이렇게 된 걸 홍길동으로 바꾸는 데 관심 있고 실제로 필요한 데이터를 제가 눈으로 보면서 검색도 하면서 바꿔본 적이 있거든요. 아마 사용하신 걸 응용하면 바로 적용도 할 수 있을 것 같은데 어디 코드를 공개해주시거나 어쨌든 트레이닝은 이게 지금 이거로 제 데이터를 바로 바꿀 수 있을지 확신은 안 드는데 질문 드립니다.
-질문의 요지는 소스를 공개해달라는 말씀이시죠? 사실 그거는 질문이 아니기 때문에 대답을 여기서 해드릴 필요는 없고 같이 얘기해보면 될 것 같습니다. 끝나고 저랑 얘기해보면 될 것 같습니다.
-발표 시간이 종료돼서 세션을 마무리하도록 하겠습니다. 지금까지 발표해주신 이홍주 님께 큰 박수 부탁드립니다. 지금부터 쉬는 시간을 가지도록 하겠습니다. 중앙 홀에 마련된 스폰서 부스에서 다양한 프로그램을 즐겨 주시면 감사하겠습니다. 감사합니다.