-곧 발표가 시작될 예정이니 자리에 앉아주시기 바랍니다.
안녕하세요?
이번 시간에는 이준범 님께서 온라인 뉴스 댓글은 정말 사람들의 목소리일까? PART2라는 제목으로 발표해주시겠습니다. 질의응답은 발표 시간이 남으면 진행하도록 하겠습니다.
그럼 큰 박수 부탁드립니다.
(박수)
-(이준범) 안녕하세요? 온라인 뉴스 댓글은 정말 사람들의 목소리일까? 파트2 발표를 진행할 이준범입니다. 반갑습니다.
(박수)
그래서 본격적으로 발표 시작 전에 작년과 같은 걸 진행하고자 해요. 작은 부탁을 드릴 텐데요. 모든 분들 한 손을 들어주시겠어요? 네, 감사합니다.
여기서 내리시면 안 돼요! 손 들어주고 계셔야 되는데 온라인으로 뉴스를 보지 않는다, 손 내려주세요.
많은 분들이 손을 들어주고 계시는데요. 이번에는 나는 온라인 뉴스는 보는데 댓글은 보지 않는다 손 내려주세요. 아직도 많은 분들이 손을 들어주고 계세요. 굉장히 많은 분들이 손을 들고 있는 걸 보실 수 있는데요. 저도 여러분과 같이 손을 들고 있어요.
모두 손을 내려주셔도 됩니다. 감사합니다.
우리 이 문장 굉장히 익숙하실 거예요. 대한민국은 민주 공화국이다라는 걸로 시작하는 대한민국의 헌법 제일 첫 부분인데요. 헌법의 가장 처음에 나오기도 하고 우리나라를 가장 잘 설명하는 문장이기도 합니다.
그런데 여기에서 이런 말이 나와요.
이런 말이 나오는데 한국어로는 민주주의에서 가장 중요한 건 정보를 잘 전달받을 유권자다. 우리는 정보를 어디서 얻고 있냐? 온라인에서 기사를 보고 비판적으로 바라보는 사회예요. 기사자체보다는 댓글, 사람들이 쓴 거니까 때묻지 않고 사람들이 쓴 소스를 통해 새로운 정보를 원천으로 하기도 하는데요.
최근에 우리가 이 문장에 대해서 물음표를 내기 시작했어요.
정말 댓글이 덜 때묻은 정보의 원천으로 쓸모가 있느냐? 이런 사회적 상황을 보여주는 게 네이버 웹툰으로 등장해요. 작년 파이콘에서 사용했던 장면인데요.
어떤 기업가가 다른 기업을 인수하려고 하는데 부정적인 것이 올라오니까 언론사에 압력을 넣는 대신 이런 말을 합니다. 시민들은 댓글을 더 믿는 편이니까. 이런 것처럼 본문보다 댓글을 더 믿고 있는 그런 게 사회적으로 이슈가 되고 있지 않나. 그 댓글이 제대로 돼 있나 하는 의문인 거죠. 우리가 보는 인터넷에서의 댓글들은 정말 우리의 세상을 제대로 반영해주고 있냐라는 게 가장 큰 질문입니다.
본격적으로 시작하기 전에 이번에도 간단하게 해볼 텐데요. 사회적인 내용이 들어가 있을 수 있는데 정치, 사회적 내용을 다뤄보자는 게 아니라 사회적인 데이터를 분석할 때 어떻게 접근해야 하는지 기술적인 접근 방법에 대해 이야기하는 것이기 때문에 제시된 내용이 파이콘 전체의 입장을 표명하지 않음을 알려드립니다.
소셜 데이터, 소셜 댓글 이런 얘기를 하는데 왜 이걸 봐야 하는가? 먼저 다뤄야 해요. 소셜 데이터를 본다는 건 저는 이렇게 생각해요. 우리 사회에서 나타나는 어떤 현상들을 설명한다는 거예요. 수없이 루머들이 떠오르고 거기서 어떤 게 팩트인지 혹은 얼마나 퀄리티컬, 혹은 우리가 보는 댓글들이 정말 우리 사회를 대표를 하고 있는지 그런 것에 대해서 질문을 하는 거죠.
한편 소셜 데이터를 분석하는 걸 통해 우리 자신에 대해서 우리 사회에 대해서 이해를 돕고 사회가 성숙할 수 있도록 하는 발판 역할을 한다고 생각을 합니다. 가장 중요한 건 우리가 사용하는 개발자, 개발, 기술들이 우리 사회에 기여할 수 있는 부분이라고 생각을 합니다.
그래서 데이터 분석은 사회를 바라보는 또 하나의 눈이에요. 우리가 눈으로만 보는 게 아니라 훨씬 다양한 측면에서 접근할 수 있다는 뜻이죠.
최근에 제가 이 두 가지를 인상 깊게 봤어요. 100개는 사람이 볼 수 있다. 하지만 100만개는 사람이 다 볼 수 없다. 사람이 할 수 있는 일이면 기계도 할 수 있다. 물론 데이터가 충분히 있어야 하지만.
이런 게 나오기 시작하면서 데이터만 충분히 잘 되어 있다면 어떤 일을 할 때 사람보다, 사람만큼 잘할 수 있다는 이야기인 거죠. 그래서 우리는 두 가지 접근 방법을 생각해볼 수 있습니다.
만약에 사람이 볼 수 있는 일이다 하면 사람의 일을 줄여주는 방식으로 혹은 사람이 볼 수 없는 대규모 스케일이라면 사람이 볼 수 없던 것을 발견하게 해주는 두 가지 접근 방법인 거죠. 제가 선택한 데이터는 뉴스 그리고 댓글 이렇게 두 가지 데이터예요.
생각보다 뉴스 그리고 댓글에서 다양한 정보를 얻을 수 있는데요. 기사를 예시로 보면 헤드라인, 언론사, 기사 제목, 작성 시각을 비롯해서 기사 아래 독자들의 추천수 이런 것도 볼 수 있고요.
댓글에 좀 더 다양하게 데이터가 있는데 댓글 순위, 작성자, 작성 시간, 내용, 공감수, 비공감수 등 굉장히 많은 데이터를 가진 걸 볼 수 있어요. 그러면 이런 데이터들을 어떻게 모으느냐?가 하나의 이슈예요.
사실 우리가 말하는 웹크롤링이라는 기술을 사용하면 잘 가져올 수 있어요. 그렇게 문제가 되진 않는데 우리가 대규모 스케일로 자동으로 돌리려고 하면 하루 종일 켜놓을 수 없기 때문에 그 대신에 서버에서 동작을 하게 하고 저는 서버에서 돌리는 방법 중에 하나로 클라우드를 사용했어요. 특히 서버리스라는 방식을 사용했는데요. 이것을 사용해서 데이터를 저장을 했습니다.
그래서 2018년, 작년 발표에서 진행했던 방법은 이러하고요. 기사 목록을 쭉 넣으면 자동으로 서버에서 저장을 해주는 방식으로 진행했어요. 근데 올해는 자동으로 돌리기 위해 많은 부분 추가를 했는데요. 올해 진행하고자 했던 프로젝트는 매일 모든 댓글을 보기 위해 저장 이슈, 크롤링 이슈, 크게 두 가지 종류로 진행했어요. 첫 번째는 댓글 많은 랭킹 뉴스에 해당하는, 가장 마지막에 있는 데이터를 모두 모으는 거고요. 또 하나는 주식 가격, 매 10분마다 네이버 랭킹에 있는 것 모두 보는 방식으로 했어요.
매일 자동으로 해야 하기 때문에 클라우드워치와 저것을 사용하는 방식으로 다양하게 수집하도록 진행했습니다. 이 데이터는 제이슨이기 때문에 용량을 줄이고 분석을 쉽게 하기 위해 간단하게 변환하는 걸 진행했고요. 서버리스 진행을 했습니다. 결과적으로 데이터는, 하루에 한 번씩 모으는 경우에는 약 50기가바이트가 압축되어서 10기가 정도로, 그리고 지금 현재 원본 데이터 분량을 추적만 할 수 있고 용량 한계 때문에 일주일마다 자동으로 지우는 걸 돌려놔서 압축된 건 281GB 정도 모여 있습니다. 원본은 약 2테라바이트로 추정하고 있습니다. 이런 식으로 데이터 모으고 나면 분석을 해야 되는데요. 분석하기 위해 세 가지 방법을 사용했어요. 기본적으로 통계적인 접근 방법이에요. 데이터가 도대체 어떻게 생겼는지, 어떻게 존재하는지, 어떤 모양을 띠고 있는지를 알아봐야 해요.
머신러닝 같은 경우에는 통계적인 접근 방법이 기본에 데이터가 어떻게 생겼는지 설명하기 위한 거라면 머신러닝은 이렇게 짐작했으니까 앞으로 어떻게 예측하는 걸 하는 거예요. 물론 머신러닝에 넣는 데이터는 굉장히 다양한 걸 넣을 수 있지만 어쨌든 머선러닝과 통계적 모두 어떤 데이터를 어떤 식으로 우리가 가설을 세울지 이런 부분이 굉장히 중요해요. 이때 모든 데이터를 넣어서 볼 건지 아니면 일부만 선택적으로 볼 건지 그런 식으로 선택하는 것도 필요합니다. 마지막으로 딥러닝 같은 경우에는 데이터를 넣어서 유의미한 것을 자동으로 학습하도록 하는 것이죠. 특히 소셜 데이터에서 핵심적인 내용 중 하나가 댓글 본문같이 텍스트. 텍스트 같은 정형화돼 있지 않은 것 같은 경우 통계적 방식으로 접근이 굉장히 어렵고요. 딥러닝을 통해 텍스트 자체가 하나로 동작할 수 있도록 만들어줄 수 있습니다.
세 장의 슬라이드에서 통계적, 머신, 딥러닝 했는데요. 피처. 우리가 가진 데이터에서는 어떤 피처를 사용할 수 있느냐? 피처를 선정하려면 우리 데이터가 어떻게 생겼는지를 알아야 합니다. 우리가 가진 데이터 종류는 크게 4종류입니다. 첫 번째는 숫자형 데이터. 숫자로 표현 가능한 데이터인데요. 좋아요, 싫어요수, 대댓글 수, 댓글 순위, 댓글의 랭킹, 공감의 수 같은 게 숫자로 표현 가능하죠. 그리고 시간이 들어간 타임 같은 경우 순서. 댓글 언제 썼는지. 기사가 쓴 시간 대비해서 얼마나 빨리 썼는지. 짧은 시간 내에 얼마나 많이 쓰고 있는지 이런 시간 데이터죠.
그리고 세 번째는 텍스트 그 자체로 데이터화되는 케이스예요. 하지만 이런 건 난도가 꽤 상승하게 돼요. 데이터 종류 중 다루기 힘들지만 중요한 정보를 많이 갖고 있어서 놓치면 굉장히 곤란합니다.
마지막으로 유저 데이터는 정확하게 저 사람이 누구인지, 시간순으로 추적하는 방식으로. 사실은 문제가 최근에 하나 생겼어요. 2019년 7월 중순부터 네이버에서 쓴 사람이 누구인지 찾아내는 게 완전히 사라졌어요. 그 이전 데이터는 갖고 있는데 한 달 전부터 유저 데이터를 볼 수 없게 바뀌었습니다. 아무튼 그 이전 데이터를 살펴보게 되면 간단하게 데이터 총량부터 설명드릴게요.
네이버 뉴스 들어가서 랭킹, 댓글 많은 섹션 내에 댓글 많은 랭킹 순서 있고 섹션별로 30개 기사 매일 생성이 돼요. 총 7가지 섹션, 8개지만 포트 섹션 제외해서 7개 섹션 내에 있는 30개 기사. 맥스 210개 가져오게 됩니다.
일간 데이터는 댓글 약 5천만 개 이상, 하루 약 20만 건 데이터가 있고. 네이버에 있는 모든 뉴스 데이터 가져온 게 아니라 댓글 많은 랭킹에 들어가서 가져왔기 때문에 전체를 대표하진 않을 수 있다는 점이 있습니다.
그래서 언론사별로 얼마나 많은지 간단히 봤어요. 파란색 제일 위 그래프가 연합뉴스. 연합뉴스가 뉴스도 가장 많이 내고 댓글도 굉장히 많은 걸 볼 수 있고요. 그 뒤를 중앙일보, 조선일보 등등이 잇고 있는 걸 볼 수 있습니다.
앞에서 데이터를 여러 가지 소개해드렸는데 데이터 중에서 어떤 게 의미가 있는지는 다시 한번 생각을 해봐야 해요. 예를 들어서 한 가지가 아니라 여러 가지 종류의 데이터를 같이 볼 때 생기는 피처들이 있어요. 베스트 댓글 같은 경우 랭킹을 세우는데 랭킹이 기사별로 댓글을 바꿔서 거기서 좋아요 많은 순서대로 정렬해주는 방법.
유저가 하루에 한 사람이 단 댓글 모두 모아서 시간 순서로 정렬하고 댓글 사이의 시간을 계산해줘요. 이런 식으로 데이터는 존재하지만 한 번 가공한 새로운 데이터가 새로운 피처로 동작할 수 있게 됩니다. 물론 이런 데이터는 수치적인 데이터는 굉장히 다루기 쉬운데 텍스트 데이터는 까다로워요. 모델이 학습할 수 있도록 컴퓨터가 이해하는 숫자 형식으로 바꾸는 작업이 필요하기 때문이죠. 그 외에도 텍스트에는 다양한 특성이 있어요. 댓글 단어를 살펴보는 것뿐만 아니라 댓글 안에 링크가 있는가, 혹은 한 사람 기준으로 했을 때 얼마나 다양한 단어를 사용하고 있는가. 혹은 유저가 얼마나 긴 댓글을 쓰는가. 텍스트의 내용뿐만 아니라 그 자체로써 더 다양한 피처들을 뽑아줄 수 있어요. 이렇게 데이터에 대해서 여러 가지 생각을 하고 나면 가설을 세워요. 어떤 패턴의 유저를 볼 수 있을지.
사실 이런 프로젝트가 굉장히 중요한 게 그라운드 트루스. 실제로 어떤 유저들인지 정보가 필요해요. 그런데 제가 수집한 정보에는 그게 없어요. 네이버가 이 유저는 이런 유저다라고 알려주지도 않고 이 유저가 현실과 연결될 방법도 전혀 없기 때문에 알 수가 없어요. 그래서 어떤 좋은 데이터 분석 모델을 만든다고 하더라도 사실상 힘듭니다. 따라서 여러 가지 방법을 사용을 해서 아웃라이어, 그러니까 일반적인 유저들이 이런 식으로 있다면 여기 혼자 동떨어져 있는 유저를 찾는 걸 진행하는 거죠. 혹은 어떻게든 직접 라벨을 붙여서 어떤 사람이다, 어떤 댓글이다라고 적어주는 작업이 필요해요. 따라서 무엇이 정상이고 무엇이 비정상이다라는 댓글을 찾는 거라기보다는 그것보다 조금 더 쉬운 이슈로 좀 더 극단적인 댓글들을 찾자라는 방향으로 선회하게 됐어요. 찾게 된 것이 수량, 내용 그리고 전체적인 그런 게 들어가 있나, 그 외 여러 가지 아웃라이어들을 찾는 거였어요.
아웃라이어를 찾는 건 일종의 아까 말씀드렸던 것처럼 그래프가 이렇게 있을 때 갑자기 혼자 툭 튀는 이런 친구들을 찾는 거라고 볼 수 있어요. 어떤 피처를 X, Y축 이렇게 축으로 잡고 사람들을 세웠을 때 어떤 쪽은 툭 튄다면 이 유저는 아웃라이어다라고 생각을 해줄 수 있는 거죠. 그리고 이러한 정치적으로 얼마나 편향되어 있는지 그런 거에서도 연구를 진행하고 있어요.
이제부터는 지금까지 어떻게 피처를 잡고 피처를 어떤 식으로 세울 수 있는지 설명해드렸는데요. 이런 피처로 축을 세우고 아웃라이어들이 어떻게 있는지 살펴보도록 할게요. 제가 세웠던 아이디어는 여러 가지가 있는데요. 첫 번째 아이디어는 댓글의 URL을 많이 가져다 쓰는 경우 혹은 평균 길이가 긴 경우. 댓글 300자 꽉꽉 채워 쓰는 경우가 있겠죠. 유저별로 다양한 워딩을 사용하지 않는 경우. 댓글을 몰아 쓰거나 일상적인 쓰거나. 혹은 얼마나 댓글을 빨리 다는지 분석해보기로 했어요. 마지막으로 댓글을 빨리 다는 경우에는 빨리 단다는 정의가 모호해서 두 가지로. 하나는 뉴스 올라오자마자 다는 경우 혹은 사람이 댓글을 달고 그 직후에 바로 댓글 다는 경우. 이렇게 두 가지로 생각해봤어요. 파이썬과 약간의 노가다를 통해 여러 특징을 알아봤고 결과를 알아보려고 해요.
우선 URL을 붙이고 다니는 사람들의 경우예요. 댓글에서는 잘 생각해보시면 http 갖다 붙여도 클릭도 안 되고 터치해서 아무런 일 일어나지 않요. 직접 복사해서 인터넷 창에 붙여야 되는. 그런 방식인데 그럼에도 불구하고 URL을 붙이는 사람이 있다는 거죠. 자, 그럼 URL을 얼마나 많이 달고 있냐? 보니까 처음에는 URL 다는 비율이 평균적으로 0에서 2%까지 찾아봤을 때 0%가 너무 많아요. 하늘에 뜨고 있어서 나타나지 않아서 1% 이상으로 나타낸 그래프가 오른쪽에 있는 그래프고요. 여기서만 보아도 가장 오른쪽, 100%에 가까운 게 누가 봐도 아웃라이어다라고 나타나는 모습이 보입니다. 실제로 URL 댓글은 전체 댓글 중에서 약 4만 개 정도가 달려 있고요. 전체 중 0.18%에 해당합니다. URL 비율이 1% 이상의 유저 같은 경우 1만 명 정도, 한 번이라도 URL 가져다 쓴 경우도 1만 명 정도인 것 같습니다.
한편 댓글 중에서 URL 댓글 비율이 URL, 댓글 단 수가 분모가 되고 URL 댓글이 분자가 되는 건데, 유저별 댓글을 찍어보면 많은 사람들이 많이 가는, 전형적인 패턴이 보여요. 내용을 살펴보면 QR코드 찍어보면 어떤 댓글이 있는지 보실 수 있는데요. 청와대 청원 URL 가져와서 청원해달라는 요청이 굉장히 많은 편이고요. 특정 기사 혹은 매체로의, 여기로 들어가봐라 하는 것들이 굉장히 많아요. 혹은 너네들이 말하는 건 이런데 팩트는 이렇다, 팩트 체크성 URL 가져오는 경우도 있고요.
이제 댓글을 길게 다는 사람들은 어떤 사람이 있는지 볼게요. 최소글자 채우는 것도 귀찮은데 꽉꽉 채워서 쓴다? 굉장히 어려운 것 같아요. 네이버 최대 길이는 300자 제한이 있어요. 그래서 그 이내에서 유저들이 유저별, 유저별로 얼마나 평균 글자 길이가 긴지 살펴봤어요. 유저별 평균 길이를 살펴보면요. 50 글자 이내로 평균적으로 작성하는 경우가 60% 넘었고, 100 이하로 할 경우에는 87%가 넘었습니다.
여기 그래프도 잘 보시면 오른쪽 끝에 쭉 올라가 있는 케이스를 보실 수 있어요. 아래 부분 확대해서 살펴보면 290에서 300이 확 튀는 걸 볼 수 있어요. 이 케이스도 QR코드 들어가서 보실 수 있고요. 이런 유저들이 어떤 댓글을 쓰고 있나 살펴보면 정치적 내용이 굉장히 긴 댓글을 쓰거나 혹은 비슷한 내용을 굉장히 자주 다는 그런 경우가 보여요.
근데 이런 댓글을 다는 유저들은 얼마나 많은 댓글을 쓰고 있나. 평균 길이가 290 이상인 유저 찾아보니까 만 4천 개에 해당하는 댓글을 달고 있었습니다. 그래프로 봤을 때 오른쪽이 아웃라이어에 해당하는 거죠.
또 다른 방법으로 살펴본 것은 비슷한 말을 하는 거였어요. 사용자들이 일반적으로 댓글을 달 때 매번 새롭게 다나, 똑같은 말을 하고 있다면 뭔가 의도가 있지 않을까 생각한 거죠. 그리고 우리가 만약 직접 댓글을 단다면 항상 동일한 말을 하고 있진 않기 때문에 좀 더 다양한 워딩을 사용하고 있을 거다라는 가설을 세웠습니다.
첫 번째 방법은 유저별로 사용한 단어 목록을 모아본 거예요. 2019년 6월 첫 주 데이터 기준으로 볼 때 단어를 잘라서 사용했고, 이 과정을 통해서 결과적으로 나오는 건 한 사람이 댓글을 쭉 쓰고 문자별로 모은 다음에 이 사람이 쓴 단어를 뽑아내는 거예요. 단어가 결과적으로 나오게 하는 거죠. 생각보다 좀 오래 걸리고 한 달, 1년 치 하면 시간이 어마어마하게 오래 걸려요. 실제 사용자들 케이스 보면 이게 유저 아이디고 오른쪽이 사용한 단어들. 어떠한 단어 위주로 사용했는지 볼 수 있는 단어 패턴을 통해서 이 사용자가 어떤 특성의 사용자다라는 걸 추측해볼 수 있어요.
그건 제가 해보려는 게 아니고요. 이런 단어를 사용하는데 실제로 그 사람이 쓴 댓글의 개수가 몇 개였냐. 이걸 같이 보자는 거죠. 댓글은 많이 보는데 개수는 적어요. 그러면 아웃라이어가 아닌가? 가설을 세우는 거죠. 그래서 토픽 개수 나누기 댓글 수 비율이 적은 아웃라이어를 찾고자 했어요. 실제로 결과를 살펴보면 다 0에서 5 사이에 몰려 있어요. 일반적인 유저들도 0, 5 사이에 몰려 있고. 토픽이 적게 나오는 경우가 많아서 이 방법은 적절치 않다. 새로운 방법이 필요하단 생각을 했어요.
비슷한 문장을 쓰는 유저를 다른 방식으로 찾아보고자 했는데요. 유저별로 1 단위 혹은 1단위에서 같은 혹은 비슷한 문장을 쓴 경우를 살펴보는 걸 선택했어요. 즉 유저가 쓴 댓글 사이에 그 댓글 텍스트 사이의 비율. 비슷한 문장을 찾는 것도 세 가지, 여러 가지 방법이 있어요. 다양한 방법이 있는데 첫 번째 완전히 두 댓글이 같은지 비교하는 거예요. 굉장히 쉬워요. 같은지 비교만 하면 되니까. 빠르고 편하고 좋은데 한 글자라도 달라지면 찾을 수 없는 치명적인 단점이 있어서 제외했습니다.
두 번째 방법은 이 글자를 얼마나 바꾸면 되나라는, 얼마나 글자면 바꾸면 두 댓글이 같아지나 확인해보는 방식인데요. 이것도 계산량이 굉장히 많아서 시간이 오래 걸려요. 이 방식도 제외했습니다. 제가 실제로 사용한 방법은 얼마나 비슷한 단어를 쓰느냐. 사람들이 사용하는 댓글을 단어 단위로 잘라주고 단어 사이에 유사한 것들을 비교해주고 또 다른 방식들로 댓글 문장이 나타내는 벡터가 얼마나 유사한지 비교해보는 방식을 사용했습니다.
비슷한 말을 쓰는 유저들의 그래프를 보면 다음과 같은데요. 이건 단어를 잘라서 유사어를 비교한 결과입니다. 실제로 내용을 살펴보면 위아래가 완전히 동일한 내용을 가져다가 사용하는 유저들도 있고요. 혹은 일부 단어, 다음과 같이 바꿔서 사용하는 케이스도 있습니다. 보시면 일부 단어만 자동적으로 바꿔서 사용하는 케이스를 볼 수 있죠. 한편 블루 스코어 계산하는 방법도 있는데요. 두 개 문장 비교해봤을 때 괄호 친 부분만 다르고 나머지는 모두 동일한 결과를 볼 수 있습니다.
그리고 앞서 언급한 방식을 통해서 두 문장의 코사인 시그널리티를 계산을 하면 단어가 다르더라도 얼마나 비슷한지 확인해볼 수 있어요. 문장의 의미가 얼마나 비슷한지 찾아볼 수 있는데요.
완전히 문장의 글자가 같진 않지만 비슷한 문장들을 찾기도 하고요. 한편 앞선 분석들은 하루에 한 유저들이 쓴 댓글만 기준으로 살펴본 건데요. 한 유저가 아니라 전체 댓글 대상으로 살펴보면 어떻게 나타나는지 보겠습니다. 같지 않은 유저가 같은 혹은 비슷한 댓글을 쓴 케이스인데요. 실제로 확인해보면 하루에도 몇 백명의 유저가 있는 걸 볼 수 있어요. 케이스를 하나 살펴보면 위아래, 유저 네임이 분명 다른데도 별표 글자 제외한 나머지 댓글이 내용이 완전 동일한 것을 볼 수 있어요. 작년 케이스에서 비교해봤는데 이번에는 하루 단위로 유저 모두 비교를 해서 결과적으로 어떤 유저들이 서로서로 비슷한 내용을 쓰고 있는지 살펴볼 수 있게 됐어요.
그러면 유저들 간에 비슷한 내용을 쓰는 게 나왔죠. 그러면 뭘 해야 될까요? 당연히 네트워크를 그려봐야겠죠. 비슷한 말을 하는 유저들을 모아봤으니까 해당 유저들이 얼마나. 연결된 걸 한 번 연결되면 카운트 1이라고 하면 10번 이상 연결되면 네트워크에 나타나도록 그려봤는데요. 실제로 그러면 중간에 가장 모여 있는 경우가 있는데요. 이 유저들, 어떤 유저들인 것 같으세요? 예측하기 쉽지 않은데 사실은 저 중앙에 몰려 있는 분들은 강다니엘 응원. 잘되기를 원하니까 비슷한 말을 하다 보니까 응원합니다 이런 댓글들. 자세한 내용은 이거 외에도 많은데 QR코드 들어가보면 직접 확인해보실 수 있습니다.
그래서 비슷한 말을 하는 유저, 살펴봤는데 그것 외에도 다른 아웃라이어가 있었죠. 이번에는 손이 눈보다 빠른 아웃라이어를 찾아봤습니다. 제가 정리한 건 두 가지 케이스. 뉴스 올라오자마자 댓글 다는 경우. 혹은 유저가 댓글 단 경우 그 직후 빠르게 다는 경우. 두 가지 봤을 때 첫 번째 케이스는 뉴스 올라오자마자 댓글 다는 거예요. 아침에 보통 새벽 6시 30분에서 50분 사이에 많이 올라오는데 이때 사용자들이 댓글을 바로 달아? 제 입장에서는 쉽지 않았어요. 저는 늦게 일어나는 편이기 때문에 쉽지 않은데, 아무튼 댓글이 올라오기까지 한 시간 이내 유저들을 숫자를 세봤어요. 그랬더니 실제로 그렇게 엄청나게 튀는 그래프는 나타나지 않더라고요.
그래서 단순하게 빨리 댓글을 단다라는 건 큰 정보가 되지 않았습니다. 그래서 유저가 N초 이내에 댓글 단 케이스를 보기로 했죠. 사실 제가 실험을 해봤는데 댓글을 달고 바로 댓글을 달려고 하니까 댓글과 댓글은 60초 이내에 달 수 있다고 경고창이 나오더라고요. 타임 계산해서 첫 번째, 두 번째 댓글, 사이의 간격을 계산해봤어요. 10분 이내에 얼마나. 찍어보니까 실제로 60초 끝나자마자 댓글 다는 케이스가 은근히 많은 걸 볼 수 있었어요.
실제로 이 유저 같은 경우에는 댓글 사이 간격이 딱 60초로 나오는 케이스 있었고요. 이번에는 다른 접근 방법인데요. 대댓의 순위가 어떻게 바뀌나. 베댓이 어떻게 만들어지는지, 한 번 베댓이면 영원히 베댓인지. 첫 번째 질문은 제가 답할 수 없는 거고요. 두 번째, 세 번째 질문에 대해 확인해보기로 했습니다.
오전 7시부터 밤 12시까지 즉 매 10분 간격으로 모은 데이터고요. 랭킹 뉴스에 기사가 들어오는 순간부터 시작해서 10분 간격으로 모두 데이터를 가져온 케이스예요. 그래서 이런 데이터를 가져오면 좋아요 수가 얼마나 어떻게 변했는지 어떤 댓글이 생겼다 사라졌는지 모두 확인해볼 수 있어요. 실제로 하루 치 데이터를 그래프로 그려봤는데요. 7시 20분까지 시작해서 00시까지. 한 시간마다 잘려서 내려가는 경우 있는데 네이버 랭킹 뉴스 업데이트 되면서 전날 것이 사라지고 하는 방식으로 기사가 업데이트 돼요. 그래서 댓글 수 줄어들었다가 오르고 이런 모습이 반복됩니다. 베스트 댓글이 변하는 모습을 봤어요. 기본적으로 베댓은 순위가 안 바뀐다. 거의 안 바뀌어요.
초반부에는 이런 식으로 약간 순위가 변하는 경우도 있어요. 순위가 바뀌는 건 어떤 댓글인가 살펴봤더니 이런 식으로 순위가 매우 치열하게 바뀌는 케이스에서 하늘색 그래프가 하나가 올라갔다가 밑으로 내려가고 베스트 댓글에서 사라집니다. 저 댓글은 뭐하는 댓글인가 봤더니 저런 식으로 어떤 팬 관련된 내용이었는데. 이런 케이스도 있었어요. 이 뉴스는 일본 관련한 내용인데요. 일본 이슈에서 처음에 10개 베스트 댓글이 전부 비판적인 거였다가 7시에 올라왔다가 10시 지나면 1등부터 5등까지는 그대로인데 6등부터 10등까지는 옹호하는 걸로 바뀐 것을 볼 수 있었어요. 이런 과정은 제가 저 댓글들을 직접 보고 저건 비판적인 것 같아라고 자의적으로 해석을 했기 때문에 생기는 판단이고요. 이런 케이스들을 직접 좀 더 자동적으로 보려면 다른 게 필요해요. 자동화된 폴리티컬 베이스를 찾는 거죠. 이때 딥러닝을 사용했습니다.
정치적 편향성을 찾아보기 위해 여러 가지 방법을 사용했고 가설을 세워봤어요. 키워드 방법으로, 문맥상 특정 정당을 지지하는 것, 이건 그 내용을 보고 나서 보수인지 진보인지 특정 정당을 지지하는지 아닌지 살펴봐야 돼요.
한편 우리가 모두가 알고 있듯이 라벨링된 데이터가 필요해요. 이 댓글을 보고 이 댓글은 누구 편이다라는 게 필요한 거죠. 한편 라벨링을 하는 과정에 있어서 하루에 20만 개 댓글 올라오는데 직접 다 한다? 당연히 불가능해요. 좀 더 적은 데이터를 가지고 높은 효율을 끌어오기 위해 가설을 세워봤습니다. 유저들의 공감을 많이 받는, 이 기사에 대해 어떤 댓글이 굉장히 좋은 것을 가지고 있다. 베스트 댓글을 잡아서 약 5만 개 데이터를 봤어요. 여전히 5만 개는 직접 라벨링하기에는 너무 많은 데이터죠. 한번 라벨링 진행해봤는데 하루에 2천 개 정도 할 수 있고 그날 머리가 정말 많이 아프더라고요. 5만 개를 하려면 한 달... 이라고 잡았지만 몇 달 걸릴 것 같습니다.
그래서 좀 더 빠르고 편한 라벨링을 위해서 조금 부정확하더라도 빠르고 비슷한 걸 추천해주는 시스템을 개발해보자 생각했어요. 그래서 처음으로 생각한 건 라벨링 없이 할 수 있는 방법 없나? 그래서 조금 부정확하더라도 대충 라벨링 하는 걸 생각을 해봤어요. 가설을 세운 건 사람은 잘 변하지 않는 것 같아. 한 사람의 정치적 견해가 극단적으로 바뀌진 않는 것 같다. 그 사람은 약간 보수 혹은 진보, 특정 당 친화적인 게 아닐까. 가설을 세워봐요. 물론 여러 댓글을 달기 때문에 노이즈가 굉장히 많을 거라고 생각할 수 있어요. 베이스라인 모델로 직접 라벨링 하기 전에 한 가지 진행해봤어요. 폴리티컬 굉장히 심한 것들. 정치 섹션의 제목을 가져와서 긁은 데이터를 트레이닝 시켜봤어요. 데이터셋을 정치적으로 편향 심한 곳에서. 2만 7천 개 정도 가져왔고요. 그래서 어쨌든 최신 모델을 써보자 해서 써봤어요. 성능이 너무 잘 나오는 거예요.
세상에. 안좋은 일이 일어나고 있다. 실제 적용해봤어요. 정말 안좋게 나옵니다. 사실 제목 스타일이랑 댓글 스타일이랑 굉장히 달라요. 기사 생각만 해봐도 제목은 팩트만 강렬하게 들어가 있는데 댓글은 오류가 굉장히 많은, 오타들도 있고 이런 특성이 있기 때문에 그런 거에서 많은 차이가 나타나고 있더라고요.
다음 방식으로 선택할 수밖에 없었어요. 라벨링 최대한 줄여서 해보자. 아까 전에 세웠던 가설. 온라인에서 사람 성향 그렇게 잘 바뀌지 않을 거다 가설을 세운 거죠. 100명 이상 라벨링 하고 이 유저들 데이터를 끌어와보자. 아웃라이어에 몇 번 이상 돼 있다 확인해보자. 모델에는 잡혀 있지 않았지만 기존 탐지된 유저와 비슷한 패턴 보이는 케이스 있을까? 이런 방식을 통해 실제로 봤던 데이터에 비해 다양한 데이터를 해줄 수 있어요.
실제로 라벨링 한 걸 그룹바이 해서 목록을 세워본 건데요. 빨간색으로 돼 있는 첫 번째 유저는 제가 라벨링한 2500개 데이터 중에서 무려 26개 달려 있었어요. 이 유저 같은 경우에는 1에 가까우니까 좀 더 보수 쪽에 가깝지 않을까. 이것도 사실 추정입니다. 완전 추정인데. 그리고 1에 가까우면 보수, -1에 가까우면 진보에 가까울 것 같다고 가설을 세우는 거죠. 순전히 가설입니다. 실제로 저 파란색으로 된 유저는 -1만 6개 나와 있는 케이스를 볼 수 있고요.
실제로 제가 2500개를 랜덤 해서 라벨링을 해봤는데 보수:진보가 약 7:3. 댓글 단 비율은 약 9:1에 해당했습니다. 진보의 댓글이 굉장히 적은 편이더라고요. 결과적으로 트레이닝을 하려면 어떤 샘플링이 필요한데 2만 9천 개를 다 쓸 수는 없고 3천 개로 내려서 진행해야 돼요. 그래도 시작을 할 수 있는 데이터라는 점에서 의미는 있었어요. 그래서 진보 유저가 댓글이 적으니까 그러면 진보 성향 좀 더 찾아보자 생각을 해봤죠. 진보 뉴스랑 보수 뉴스에 댓글 달리는 성향이 다를까? 이건 진행 중인 건데요. 보수에 달리는 것이 보수. 가설인데요. 이런 가설을 세워보는 거죠. 아직 연구 중인데. 결국은 보수, 진보. 혹은 더 다양한 것들 그런 것들이 필요한 거죠. 실제로 진보 뉴스 댓글들 추가적으로 라벨링 하고 있고요. 이를 통해 알고자 하는 건 유저의 성향을 잘 쓸 수 있는 방식이 없을까? 유저를 태깅하는 웹서비스를 만들기로 했습니다. 제가 아까 전에 보여드렸던 아웃라이어들이 찾는 그것. 그것들이 매번 데이터에서 제가 직접 돌리면서 확인하려면 굉장히 오래 걸려요. 그리고 데이터 베이스 자르는 것도 일이겠죠. 그래서 웹에서 사용자들이 가져다가 달 수 있는 시스템을 만들자 생각했어요. 장고 사용해서 기초 분석 모델을 저장하고, 저기 나와 있는 것처럼 태그. 이 사용자는 어떤 사용자다라고 사용자들이 참여할 수 있는 방식으로 했고. 비슷한 유저들을 태깅하겠어요? 라는 추천 시스템 개발을 생각하고 있습니다. 결과적으로 이런 건 시민들의 참여를 필요로 하는 거예요. 사람들이 참여할수록 정확도가 높아지고 일종의 추천 시스템과 같은 개념이죠. 태그된 유저별로 진행을 해줘서 새로운 데이터가 일정 이상이 되면 새로 모델을 학습하는 방식을 생각을 하고 있습니다.
이제 마지막으로 하는 말인데요. 우리가 보는 사회는 결과적으로 우리가 볼 수 있는 범위 내. 사람이 볼 수 있는 건 제약이 있어요. 따라서 사람이 하나하나 보는 것 대신에 자동화된 작업으로 해결하는 게 필요합니다. 우리가 좀 더 신경을 쓸 수 있도록 하는 환경 역시 데이터 분석을 하는 것 그 이상으로도 필요하고요. 개발자가 아닌 우리 같은 사람들이 아니라 일반 시민들이 참여할 수 있도록 하는 것을 만드는 것도 필요합니다.
제 발표는 여기까지고요. 들어주셔서 감사합니다.
(박수)
아직 끝나지 않았어요. 최근에 슬픈 이야기가 하나 있었어요. 크레딧으로 같이 진행해주시는 교수님들 감사의 말씀을 드리고요.
사실은 한 가지 개인적인 사소한 이슈인데요. 제가 저번주 카이스트를 넣었는데 서류에서 떨어졌어요... 그래서 행복하게 연구하는 랩을 추천을 받습니다. 제 발표는 여기까지고요. 들어주셔서 정말 감사합니다.
(박수) 
-발표를 해준 이준범 님 감사합니다.
지금 세션 시간이 끝나서 질의응답 시간은 따로 받지 않고요. 질문 있으신 분들은 앞쪽에서 직접 질문 받겠습니다. 감사합니다.