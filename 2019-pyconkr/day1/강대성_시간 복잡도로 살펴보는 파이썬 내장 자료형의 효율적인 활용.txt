[발표자: 강대성]
-(사회자) 이번 시간에는 시간 복잡도로 살펴보는 Python 내장자료형의 효율적인 활용 주제로 강대성 님 발표해주시겠습니다.
-(강대성) 안녕하세요.
제가 발표할 주제는 시간 복잡도로 살펴보는 Python 내장자료형의 효율적인 활용이라는 주제로 발표를 하겠습니다.
제 소개를 하겠습니다.
저는 16년차 개발자고요.
좀 오래됐죠? 한번쯤 웃어주셔야 하는데... 16년째 개발하다 보니까 주변업무들을 많이 하는 거 같아요. 인프라, 네트워크, 보안 쪽 많이 하고 있고요.
지금은 피플펀드 컴퍼니 기술고문으로 일하고 있습니다.
그리고 2015년, 2017년에 스피커를 했었고요.
2년에 한 번씩 하고 있네요. 2019년도 하고 있네요.
안 나와도 그냥 하겠습니다.
일단 오늘 이야기할 주제는 시간 복잡도를 먼저 이해를 할
거예요. 그리고 하는 건 내장자료형의 시간 복잡도. 자료를 하나씩 넣을 때 어떻게 바뀌는지 할 거예요. 느린 부분을 극복하는 방법 등 하게 될 겁니다.
제 목소리 잘 들리죠? 일단 시간 복잡도가 뭔지 이해를 할게요.
일단 이야기를 좀 나눠볼게요.
일단 시간 복잡도는 시간을 정량해서 표기하는 겁니다.
낮은 차수를 제한하는 거로 합니다. 어떤 거를 계산할 때, 여러분 이거 안 찍어도 돼요.
ON세제곱 그리고 3n으로 표현할 때, n 세제곱 하면 이 안에 뭐가 숨어있을지 모르겠으나 n의 세제곱 만큼 시간 복잡도을 가진다.
한 건일 때 O, 이게 100건일 때는 얼마큼 걸릴지 대략적으로 예측을 해보는 겁니다.
O의 N제곱은 100의 제곱만큼 늘어나겠죠. 이렇게 시간복잡도를 계산하는 겁니다.
대략적으로 나타내면 이렇고요. 하나하나 말씀을 드릴게요. O(1)은 맨 아래 깔려 있는 거 있죠. 어떤 데이터가 들어와도 상수처럼 데이터를 처리합니다. 이런 알고리즘 거의 없습니다.
O(N)은 데이터가 늘어나면 그에 맞춰서 처리 속도도 늘어나는 거예요.
N제곱은 데이터가 2배 늘어나면 처리 시간 2배 늘어납니다. 데이터가 늘어나면 시간이 확 올라가는 형태입니다.
N 로그 N. 이거는 데이터를 정렬할 때 최적의 성능을 가지면 이 정도 가집니다.
고개를 끄덕이시는 분들 보니까 많이 배우신 거 같아요.
로그N 같은 경우는 이 정도의 시간 복잡도가 나오고요.
이런 경우는 시간 복잡도가 어떻게 될까요?
누가 N제곱이라고 얘기를 해주셨네요. 왠지 저희 회사 사람 같은데... 이거 같은 경우는 자세히 보시면 어느 정도 시간 복잡도를 가지냐고 예측해봤을 때 좀 헷갈릴 수 있는데 input을 한 번만 돌리잖아요. input이 늘어남에 따라 시간은 input 만큼 늘어나요. 결국 이거는 O(N)만큼 시간 복잡도을 가지는 거죠.
그러니까 시간 복잡도는 뭐냐? 결국 데이터가 늘어났을 때 시간 증가의 경향을 나타낸다는 보시면 됩니다. 오늘 주로 O(1)과 O(N)정도로 다루게 됩니다. 본격적으로 들어가기 전에 immutable과 mutable을 다루게 됩니다.
immutable한번 데이터를 만지면 변하지 않아요. mutable은 변할 수 있는 객체들, 그러면 아까 tuple은 immutable에 배웠는데 이렇게 A를 변화해서 넣었잖아요. 이거는 immutable죠. 실제로 Python 내부에서 이거를 어떻게 처리하느냐 생각해보면 immutable인지 알 수 있는데 Python 내부에서는 A와 B가 여기 들어가는 거죠. 실제 Python이 구현되는 소스입니다.
첫 번째 tuple 여기로 데이터를 받고, 두 번째 노란색 보면 혹시 a가 비었는지, 두 번째 세 번째 노란색 보면 b가 비었는지 거기까지만 체크를 합니다. 둘 다 차 있는 tuple이면 2개를 더한 후에 그거를 복사합니다.
그리고 새로 변환하죠. 결국 immutable의 객체니까 어쩔 수 없이 이렇게 하겠죠.
그래서 아까 설명드리는 거, 2개 중 하나가 비었다고 하면 a하고 결국 c하고 똑같은 객체인 거예요.
뒤에 800으로 똑같이 끝나죠? 좀 헷갈릴 수도 있는데... 여러분 표정이 안 좋은 거 보니까 좀 헷갈릴 수 있는 거 같은데 여기세 번째 노란색 박스 있죠? 그 부분에 해당되는 경우입니다.
tuple 같은 경우에는 더하기를 하면 새로운 tuple 만들고 다시 복사해서 유턴해줍니다. 앞의 tuple과 뒤의 tuple의 더한 것만큼 시간 복잡도를 가지겠죠.
이제 리스트로 넘어왔어요. 이거는 immutable처럼 쓴 거처럼 보이죠.
실제로 Python 내부에서 이거를 어떻게 처리하느냐 보면 이거 같은 경우는 tuple 하고 다르게 리스트가 안에 데이터가 없는지 비교하지 않아요.
그냥 2개를 사이즈를 더해서 새로운 걸 만들어서, 할당해서 다시 내뱉는 역할만 해요.
결국에는 아까처럼 이렇게 쓰면 tuple과 다르게 쓴 건 없죠?
내부적으로 실제로는 mutable한 객체를 immutable하게 쓴 예제 중 하나입니다.
이거는 tuple하고 비교하기 위해서 썼는데요. 실제로 아까 이거를 지금 덧셈한 거를 실제로 익스텐드 한 것처럼 해야 맞겠죠.
익스텐드는 실제로 Python 내부에서 이렇게 운영됩니다.
실제 2개의 객체, 앞에 1, 2를 가진 것과 3, 4를 가진 객체 길이를 보고 그거를 그만큼 앞의 거, 원래 l이라는 리스트를 리사이즈 하고 한 후에 뒤의 거를 복사해 넣습니다.
리스트는 익스텐드 오퍼레이션은 O(N M) 정도, 여기까지 서론적인 이야기고요. 진짜 본론으로 들어갈게요.
여러분 리스트 많이 쓰시죠? Python 스시면서 리스트 안 쓰시는 분들 없을 겁니다.
리스트 pop할 때 어떤 일이 일어나는지 볼게요.
pop 세 번째 거, Python은 제로베이스니까 3이라고 하면 네 번째를 이야기하겠죠.
세 번째 거를 뺀 후에 그다음 거를 앞으로 밀어버리겠죠. 이렇게 내부적으로 구현됩니다.
여기까지는 크게 아름답다, 아름답지 않다가 없을 거예요. 그런데 만약에 데이터가 더 많다고 가정을 할게요.
네 번째 거를 뺐어요. 그러면 뒤의 거를 다 밀어줘야 해요.
이렇게 밀면 시스템 내부적으로는 뒤에 있는 걸 다 민다고 생각하는 거예요.
에너자이저가 미는 것처럼 다 민다고 생각을 해야 돼요.
정말 시간이 빨리 처리가 되는가 보셔야 하는데 Python 내부에서는 일단 pop이라는 걸 호출할 거예요. pop이라는 함수를 호출하면 내부적으로는 리스트 pop이라는 함수를 호출하고, 그리고 마지막으로 메모리를 옮기라고 호출을 할 겁니다.
그런데 메모리 옮기라는 게 엄청난 무브를 가져옵니다. 여러분이 상상할 수 없을 만큼 엄청난 오퍼레이션을 가져오죠.
결국 이거는 O(N)이라는 시간 복잡도를 가지는 겁니다.
하나의 데이터를 뺐을 뿐인데, 결국 하나를 함으로써 엄청난 부화가 CPU나 메모리에 갈 수 있다는 겁니다. 컴퓨터가 약간 고생할 거를 생각하면서 프로그램을 만드시면 됩니다.
좀 복잡한 append를 볼게요.
append 4를 했어요. 이게 O(N) 같은가요? O(1) 같은가요?
4부터 리스트 a를 할당하고 1부터 인서트, 이게 O(N)에 끝날까 약간 고민이 있어야 해요.
append 하면 어떤 일이 발생하는지 보겠습니다.
처음에는 빈 리스트입니다. append(1) 새로운 메모리를 할당해서 넣고요. 두 번째도 메모리를 할당해서 넣어요.
3을 또 넣습니다. 복사한다는 의미는 O(N), 기존 데이터를 N이라고 생각했을 때 O(N)만큼 소요된다. 할 때마다 이 시간이 소요될까? 이게 괜찮을까? 제가 봤을 때는 이거 괜찮지 않은 부분입니다.
그래서 설마 Python을 이렇게 대충 만들었을리 없다. 자세히 살펴 보면 리스트 리사이즈 할 때 0, 4, 8, 16, 25, 35, 46 순으로 추가를 합니다. 하나만 추가해도 메모리 4개를 할당해요.
하나씩 계속 추가하면 엄청난 오퍼레이션이 일어나니까.
예를 보여드릴게요. 빈 리스트입니다. 하나를 추가해도 1 할당하고 나머지 3반 빈칸으로 둡니다. 또 하나 추가하면 메모리 복사할 필요 없이 그냥 2를 넣어요.
3, 4 넣습니다. 이제 5를 넣을 때 비로소 데이터를 늘려줍니다.
6 넣고 이런 식으로 하는 거죠. 결국 Python이 취하는 방식은 메모리를 조금만 더 낭비를 하고 시간에서 더 이득이 생기도록 만들었어요.
조금씩 append를 해도 큰 부담이 안 가게. 보통 할당을 할 때 전체 크기의 나누기 8 정도 한 만큼 더 여유 공간을 확보합니다. 항상 확보하는 건 아니고 모자란다 싶을 때 8분의 1만큼 할당을 해놔요.
그래서 이제 얼마나 느린가 테스트를 해봤어요. 100개의 리스트를 일단 준비를 하고요.
그 리스트에 데이터를 하나씩 넣는 걸 테스트 해봤습니다. 리사이즈가 발생하면 뒤에 트루라고 찍혀 있는 거예요.
리사이즈가 될 때나 안 될 때나 큰 차이가 없어보입니다. 많이 차이나봤자 10배 차이나죠.
표정이 어두우시네요.
그런데 사이즈가 커진다. 많이 커지면 이게 1000배 이상 차이도 납니다.
이게 맨 아래 쪽 0.09초, 100번 했으니까 그만큼 걸리는 거겠죠. 100번 한 거 기준입니다. 100번 할 때 리사이즈 하면 이만큼 시간이 걸립니다.
평균적으로 보면 O(1)에 가까워요. 그런데 가끔 O(N)이 발생합니다.
그리고 만약에 나는 리사이즈 하면 안 돼. 그러면 미리 할당을 하고 시작을 합니다. 100만 개를 한 번에 할당을 시킬 수도 있고요. Python에서 정한 최고 사이즈 크기 만큼 미리 할당하고 할 수도 있어요.
여기서는 안 했는데 딕셔너리 할 때는 미리 할당하면 어떤 차이가 있는지 보여드릴게요.
그리고 append 하는 대신에 이게 진짜 할당을 하면 되겠죠.
딕셔너리로 넘어갈게요.
다들 써보셨죠?
내부 구조는 지금 Python에서는 이런 구조를 가지지 않아요.
인터넷에 해시태그로 찾아보면 가장 먼저 쓰는 게 이 그림입니다.
해시펑션의 키를 넣으면 특정 버킷을 지정해줘요.
나중에 데이터를 찾을 때 키만 넣으면 내가 원하는 데이터를 빨리 끄집어낼 수 있어요.
거의 모든 오퍼레이션이 상수값으로 거의 끝나버려요. 문제는 버켓 사이즈가 문제입니다.
크게 하고 시작하지 못해요. 대부분 Python은 버켓 사이즈 8로 시작을 합니다.
버켓이 다 찰 때까지 기다리지 않고 3분의 2가 차면 리사이즈 해버려요.
그리고 이거는 나름 컬리전을 생각해서 최적의 수치라고 저는 믿고 있어요. 그래서 처음에 8로 했는데 3분의 2가 차면 현재 테의 3배 성장을 시킵니다. 무조건 현재 데이터에서 리사이즈 할 때 2배씩, 8이면 16 이런 식으로 성장할것죠.
실제 Python 소스는 이렇게 생겼습니다.
인서트 할 때 딕셔너리 리사이즈에, mp가 딕셔너리오브젝트입니다.
계속 2배만큼 성장을 시킵니다. 리사이즈 할 때도 2배 단위로 성장하도록 해놨어요.
리사이즈 하게 되면 실제 Python에서 벌어지는 건 새로운 메모리를 할당, 해시펑션 상태에서 버켓에 할당하는 과정, Python도 똑같이 이 과정을 합니다. 해시밸류는 미리 구해놨어요. 그거를 보고 버켓에 지정을 해주는 역할을 합니다.
지금 노란색 박스 부분이 3분의 2가 찼을 때 2배씩 성장하는 걸 흉내내봤고 실제로 얼마나 걸리는지 봤어요.
리사이즈 할 때 0.05초 걸려요.
100건 했을 때, 이게 100건 기준의 데이터니까 위에 100건 만드는 거 보이시죠?
69만건 넘어가는데 0.05초, 상당히 큰 수치예요. 이만큼 딕셔너리를 다룰 때 이럴 수 있다는 걸 이해하셔야 됩니다.
리사이즈를 최소화하고 싶다. 그러면 미리 크게 할당하고 시작하면 되겠죠.
Python에서 이거를 제공하는지 열심히 찾아봤는데 딱 하나를 찾았어요.
뉴프리사이즈라는 함수를 찾았는데 이 함수에서 보면 미리 할당하고 시작할 수 있게 제공을 해줘요.
그런데 좀 어렵게 호출을 하게 만들어놨더라고요.
맨 위에 보면, 노란색보다 위에 보면 어떤 타입으로 리턴됐는지 지정을 해놓고, 처음 노란박스 보면 프리사이즈, 몇 개로 버켓을 할지 지정을 하는 과정이 있어요.
이렇게 하고 진행을 해봤습니다. 프리사이즈 했을 때 뒤의 69만 건까지는 안 해주고 Python은 딱 12만 건 정도까지만 해줍니다.
그러니까 이 중 2분의 3 하면 4만 건 정도 되네요. 여기서 리사이즈 안 하고 데이터가 그로스 할 수 있게 해줘요.
43천 건 쪽 봤을 때 리사이즈라고 예상되는 구간에도 안 하고 시간이 절약되는 부분을 볼 수 있죠. 4만 건 했을 때 0.2초 정도 걸리는 걸 볼 수 있습니다. 결국 리사이즈가 어느 정도 부담이 되는지 느낌이 오시죠.
실제 이건 Python 소스고요. 프리사이즈를 했을 때 맥스로는 12만 건 정도까지만 최대프리사이즈를 할 수 있고요.
아래 실제 어떻게 데이터를 할당하는지 나와 있습니다.
생각보다 빨리 끝났어요.
내부 구조가 궁금하면 여러분들 소스를 볼 수 있어요.
C만 조금 이해할 수 있다면 쉽게 접근할 수 있을 거라고 생각합니다.
제가 참고한 부분입니다.
깃허브에 보면 Python 소스가 있어요. 버전별로도 올라와 있고요.
제가 참고한 자료들이 딱 3가지 있습니다. 이거 하면서 Python 소스 봤고요. 시간 복잡도, 해시테이블 봤습니다.
여러분이 프로그램 만드실 때 성능을 고려해야 할 때, 고려하지 않아야 될 때를 생각해야 될 텐데 성능이 중요하지 않은 거 같으면 성능을 고려하지 않고 빨리 개발하는 걸 저는 택하고 있어요.
왜냐하면 사람이 기계보다 비싸기 때문에 컴퓨터 좀 고생시키면 내가 빨리 개발하고 저는 다른 걸 할 수 있으니까. 그런데 가끔 보면 성능에 크리티컬한 부분이 있어요. 그런 부분은 끝까지 한번 파보는 게 개발할 때 중요합니다.
Python 볼 때 소스를 보면 어떤 결과를 보일지 쉽게 생각할 수 있는.
저희 회사 3명 채용중입니다. 저희 회사 피플펀드 채용 어떻게 하는지 궁금하면 URL 따라와서 보시면 됩니다.
-(사회자) 질의응답 시간 10분 가질 수 있게 됐습니다.
질문하실 분은 이 앞에 있는 스탠딩 마이크에 가서 질문을 해주시고요. 가급적이면 1~2개 정도의 질문만 부탁드립니다.
질문을 하면 책을 드려요.
-(플로어) 아까 immutable, mutable 설명하면서 immutable인 것처럼 tuple을 사용했다 설명을 해주셨는지 그 부분이 헷갈렸어요.
-(강대성) 다시 한번 볼게요. 이 부분이죠.
보통 mutable이라고 하면 객체가 변할 수 있는 게 mutable인데요. A=A B라고 했어요. 그러면 A가 만약에 변한다고 생각하고 이거를 만드시는 분도 있을 거예요. 하지만 실제 내부로는 객체를 없애고 새로 만든다고 보시면 돼요.
그래서 이거는 immutable이라고 보는 게 더 맞지 않을까 싶은데, 혹시 이해되셨나요?
그리고 뒤에 보면 이거 같은 경우에는 이게 mutable한 객체인데 immutable 하게 쓴 것 중 하나입니다.
Python 내부에서 좀 더 빠르게 처리했다를 보여드리고 싶었던 거예요.
아까 보시면 이 부분인데요. 1, 2가 있는 것과 빈 tuple을 더하기 했을 때 Python에서는 굳이 이거를 연산할 필요가 있었을까, 똑같은 객체로 변환해도 상관이 없는 거예요.
그래서 똑같이 한 거고요. 이게 옛날 Python에서는 똑같이 반환하지 않아요. Python 3.7인가 그때부터, 정확한 버전은 기억이 난 나는데 그때부터 바뀌어서 똑같은 객체를 뱉어주는 거. 만약에 뒤에 tuple이 있었다고 하면 다른 아이디로 변환해서 반환을 해주겠죠.
-(플로어) pop 그 부분 여쭤보고 싶은데요. 시간 복잡도가 pop을 하면 리스트에서 O(N)으로 계속 된다고 했는데 제가 임포트 컬렉션을 쓰면 시간 복잡도가 1로 될 수 있다고 알고 있는데 굳이 Python에서 pop을 하면 당겨 와서 그렇게 시간 복잡도를 길게 한 이유가 있는지, 그리고 그거를 그냥 임포트를 안 받고 시간 복잡도를 1로 할 수 있는 방도가 있는지 여쭤보고 싶습니다.
-(강대성) 이거를 pop을 하면, 리스트라고 하면 pop을 했으면 리스트는 메모리 구조상으로 봤을 때 연속된 메모리에 있는 거예요. 그런데 이거 중간에 하나 빼면 이 연속성을 맞춰야 하거든요.
이거는 동일 연속된 메모리에 있기 때문에 빠지면 뒤에 있는 걸 당겨v와야 하는 부담이 있어요. 실제로 Python 구현된 걸 봐도 당겨오게 되어 있고요.
만약에 이거를 시간 복잡도를 짧게 하고 싶으면 다른 걸 찾아야 할 거예요.
-(플로어) 그러면 혹시 알고리즘 시험 볼 때 시간적인 최적화를 위해서 할 때 링크리스트 그런 방식으로 해야 할까요?
-(강대성) 네.
-(사회자) 5분 남았네요.
-(플로어) 멀티코어를 사용하는 경우에 일정수 이상의 코어를 사용할 때 오히려 더 느려지는 경우가 있는데, 그런 경우가 메모리 할당하는 거에서 문제가 발생해서 발생하는 걸까요?
-(강대성) 멀티 코어를 사용할 때 물론 늦어지는 경우가 있을 수 있겠죠. 그런데 제가 여기서 다룬 주제랑 좀 많이 다른 거 같아요. 제가 거기까지는 알고 있지 않고요. 멀티코어를 사용하면 늦어질 수 있을 거 같아요. 왜냐하면 CPU를 왔다 갔다 하는 부담이 있는데...
-(플로어) 메모리를 초과한다든지 하는 이슈 때문에...
-(강대성) 멀티코어를 사용한다고 해서 메모리를 초과한다는 이슈는 없을 거고요. 만약에 똑같은 프로그램을 여러 개 돌린다고 하면 메모리 이슈가 조금 있을 수 있겠죠. 만약에 실제 메모리보다 더 큰 영역을 써야 한다고 하면 디스크를 써서 사용할 거고, 그러면 약간 느려질 수 있지만 그게 아니라면 멀티코어를 써서 많이 느려지는 부분은 없을 거 같아요.
-(플로어) 우선 좋은 강연 감사합니다. 개발을 오래 하셨는데 현업에서 어떻게 사용하는지 궁금했어요. 리스트나 이런 거를 바꿔가면서까지 하는지, 판다 시리즈라든지 대안 자료형들이 많이 있잖아요. 그런데 그런 것을 사용하지 않고 기본 자료형을 더 선호하는지 궁금했습니다.
-(강대성) 제가 큰 자료형을 다뤄보지 않아서, 거기에 대한 답은 좀 해드리기 어려울 거 같아요. 다른 거를 썼냐는 부분은... 실제 업무할 때 이런 거를 살펴보면서 업무를 하느냐 하면 그렇지 않아요. 만약에 성능에 진짜 중요한 부분이 있다.
평상시에는 문제가 없어요. 그런데 뭔가 심도 있게 프로그램을 해야 할 때 그때 문제가 발생하면 여기서 답을 찾을 수 있는 거죠.
왜 이건 갑자기 시간이 많이 걸렸을까? 시간이 정말 중요한데.
저희 회사 패스워드가 들어오면 해싱 하는 데 0.1초가 걸려요. 패스워드 해싱을 빠르게 할 수 있느냐? 그건 또 어려울 거 같고, 그러면 어떻게 해야 하느냐 그런 영역까지 가는 거죠. 크리티컬한 영역에서만 작용한다고 보시면 됩니다.
추가 질문 있으면 앞에서 하시면 되고요.
그러면 긴 시간 동안 발표해주신 강대성 님께 큰 박수 부탁드립니다.
Lighting Talk가 파이콘의 꽃입니다. 끝까지 남아주셔서 토크를 들어주시면 감사하겠습니다.
쉬는 시간 15분 가지고 잠시 후에 뵙겠습니다.