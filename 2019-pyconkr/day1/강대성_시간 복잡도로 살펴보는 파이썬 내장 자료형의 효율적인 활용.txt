https://youtu.be/XXGd_t6YF50

네 안녕하세요.

안녕하세요.
(안녕하세요)

제 목소리가 안 들리나 살짝 고민을 했습니다.

제가 발표할 주제는 시간 복잡도로 살펴보는

Python 내장자료형의 효율적인 활용이라는
주제로 발표를 하게되었습니다.

일단 제 소개를 간단하게 할게요.

저는 일단 16년차 개발자구요.

좀 오래됐죠? 한번쯤 웃어주셔야 하는데
안 웃어주네요.

16년째 개발하다 보니까 주변업무들을

개발보다 주변 업무들을 
많이 하는 거 같아요.

인프라, 네트워크, 보안 쪽 업무를 
많이 한 거 같고요.

지금은 피플펀드 컴퍼니에서 
기술고문으로 일하고 있습니다.

작년까지 cto 로 일하고 있었는데

제가 가정에 시간을 많이 써야 되는
일이 좀 생겨가지고

올해부터 기술고문으로 일하기로 
했습니다.

그리고

(스태프 마이크 조정 중)

그리고 2015년, 2017년에 스피커를 했었고요.

2년에 한 번씩 하고 있네요. 
2019년에도 하고 있네요.

(스태프 - 발표화면이 나오고 있지 않다고 이야기)

어 화면이 여기 안 나오네요.

안 나와도 그냥 제 목소리를 들으시고 
하시면 됩니다.

일단 오늘 이야기할 주제는 시간 복잡도를
먼저 이해를 할 거에요.

그리고 Mutable 과 Immutable

아시는 분도 있겠지만 이게

무슨 차이인지 이해하는 걸 할 거고요.

그 다음에 해야될 건 내장자료형의 시간 복잡도.

다 다루지는 못해요. 시간상.

자료를 하나씩 넣을 때 어떻게 바뀌는지 
그런 거를 할 거구요.

그리고 느린 부분을 극복하는 방법 
그걸 하게 될 겁니다.

화면이 안 나오니까 저도 좀 당황스럽지만

그래도 계속 하겠습니다.

일단 다시 한 번 (화면연결 잭을) 끼어보고

제 목소리 잘 들리죠? 
(네)

일단 시간 복잡도가 뭔지부터 이해를 할게요.

어 이것 좀 보여 드려야 되는건데

일단 위키에 있는 내용을 좀 가져왔어요.

다음.
저 혼자 (발표자료를) 계속 넘기면서 할게요.

(청중 웃음)

(스태프 - 5분정도 시간 여유를 드릴테니까 이야기를 더 나눠주시면)

(스태프에게) 그럴까요.

여러분 그럼 저희 무슨 이야기를 나눌 수 있을까요?

(청중 웃음)

피플펀드 컴퍼니에 대해서 좀 소개를 할까요. 
어떻게 하지 그러면 이걸.

(부스) 앞에 가시면 Python, Django만 
해가지고 노란색 케이스 받아보신 거 보셨나요?

아, 안 받아 보셨구나. 그거 저희 회사에서 한 건데

(스태프 안내 중)

(스태프 - 발표하기 전에 질문 1명)

아 혹시 질문하실 분 있나요?

(스태프 - 지금 화면 이슈로 해결하는 중이고요)

(화면이 나왔다가 꺼져서 
청중, 발표자, 스태프 모두 어~~)

(스태프 - 책을 받아 갈수 있는 기회인데)

(스태프 - 화면 이슈를 해결하기 전까지 
선착순 1명 으로 질문 받아서)

(스태프 - 질문해 주시는 분한테
책을 드리겠습니다.)

(청중 웃음)

오 (화면이)나와 버렸어 어떡해.

(스태프 - 책은 다음 기회에...)

(청중 웃음)

일단 계속 할게요.

원래 화면이 이거였어요.

근데 이거 읽으라고 하면 여러분들 
미칠 거 같으니까 이렇게 준비했습니다.

일단 시간 복잡도는 시간을 정량해서 
표기하는 거에요.

주로 빅 오 노테이션[Big-O notiation]으로 
많이 표현하고요.

낮은 차수항은 제하는 방법으로 합니다.

먼저 어떤 걸 계산을 할 때,

여러분 이거 안 찍으셔도 돼요. 
그 (발표자료)올라가요.

[5n^3 + 3n]으로 데이터가 나왔다면은

이거를 표현할 때 O의 n 세 제곱[O(n^3)]으로만
표현합니다.

이렇게 시간 복잡도를 표현한다는 것.

n 세 제곱하면 이 안에 뭐가 숨어있을지 모르겠으나
어쨌든 최고는

n의 세제곱 만큼 시간 복잡도을 가진다.
그 정도로 이해하시면 됩니다.

수행 시간을 간단하게 비교해볼 거에요.

상수 1, O(N), O(N^2)

앞에 한 건 일때의 숫자는 
별로 중요하지 않아요.

한 건 일때 5

아 어떻게 설명드려야하지.

한 건일때 시간이 5 만큼 걸렸고

O(N)은 10만큼 걸렸고,
O(N^2)은 10만큼 걸렸어요.

이게 100건일 때는 얼마큼 걸릴지
대략적으로 예측을 해보는 거에요.

O(1)은 5만큼 걸리고, 
O(N)은 1,000만큼 걸리고

100배만큼 늘어났으니까.

O(N^2)은 100의 제곱만큼 늘어나겠죠.

이렇게 시간복잡도를 계산하는 겁니다.

대략적으로 나타내면 이렇고요.
하나하나 말씀을 드릴게요.

O(1)같은 경우에는 
맨 아래 깔려 있는 거 있죠.

어떤 데이터가 들어와도 상수적으로 
데이터를 처리합니다.

진짜 빨리 처리해내는거죠.

이런 알고리즘 거의 없습니다.

O(N)은 데이터가 늘어나면 그에 맞춰서 
처리 속도도 늘어나는 거예요.

이해하셨죠? 네.

N제곱은 데이터가 4배 늘어나면 
처리 시간 4배 늘어납니다.

데이터가 늘어남에 따라서 시간이 
확 올라가는 형태에요.

좀 안 좋은 알고리즘 중에 하나인데

뭐 별 수 없이 이런 걸 써야할 때도 많이 있죠.

[NlogN].

이거는 데이터를 정렬할 때 최적의 성능을
가지면 이 정도 가집니다.

QuickSort로 구현하신다 그러면
NlogN은 가능할 거에요.

고개를 많이 끄덕거리는거 보니까
많이 배우신 내용 같네요.

빨리 하고 넘어갈게요.

logN 같은 경우는 [Binary Search]

logN으로 간단히 처리할 수 있게.
이 정도의 시간 복잡도가 나오겠죠.

이런 경우는 시간 복잡도가 어떻게 될까요?

(청중 : 2 제곱)

누가 n 제곱이라고 말씀을 해주셨네요.

왠지 저희 회사 사람 같지만

이거 같은 경우는 자세히 보시면

어느 정도 시간 복잡도를 가지냐고 예측해봤을 때

O(N^2)이라고 봐야하나 O(N)이라고 봐야하나
좀 헷갈릴 수도 있을 거에요.

근데 5천 번 루프로 돌리고 
input을 한 번만 돌렸잖아요.

input이 늘어남에 따라 
시간은 input 만큼만 늘어나요.

결국 이거는 O(N)만큼 시간 복잡도을 가지는 거죠.

5000N 이더라도 5000은 빼버리고 
N만큼의 시간복잡도를 가지는 겁니다.

그러니까 시간 복잡도는 뭐냐?

결국 데이터가 늘어났을 때 시간 증가의 경향을
나타낸다고 보시면 됩니다.

오늘 주로 O(1)과 O(N)정도만 다루게 됩니다.

복잡한 것들이 아니라서 이 정도만 다루게 되고요.

본격적으로 들어가기 전에 
immutable과 mutable을 다루게 될 거에요.

immutable은 한 번 만들어지면 
변하지 않는 객체들.

튜플[Tuple]이나 스트링[String]이나 바이트[Byte]나 
프로즌 셋[Forzen Set] 같은 걸 이야기하는 거죠.

한 번 데이터를 만들면은 변하지 않아요.

mutable은 변할 수 있는 객체들.

리스트[List]나 딕셔너리[Dictionary] 같은
경우에는

리스트에는 뒤에 데이터를 
하나씩 추가시킬 수 있잖아요.

여러분 처음 파이썬 배우실 때

튜플하고 리스트하고 가장 다른 것은 
변화할 수 있냐 없냐

그 정도로 이야기를 다루잖아요.

딱 그 정도 입니다.

그러면 아까 tuple은 immutable이라고 배웠는

과연 여기서, 이렇게 A를 변환해서 넣었잖아요.

이거는 mutable일까요 immutable일까요?

이거는 immutable죠. 
mutable로 생각하고 코딩하시는 분들도 가끔 있는데.

immutable입니다.

실제로 Python 내부에서 이거를 
어떻게 처리하느냐 생각해보면

확실히 immutable인지 알 수 있는데

Python 내부에서는 [tupleconcat]이라는 
함수를 통해서 처리를 해요.

[a]와 [b]가 여기 들어가는 거죠.

실제 Python이 구현된 소스입니다.

C 로 되어있고요.
파이썬은 C로 구현되어 있으니까 C로 구현된 거고.

여기 첫 번째 tupleconcat 이라는 곳으로
데이터를 받고

두 번째 노란색 보면

이게 혹시 a가 비었는지

그리고 두번째 세번째 노란색 보면

앞에 b가 비었는지 그것까지 체크합니다.

만약에 비어있는 거면은 빠르게 return하고 
끝나버려요.

안 빈 곳 쪽으로.

만약에 둘 다 비어있지 않고 
둘 다 차 있는 튜플이면

두 개를 더한 후에 그거를 복사합니다.

그리고 그걸 새로 반환하는거죠.

결국 immutable의 객체니까 
어쩔 수 없이 이렇게 하겠죠.

그래서 아까 설명드리려고 한 거,

a 와  b를 더했는데 하나가 비었다, 
b가 비었다고 하면

a하고 c하고 결국 똑같은 객체인 거예요.

이해하셨죠?

여기IDE 화면은 객체 레퍼런스를 이야기 한건데

뒤에 800 으로 똑같이 끝나죠

여러분 표정이 안 좋은 거 보니까 
좀 헷갈릴 수 있겠지만

두 개 같은겁니다.

아까 딱 그 여기 세 번째 노란색 박스 있죠.

그 부분에 해당되는 경우입니다.

b가 비었으니까 a를 리턴하고 끝내버리는
경우에요.

tuple 같은 경우에는 더하기를 하면

새로운 tuple 만들고 복사해서 
다시 리턴해주는 구조입니다.

그리고 이것의 시간복잡도는
O(N+M)이겠죠.

앞의 tuple과 뒤의 tuple의 더한 것만큼
시간 복잡도를 가지겠죠.

이제 리스트로 넘어왔어요.

이거는 immutable처럼 쓴 거처럼 보이죠.

넘어갈게요.

실제로 Python 내부에서 이거를 어떻게 
처리하느냐보면

이거 같은 경우는 tuple 하고 다르게

리스트 안에 데이터가 없는지 비교하지 않아요.

비교하지 않고.

그냥 2개를 사이즈를 더해서 새로운 걸 만들어서,
할당해서 다시 내뱉는 역할만 해요.

결국에는 아까처럼 이렇게 쓰면 
tuple과 다르게 쓴 건 없죠?

내부적으로 실제로는 mutable한 객체를
immutable하게 쓴 예제 중 하나일 뿐입니다.

이거는 tuple하고 비교하기 위해서 써봤고요.

이거는 b가 비어도 새로운 객체를 만들어서 
뱉어 준다.

그 정도로 이해하시면 되고요.

실제로 아까 이거를 지금 덧셈한 거를

실제로는 익스텐드[extend] 한 것처럼 써야지 
맞겠죠. mutable하게 쓰려면

이제 extend에 대해서 좀 이야기 할게요

익스텐드는 실제로 Python 내부에서 
이렇게 운영됩니다.

m + n, 실제 두 개의 객체

앞에 1,2를 가진 것과 
3,4를 가진 것

객체를 길이를 보고

그거를 그만큼 그 앞에 거

원래 l이라는 리스트를 리사이즈 한 후에
뒤의 거를 복사해 넣습니다.

리스트는 익스텐드 오퍼레이션은 
O(N+M)의 시간복잡도를 일반적으로 가지고요.

리사이즈를 하지 않을 경우에는
[O(M)] 정도로 시간복잡도를 가집니다.

여기까지 좀 서론적인 이야기였구요.

이제 진짜 본론적으로 시작할게요.

여러분 리스트 많이 쓰시죠?

Python 쓰시면서 리스트 안 쓰시는 분들
없을 겁니다.

리스트 pop할 때 어떤 일이 일어나는지 볼게요.

pop 세 번째 거, Python은 제로베이스니까
3이라고 하면 네 번째를 이야기하겠죠.

네번째 걸 pop을 한다 가정하면,

세 번째 거를 뺀 후에 그 다음 거를 
앞으로 밀어버리겠죠.

이렇게 내부적으로 구현됩니다.

여기까지는 크게 아름답다, 아름답지 않다 없이
보통 이렇게 처리된다고 생각할 수 있을 거에요.

그런데 만약에 데이터가 더 많다고 가정을 할게요

세번째 걸 뺐어요.

사람으로 따지면 네번째를 뺐어요.

그러면 뒤에 있는 거를 다 밀어줘야 해요.

이렇게 여기서 미는데

시스템 내부적으로는, 
우리가 아름다움까지 고민할 필요가 있는데

내부적으로는 뒤에 있는 걸 
다 민다고 생각하는 거예요.

제가 [백만스물하나] 적은 것은 
에너자이저 생각해서 적은 건데

에너자이저가 미는 것처럼 다 민다고
생각을 하셔야 돼요.

여러분 정말 아름다운가,

시간이 빨리 처리되는가를 보셔야하는데

파이썬 내부에서는 이걸 어떻게 구현해냈냐

일단 pop 이라는 걸 호출할 거고요

pop이라는 함수를 호출하면 내부적으로는 
[list_pop]이라는 것을 호출하게 될 거고,

그 다음에 짜르는 걸 호출할 거고

그리고 마지막으로 메모리를 옮기라고 
호출을 할 거에요.

그런데 세번째, 메모리 옮기라는 게 
엄청난 무브를 가져옵니다.

여러분이 상상할 수 없을 만큼
엄청난 오퍼레이션을 가져오게 되는 거죠.

결국 이거는 O(N)이라는 시간 복잡도를 
가지게 되는 거에요.

하나의 데이터를 뺐을 뿐인데.

뺀 거에서 뒤에 있는 거를 싹 밀면서
O(N)이라는 시간을 가지는 거에요.

결국 이 하나를 함으로써 엄청난 부하가 
CPU나 메모리에 갈 수 있다는 겁니다.

물론 여러분은 고생을 안하겠죠.

컴퓨터가 약간 고생할 거를 생각하면서

이거 하면 고생하지 않을까라고 생각하시면서 
프로그램을 만드시면 됩니다.

좀 복잡한 append를 고민해볼게요.

append를 하게 되면은

append 4를 했어요.

이게 O(1) 같은가요? O(N) 같은가요?

역시 이야기 안 하실 줄 알았어요.

이게 아름다워보이느냐에 대해서 
약간 고민할 필요가 있어요.

역시 여기도 [백만스물하나]가 나왔는데

4부터 list에 a을 할당해놓고

4 부터 n 까지 백만스물하나까지

[append]를  하는 것을 고민해 봤어요

지금 박스 처져있는 루틴이 
O(N)만에  끝날까 안 끝날까

대해서 약간 고민할 필요가 있는 거예요.

리스트에서 실제로 어떤 일이 발생할지,

append하면 어떤 일이 발생하는지 좀 보겠습니다.

처음에 빈 리스트죠.

[append(1)]하면 새로운 메모리를 할당해서 
1을 넣습니다.

두번째 2를 append하면 1,2 를 
할당해서 넣죠.

메모리를 할당해서
복사합니다.

그러니까 1을 없애고 1,2 를 할당해서 
넣는 거죠.

자 3을 하면 이렇게 넣어요.

그러니까 복사한다는 의미는

이게 아까 빅-오-어노테이션 나타냈을때
O(N)

기존 데이터를 N이라고 가정했을 때 
O(N) 만큼 시간이 소요된다는 걸 의미해요.

할 때마다 O(N) 만큼 시간이 걸릴까에 대해서
약간 고민할 필요가 있어요.

만약 [list.append]를 자주 썼다

여러분이 데이터 많이 넣고 많이 썼다

괜찮을까. 좀 충격이지는 않으신가요...

네 아니군요. 표정은 네.

하지만 제가 봤을 때는 이거 
괜찮지 않은 부분입니다.

그래서 설마 Python을 이렇게 
대충 만들었을리 없다하고

자세히 살펴보면은

리스트 리사이즈 할 때 
0, 4, 8, 16, 25, 35, 46 단위로 list를 추가합니다.

보통 리스트에서는 그로스 패턴이라고 
이야기를 하는데

하나만 추가해도 메모리 4개 를 할당해버려요.

왜냐하면 하나씩 계속 추가하면

엄청난 오퍼레이션 일어나니까 
하나씩 추가합니다.

예를 좀 보여 드릴게요.

빈 리스트에요.

하나를 추가해도 1 할당하고 
나머지 3칸은 빈 칸으로 놔둡니다.

또 하나 추가하면 메모리 복사할 필요없이 
새로 할당할 필요없이 1을 넣어요

3 넣고 4 넣습니다.

이제 5를 넣을 때에야 
아까 4 다음에 8, 그로스 패턴이라고 말을 했죠.

5를 넣을 때 비로소 데이터를 늘려줍니다.

6 넣고 이런 식으로 하는 거죠.

결국 Python이 취한 방식은

메모리를 조금 더 낭비를 하고 
시간에서 더 이득이 생기도록 만들었어요.

조금씩 append를 해도 큰 부담이 안 가게.

아까 말씀드렸다시피 그로스 패턴이 있고요
리스트에는

보통 할당할 때

[전체 크기 / 8]한 것 만큼 점거한 거만큼 
또 여유 공간을 확보해 줍니다.

항상 확보하는 아니고 모자라다 싶을 때 
그때 1/8 만큼

정확히 말하면
[n/8 + (6 or 3)] 이에요.

그만큼 더 할당을 해놔요.

3이 되는 것은 9보다 작은 원소를 가질 때고
보통 6을 씁니다.

그래서 이제 얼마나 느린가 테스트를 해봤어요.

100개의 리스트를 위해 
[위에 노란색] 보면

100개의 리스트를 다 준비하고

그 리스트에 데이터를 하나씩 넣는 걸 
테스트 해봤습니다.

리사이즈가 발생하면 저 뒤에 true 라고 
찍혀 있는 거예요.

여기 보면 리사이즈가 될 때나 
안 될 때나

큰 차이가 없어보입니다.

많이 차이나봤자 10배 밖에 
차이가 안나죠.

네, 표정이 어두우시네요. 네.

혹시 잘 이해가 안되시나요?

이해되신다고 하고 갈게요.

근데 사이즈가 커진다,

많이 커지면 이게 뭐 1000배 이상 차이가 나기도 합니다.

만약에 맨 아래쪽에 0.09초, 
약 0.1 초 정도 걸린 건데

100번 했으니까...
계산이 좀 제가 안되네요

그만큼 걸리는 거 겠죠. 
그게 100번 한 거 기준이에요.

그러니까 100번 할 때,

리사이즈하면은 이만큼의 시간이 
더 걸립니다.

평균적으로 보면 O(1)에 가까워요.

그런데 가끔 O(N)이 발생합니다.

이걸 이해하시고 인제 개발하시면 
더 좋긴 하겠죠.

그리고 만약에 나는 리사이즈 하면 안 돼,

저렇게 성능이 튀는 걸 용서할 수 없어
그러면은

미리 할당을 하고 시작합니다.

100만 개를 한 번에 할당을 시킬 수도 있고요.

미리 할당을 하고.

Python에서 정한 최고 사이즈 크기가 있을 거에요.

그 사이즈만큼 미리 할당하고 
할 수도 있어요.
(여기까지 자막싱크)

여기서 안 했는데 측정을 할 때는 
미리 할당하고 했을 때 어떤 차이가 있는지 보여드릴게요.

그리고 append 하는 것 대신에

진짜 할당하면 되겠죠.

그러니까 [None] 으로 이미 다 채워놨으니까

할당하는 것이라고 할 수도 있습니다.

딕셔너리로 넘어갈게요.

딕셔너리는 다들 써보셨죠?

다 써 봤을 거라 믿습니다.

일단 딕셔너리 내부 구조는 
지금 Python에서는 이런 구조를 가지지 않아요.

근데 인터넷에 해쉬 테이블 찾아보면 
가장 먼저 위키에 뜨는 것이 이 그림입니다.

여러분도 이해하기 쉬우실 거고요.

해쉬펑션(hash function)의 키를 넣으면 
특정 버켓(bucket)을 지정해줘요.

그럼 나중에 데이터를 찾을 때

키만 넣으면 내가 원하는 데이터를 
빨리 끄집어낼 수 있어요.

결국에는 시간복잡도가 거의 모든 오퍼레이션이

빅-오-노테이션으로 나타냈을 때 
상수값으로 거의 끝나버려요.

문제는 버켓 사이즈가 문제가 됩니다.

우리 자원이 무한하다면 버킷사이즈를
크게 하고 시작할 수 있겠지만

크게 하고 시작하지 못해요.

대부분 Python은 버켓 사이즈 
8로 해놓고 시작을 합니다.

버켓이 다 찰 때까지 기다리지 않고

3분의 2가 차면 리사이즈 해버려요.

아, 네. 제가 답을 말해버렸네요.

아 이거 퀴즈로 내려 그랬는데.

3분의 2가 차면 리사이즈 하는 
형식으로 가게 됩니다.

그리고 이거는 나름 [collision]을 생각해서 
최적의 수치라고 저는 믿고 있어요.

파이썬 개발하신 분들이.

그래서 처음에 8로 했는데

3분의 2가 차면 현재 있는 
데이터량의 3배로 성장을 시킵니다.

이 말은 [2/3 * 3] 하면은 2배죠.

파이썬 3.7 부터 2배씩 성장하게 되어있어요.

무조건 현재 데이터에서 리사이즈 할 때 2배씩,

8이면 16 그 다음엔 32
이런 식으로 성장하겠죠.

실제 Python 소스는 이렇게 생겼어요.

[insertion_resize]라는게 호출이 되는데 
insert 할 때에.

그게 보면은

[dictresize]에 [GROWTH_RATE] 하고

mp, mp라고 되어있죠. 
mp가 딕셔너리 오브젝트입니다.

그 밑에 네모난 곳 보면은

[ma_used] * 3 이라고 되어있어요.

현재 사용한 거에 3배로 성장시켜라

3분의 2만큼 썼으니까, 곱하기 3하면
2배 만큼 성장 시키는 거죠

계속 2배만큼 성장시킵니다.

리사이즈 할 때도 2배 단위로만
성장하도록 해놨어요.

C 를 많이 써 보신 분은 알겠지만 
이 for의 가장 아래 것.

[newsize <<=1] 이게 비트 연산한다는 거
곱하기 2 라는거 아실 거에요.

아실 거라 믿습니다.

리사이즈 하게 되면 실제 Python에서 
벌어지는 일이 뭐냐

새로운 메모리를 할당하고 복사를 합니다.

그리고 해시 밸류를 보고

버켓과 연결하는 과정이 필요해요.

여러분 아까 보신 이거

해쉬 펑션을 태워서

버켓에 할당하는 과정,

Python도 똑같이 이 과정을 하는데

해쉬밸류는 미리 구해놨어요.

그거를 보고 버켓에 지정을 해주는 
역할을 합니다.

흉내내봤어요. 
저도 이제 시간이 얼마나 걸리나 봐야되니까

지금 노란색 박스 부분이

3분의 2가 찼을 때 
2배씩 성장하는 걸 흉내내봤고

실제로 얼마나 걸리는지 봤어요.

69만 건이 넘어가니까 
리사이즈 할 때 0.05초 걸려요.

69만 건을 다룰 일이 없을 수도 있겠지만 
이만큼 다룰 때는

리사이즈하면 0.05초

100건 했을 때, 이게 100건 기준의 데이터니까 
위에 100건 만드는 거 보이시죠?

네 (발표종료)15분 전이라고 이야기를 해주시는데 
저는 곧 끝날 거 같네요.

천천히 하겠습니다.

69만건 넘어가는데 0.05초, 
상당히 큰 수치예요.

이만큼 딕셔너리를 다룰 때

이럴 수 있다는 걸 이해하셔야 됩니다.

리사이즈를 최소화하고 싶다,

그러면 미리 크게 할당하고 시작하면 되겠죠.

Python에서 이거를 제공하는지 
열심히 찾아봤는데 딱 하나를 찾아왔어요.

잠깐 이거부터 보여드릴게요.

[NewPresized]라는 함수를 찾았는데

이 함수에서 보면은

미리 할당하고 시작할 수 있게 제공을 해줘요.

그런데 좀 어렵게 호출을 하게 만들어놨더라고요.

맨 위에 보면, 노란색보다 위에 보면

어떤 타입으로 리턴됐는지 지정을 해놓고,

처음 노란박스 보면

프리사이즈
하고 몇 개로 프리사이즈 할지

버켓을 몇으로 할지 지정해놓는 과정이 있어요.

이렇게 하고 좀 해봤어요.

프리사이즈 했을 때,
뒤에 69만 건까지는 안해주고

Python은 딱 12만 건 정도까지만 해줍니다.

그러니까 이 중 2분의 3 하니까

4만 건 정도 되네요.

여기서 리사이즈 안 하고 
데이터가 그로스(growth) 할 수 있게 만들어줘요.

지금보면 4만 3천건 쪽에 봤을 때

데이터가 리사이즈해도

리사이즈라고 예상되는 구간에도 
리사이즈 안되고

시간이 절약되는 것 볼 수 있죠.
그 윗 부분까지.

아까 결과를 봤을 땐 4만 건 정도에 
0.2초 정도 걸리는 걸 볼 수 있습니다.

결국 리사이즈가 어느 정도 부담이 되는지 
느낌이 오시죠.

실제 이건 Python 소스고요.

프리사이즈를 했을 때

Max로는 12만 건 정도까지만 
최대 프리사이즈를 할 수 있고요.

아래 실제 어떻게 데이터를 할당하는지 
나와 있습니다.

생각보다 빨리 끝났네요.

중간에 안되어서 빨리 해버렸지만

그래도 가겠습니다.

내부 구조가 궁금하면 여러분들도 
소스를 볼 수 있어요.

소스가 그렇게 어렵지 않고요.

C만 조금 이해할 수 있으면, 
C랑 중요한 건 포인터 볼 수 있으면

소스 볼 수 있습니다

처음에 어렵지만 좀 공부해서 보시면은 
좀 쉽게 접근할 수 있을거라 생각합니다.

제가 참고한 건데, github에 보면 
파이썬 소스가 있어요.

각 버전별로도 올라와 있고요.

이건 뭐 가서 어렵지 않게 볼 수 있지 않을거라
생각합니다.

제가 참고한 자료들이 딱 3가지 있습니다.

이거 하면서 Python 소스 봤고요.

시간 복잡도, 해시테이블 봤습니다.

여러분이 프로그램 만드실 때 성능을 고려해야 할 때,

고려하지 않아야 될 때를 생각하셨으면 좋겠는데

성능이 중요하지 않은 거 같으면

성능을 고려하지 않고 빨리 개발하는 걸 
저는 택하고 있어요. 일반적으로.

왜냐하면 사람이 기계보다 비싸기 때문에

컴퓨터 좀 고생시키면

내가 빨리 개발하고 
저는 다른 걸 할 수 있으니까.

그런데 가끔 보면 성능에 크리티컬한 부분이 
있어요.

그런 부분은 끝까지 한번 파보는 게 
개발할 때 중요합니다.

Python 볼 때 소스를 보면 어떤 결과를 보일지 
쉽게 생각할 수 있는.

이거 몇 분 보신 분도 있겠지만

저희 회사 3명 채용중이고요.

말이 세 명이지만

암튼 세 명 채용 중이고

저희 회사 피플펀드 채용 어떻게 하는지 
궁금하면 URL 따라서 오시면 됩니다.

제 발표는 여기까지고요. 
질문있으시면 질문하셔도 됩니다.
(검수 완료)